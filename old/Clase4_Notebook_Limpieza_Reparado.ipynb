{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bd4a9e3",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color:#f2f2f2;padding:20px;text-align:center;\">\n",
    "  <h1 style=\"color:#0b61a4;margin:0;\">Clase 4 — Notebook de Limpieza y Validación de DataFrames</h1>\n",
    "  <p style=\"color:#0b61a4;margin:0;\">Pipeline pedagógico para detectar, normalizar y reportar problemas en datasets CSV</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7322d5",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color:#f9f9f9;padding:12px;\">\n",
    "<p style=\"color:#0b61a4;\">\n",
    "Este notebook está diseñado con propósito educativo: muestra un flujo completo para cargar tres CSV (ventas, clientes, marketing),\n",
    "detectar problemas (duplicados, nulos, tipos, outliers), aplicar reglas de limpieza parametrizadas y generar reportes 'antes' y 'después'.\n",
    "</p>\n",
    "\n",
    "<p style=\"color:#0b61a4;\">\n",
    "Se han unificado los nombres de variables al español y en formato <code>snake_case</code>.<br>\n",
    "- <code>issues_antes</code> → <code>problemas_antes</code><br>\n",
    "- <code>issues_despues</code> → <code>problemas_despues</code><br>\n",
    "- <code>chequeos_columnas_despues</code> → <code>chequeos_despues</code>\n",
    "</p>\n",
    "\n",
    "<p style=\"color:#0b61a4;\">\n",
    "Las celdas contienen documentación explicativa y las funciones están completamente comentadas en español para uso docente.\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43fa85de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ruta de entrada: D:\\Desktop\\Domingo\\df_caba_y_jupyter\\datasets_entrada\n",
      "Ruta de salida: D:\\Desktop\\Domingo\\df_caba_y_jupyter\\datasets_salida\n",
      "Directorio de reportes: D:\\Desktop\\Domingo\\df_caba_y_jupyter\\datasets_salida\\reportes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Importar librerías necesarias\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Para métricas estadísticas simples\n",
    "from scipy import stats\n",
    "\n",
    "# Configuración de rutas por defecto (local)\n",
    "ruta_base = os.path.abspath(\".\")\n",
    "ruta_entrada = os.path.join(ruta_base, \"datasets_entrada\")\n",
    "ruta_salida = os.path.join(ruta_base, \"datasets_salida\")\n",
    "directorio_reportes = os.path.join(ruta_salida, \"reportes\")\n",
    "\n",
    "# Asegurar que existen las carpetas necesarias\n",
    "os.makedirs(ruta_entrada, exist_ok=True)\n",
    "os.makedirs(ruta_salida, exist_ok=True)\n",
    "os.makedirs(directorio_reportes, exist_ok=True)\n",
    "\n",
    "# Mostrar rutas configuradas\n",
    "print(\"Ruta de entrada:\", ruta_entrada)\n",
    "print(\"Ruta de salida:\", ruta_salida)\n",
    "print(\"Directorio de reportes:\", directorio_reportes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1789bc7",
   "metadata": {},
   "source": [
    "\n",
    "<h3 style=\"color:#0b61a4;\">Funciones auxiliares de normalización y utilidad</h3>\n",
    "<p style=\"color:#0b61a4;\">A continuación definimos funciones reutilizables que ayudan a normalizar nombres, limpiar textos, convertir tipos y detectar duplicados.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17ad985e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalizar_nombre_columna(nombre: str) -> str:\n",
    "    \"\"\"\n",
    "    Normaliza un nombre de columna a snake_case, sin tildes y en minúsculas.\n",
    "    Ejemplo: 'Fecha Venta' -> 'fecha_venta'\n",
    "    \"\"\"\n",
    "    import re\n",
    "    # Remover tildes básicas\n",
    "    reemplazos = str.maketrans(\"ÁÉÍÓÚáéíóúÑñ\", \"AEIOUaeiouNn\")\n",
    "    nombre = nombre.translate(reemplazos)\n",
    "    # Reemplazar separadores por underscore\n",
    "    nombre = re.sub(r'[^\\w]+', '_', nombre)\n",
    "    nombre = nombre.strip('_').lower()\n",
    "    return nombre\n",
    "\n",
    "def normalizar_nombres_columnas(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aplica normalización de nombres de columnas a todo el DataFrame.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df.columns = [normalizar_nombre_columna(c) for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def convertir_a_numerico(col: pd.Series, remove_non_digits: bool = False, as_int: bool = False) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Intenta convertir una columna a tipo numérico.\n",
    "    - remove_non_digits: elimina caracteres no numéricos (útil para montos con símbolos)\n",
    "    - as_int: si True, convierte a int cuando sea posible (rellena NaN con 0 antes)\n",
    "    \"\"\"\n",
    "    s = col.astype(str).copy()\n",
    "    if remove_non_digits:\n",
    "        s = s.str.replace(r'[^0-9\\.\\-]', '', regex=True)\n",
    "    # Reemplazar valores vacíos o solo '-' por NaN\n",
    "    s = s.replace({'': np.nan, 'nan': np.nan, '-': np.nan})\n",
    "    # Convertir a float\n",
    "    s = pd.to_numeric(s, errors='coerce')\n",
    "    if as_int:\n",
    "        # Rellenar NaN con 0 temporalmente para conversion segura si se desea\n",
    "        s = s.fillna(0).round().astype('Int64')\n",
    "    return s\n",
    "\n",
    "def intentar_convertir_fecha(col: pd.Series, formatos: list = None, dayfirst: bool = True) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Intenta convertir una columna a datetime usando varios formatos.\n",
    "    Si 'formatos' es None, utiliza infer_datetime_format.\n",
    "    \"\"\"\n",
    "    if formatos:\n",
    "        for fmt in formatos:\n",
    "            try:\n",
    "                parsed = pd.to_datetime(col, format=fmt, dayfirst=dayfirst, errors='coerce')\n",
    "                # Si se parseó con éxito muchos valores, adoptamos este formato\n",
    "                if parsed.notna().sum() > 0:\n",
    "                    return parsed\n",
    "            except Exception:\n",
    "                continue\n",
    "    # Fallback: inferir\n",
    "    return pd.to_datetime(col, errors='coerce', dayfirst=dayfirst, infer_datetime_format=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a37601",
   "metadata": {},
   "source": [
    "\n",
    "<h3 style=\"color:#0b61a4;\">Función: detectar_problemas_dataframe</h3>\n",
    "<p style=\"color:#0b61a4;\">Detecta problemas comunes en un DataFrame: duplicados, nulos por columna, tipos inconsistentes y outliers básicos (IQR).</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8801764",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detectar_problemas_dataframe(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Analiza un DataFrame y devuelve:\n",
    "    - resumen: DataFrame resumen con métricas por columna\n",
    "    - chequeos: DataFrame con detalles por columna (tipos detectados, nulos, únicos)\n",
    "    - problemas: DataFrame con filas que contienen al menos un problema detectado (duplicados o nulos críticos)\n",
    "\n",
    "    NOTAS:\n",
    "    - Esta función es didáctica; se puede ampliar para detectar outliers avanzados o reglas de negocio.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    resumen = []\n",
    "    chequeos = []\n",
    "    n = len(df)\n",
    "\n",
    "    # Detección de duplicados (filas completas)\n",
    "    duplicados = df.duplicated(keep=False)\n",
    "\n",
    "    for col in df.columns:\n",
    "        serie = df[col]\n",
    "        n_nulos = serie.isna().sum()\n",
    "        n_unicos = serie.nunique(dropna=True)\n",
    "        tipo_inferido = serie.dropna().map(type).value_counts().index.tolist()\n",
    "        tipo_str = ','.join([t.__name__ for t in tipo_inferido]) if tipo_inferido else 'empty'\n",
    "        # Detectar outliers simples por IQR si la columna es numérica\n",
    "        outliers = None\n",
    "        if pd.api.types.is_numeric_dtype(serie):\n",
    "            q1 = serie.quantile(0.25)\n",
    "            q3 = serie.quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            lower = q1 - 1.5 * iqr\n",
    "            upper = q3 + 1.5 * iqr\n",
    "            outliers = int(((serie < lower) | (serie > upper)).sum())\n",
    "        else:\n",
    "            outliers = np.nan\n",
    "\n",
    "        resumen.append({\n",
    "            'columna': col,\n",
    "            'tipo_aproximado': tipo_str,\n",
    "            'nulos': int(n_nulos),\n",
    "            'unicos': int(n_unicos),\n",
    "            'outliers_estimados': outliers\n",
    "        })\n",
    "        chequeos.append({\n",
    "            'columna': col,\n",
    "            'tipo_pandas': str(serie.dtype),\n",
    "            'ejemplo_valores': ','.join(map(str, serie.dropna().unique()[:5])),\n",
    "            'porcentaje_nulos': float(n_nulos) / n if n>0 else 0.0\n",
    "        })\n",
    "    resumen_df = pd.DataFrame(resumen)\n",
    "    chequeos_df = pd.DataFrame(chequeos)\n",
    "\n",
    "    # Filas con problemas: duplicados o filas con todos los valores NaN\n",
    "    filas_problema_mask = duplicados | df.isna().all(axis=1)\n",
    "    problemas_df = df[filas_problema_mask].copy()\n",
    "    # Si no hay filas problema, devolver empty DataFrame con columnas del origen + motivo\n",
    "    if problemas_df.empty:\n",
    "        problemas_df = pd.DataFrame(columns=list(df.columns) + ['motivo'])\n",
    "    else:\n",
    "        # Añadir motivo\n",
    "        motivos = []\n",
    "        for idx, row in problemas_df.iterrows():\n",
    "            m = []\n",
    "            if duplicated := df.loc[[idx]].duplicated(keep=False).any():\n",
    "                m.append('duplicado')\n",
    "            if row.isna().all():\n",
    "                m.append('fila_vacia')\n",
    "            motivos.append(';'.join(m) if m else 'otro')\n",
    "        problemas_df['motivo'] = motivos\n",
    "\n",
    "    return resumen_df, chequeos_df, problemas_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66180b43",
   "metadata": {},
   "source": [
    "\n",
    "<h3 style=\"color:#0b61a4;\">Función: aplicar_reglas_limpieza</h3>\n",
    "<p style=\"color:#0b61a4;\">Aplica un conjunto de reglas parametrizadas a un DataFrame para normalizar tipos y valores.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c155f300",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def aplicar_reglas_limpieza(df: pd.DataFrame, reglas: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aplica reglas sobre columnas especificadas en 'reglas'.\n",
    "    'reglas' es un dict donde la clave es el nombre de la columna (ya normalizado)\n",
    "    y el valor indica la operación a realizar:\n",
    "      - 'title', 'lower' -> operaciones de cadena\n",
    "      - ('numeric', params) -> convertir a numérico (params es dict con opciones)\n",
    "      - ('date', params) -> convertir a datetime (params puede contener 'formats' list)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    for columna, regla in reglas.items():\n",
    "        if columna not in df.columns:\n",
    "            # No existe la columna; doc: se saltea silenciosamente pero se informa\n",
    "            print(f\"Aviso: la columna '{columna}' no existe en el DataFrame. Se omite.\")\n",
    "            continue\n",
    "        if regla == 'title':\n",
    "            df[columna] = df[columna].astype(str).str.strip().str.title().replace({'nan': pd.NA})\n",
    "        elif regla == 'lower':\n",
    "            df[columna] = df[columna].astype(str).str.strip().str.lower().replace({'nan': pd.NA})\n",
    "        elif isinstance(regla, tuple) and regla[0] == 'numeric':\n",
    "            params = regla[1] if len(regla) > 1 else {}\n",
    "            remove_non = params.get('remove_non_digits', False)\n",
    "            as_int = params.get('as_int', False)\n",
    "            df[columna] = convertir_a_numerico(df[columna], remove_non_digits=remove_non, as_int=as_int)\n",
    "        elif isinstance(regla, tuple) and regla[0] == 'date':\n",
    "            params = regla[1] if len(regla) > 1 else {}\n",
    "            formatos = params.get('formats', None)\n",
    "            dayfirst = params.get('dayfirst', True)\n",
    "            df[columna] = intentar_convertir_fecha(df[columna], formatos=formatos, dayfirst=dayfirst)\n",
    "        else:\n",
    "            print(f\"Aviso: regla desconocida para columna '{columna}': {regla}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c47774",
   "metadata": {},
   "source": [
    "\n",
    "<h3 style=\"color:#0b61a4;\">Función: procesar_carpeta</h3>\n",
    "<p style=\"color:#0b61a4;\">Procesa todos los CSV en una carpeta de entrada, aplica limpieza, crea reportes 'antes' y 'después' y guarda resultados.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "371cd230",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def procesar_carpeta(ruta_entrada: str, ruta_salida: str, reglas_columnas: Dict[str, Any]=None):\n",
    "    \"\"\"\n",
    "    Recorre todos los archivos CSV en 'ruta_entrada', aplica detección de problemas,\n",
    "    limpieza con 'reglas_columnas' y guarda:\n",
    "      - reportes antes y después en ruta_salida/reportes/\n",
    "      - archivos limpios en ruta_salida/csv_limpios/\n",
    "      - un ZIP con todos los reportes\n",
    "\n",
    "    Variables de salida estandarizadas en español:\n",
    "      - problemas_antes\n",
    "      - resumen_antes\n",
    "      - chequeos_antes\n",
    "      - problemas_despues\n",
    "      - resumen_despues\n",
    "      - chequeos_despues\n",
    "    \"\"\"\n",
    "    os.makedirs(ruta_salida, exist_ok=True)\n",
    "    carpeta_reportes = os.path.join(ruta_salida, \"reportes\")\n",
    "    carpeta_csv_limpios = os.path.join(ruta_salida, \"csv_limpios\")\n",
    "    os.makedirs(carpeta_reportes, exist_ok=True)\n",
    "    os.makedirs(carpeta_csv_limpios, exist_ok=True)\n",
    "\n",
    "    archivos = [f for f in os.listdir(ruta_entrada) if f.lower().endswith('.csv')]\n",
    "    if not archivos:\n",
    "        print(\"No se encontraron archivos CSV en la carpeta de entrada:\", ruta_entrada)\n",
    "        return\n",
    "\n",
    "    for archivo in archivos:\n",
    "        nombre_archivo = os.path.splitext(archivo)[0]\n",
    "        ruta_archivo = os.path.join(ruta_entrada, archivo)\n",
    "        print(f\"\\nProcesando: {ruta_archivo}\")\n",
    "\n",
    "        # Cargar CSV\n",
    "        df = pd.read_csv(ruta_archivo, encoding='utf-8', low_memory=False)\n",
    "        # Normalizar nombres de columnas\n",
    "        df = normalizar_nombres_columnas(df)\n",
    "\n",
    "        # Detectar problemas antes de limpieza\n",
    "        resumen_antes, chequeos_antes, problemas_antes = detectar_problemas_dataframe(df)\n",
    "\n",
    "        # Guardar reporte 'antes'\n",
    "        ruta_reporte_antes = os.path.join(carpeta_reportes, f\"reporte_antes_limpieza_{nombre_archivo}.csv\")\n",
    "        resumen_antes.to_csv(ruta_reporte_antes.replace('.csv','_resumen.csv'), index=False, encoding='utf-8')\n",
    "        chequeos_antes.to_csv(ruta_reporte_antes.replace('.csv','_chequeos.csv'), index=False, encoding='utf-8')\n",
    "        problemas_antes.to_csv(ruta_reporte_antes.replace('.csv','_problemas.csv'), index=False, encoding='utf-8')\n",
    "        print(\"Reportes 'antes' guardados en:\", carpeta_reportes)\n",
    "\n",
    "        # Aplicar reglas de limpieza si existen\n",
    "        if reglas_columnas:\n",
    "            df_limpio = aplicar_reglas_limpieza(df, reglas_columnas)\n",
    "        else:\n",
    "            df_limpio = df.copy()\n",
    "\n",
    "        # Detectar problemas despues de limpieza\n",
    "        resumen_despues, chequeos_despues, problemas_despues = detectar_problemas_dataframe(df_limpio)\n",
    "\n",
    "        # Guardar reporte 'despues'\n",
    "        ruta_reporte_despues = os.path.join(carpeta_reportes, f\"reporte_despues_limpieza_{nombre_archivo}.csv\")\n",
    "        resumen_despues.to_csv(ruta_reporte_despues.replace('.csv','_resumen.csv'), index=False, encoding='utf-8')\n",
    "        chequeos_despues.to_csv(ruta_reporte_despues.replace('.csv','_chequeos.csv'), index=False, encoding='utf-8')\n",
    "        problemas_despues.to_csv(ruta_reporte_despues.replace('.csv','_problemas.csv'), index=False, encoding='utf-8')\n",
    "        print(\"Reportes 'despues' guardados en:\", carpeta_reportes)\n",
    "\n",
    "        # Guardar CSV limpio\n",
    "        ruta_csv_limpio = os.path.join(carpeta_csv_limpios, f\"{nombre_archivo}_limpio.csv\")\n",
    "        df_limpio.to_csv(ruta_csv_limpio, index=False, encoding='utf-8')\n",
    "        print(\"CSV limpio guardado en:\", ruta_csv_limpios)\n",
    "\n",
    "    # Crear ZIP de reportes\n",
    "    zip_path = os.path.join(ruta_salida, \"reportes_dataset.zip\")\n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "        for root, _, files in os.walk(carpeta_reportes):\n",
    "            for f in files:\n",
    "                zf.write(os.path.join(root, f), arcname=os.path.join(os.path.basename(root), f))\n",
    "    print(\"ZIP de reportes creado en:\", zip_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0aeacc",
   "metadata": {},
   "source": [
    "\n",
    "<h3 style=\"color:#0b61a4;\">Ejecución principal: función <code>menu()</code></h3>\n",
    "<p style=\"color:#0b61a4;\">La función <code>menu()</code> prepara archivos de ejemplo si no existen y ejecuta <code>procesar_carpeta</code>.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99c62682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando pipeline de limpieza de datos...\n",
      "Directorio base: D:\\Desktop\\Domingo\\df_caba_y_jupyter\n",
      "Ruta entrada: D:\\Desktop\\Domingo\\df_caba_y_jupyter\\datasets_entrada\n",
      "Ruta salida: D:\\Desktop\\Domingo\\df_caba_y_jupyter\\datasets_salida\n",
      "\n",
      "--- PREVIEW: clientes2.csv ---\n",
      "   id_cliente               nombre  edad         ciudad  ingresos\n",
      "0           1      Aloysia Screase    44  Mar del Plata  42294.68\n",
      "1           2  Kristina Scaplehorn    25        Posadas  24735.04\n",
      "2           3       Filip Castagne    50    Resistencia  35744.85\n",
      "Dimensiones: (567, 5)\n",
      "\n",
      "--- PREVIEW: marketing2.csv ---\n",
      "   id_campanha         producto  canal  costo fecha_inicio   fecha_fin\n",
      "0           74  Adorno de pared     TV   4.81   20/03/2024  03/05/2024\n",
      "1           12           Tablet   RRSS   3.40   26/03/2024  13/05/2024\n",
      "2           32  Lámpara de mesa  Email   5.54   28/03/2024  20/04/2024\n",
      "Dimensiones: (90, 6)\n",
      "\n",
      "--- PREVIEW: ventas2.csv ---\n",
      "   id_venta           producto   precio  cantidad fecha_venta  \\\n",
      "0       792  Cuadro decorativo   $69.94       5.0  02/01/2024   \n",
      "1       811    Lámpara de mesa  $105.10       5.0  02/01/2024   \n",
      "2      1156           Secadora   $97.96       3.0  02/01/2024   \n",
      "\n",
      "           categoria  \n",
      "0         Decoración  \n",
      "1         Decoración  \n",
      "2  Electrodomésticos  \n",
      "Dimensiones: (3035, 6)\n",
      "\n",
      "Procesando: D:\\Desktop\\Domingo\\df_caba_y_jupyter\\datasets_entrada\\clientes2.csv\n",
      "Reportes 'antes' guardados en: D:\\Desktop\\Domingo\\df_caba_y_jupyter\\datasets_salida\\reportes\n",
      "Aviso: la columna 'producto' no existe en el DataFrame. Se omite.\n",
      "Aviso: la columna 'categoria' no existe en el DataFrame. Se omite.\n",
      "Aviso: la columna 'canal' no existe en el DataFrame. Se omite.\n",
      "Aviso: la columna 'precio' no existe en el DataFrame. Se omite.\n",
      "Aviso: la columna 'cantidad' no existe en el DataFrame. Se omite.\n",
      "Aviso: la columna 'costo' no existe en el DataFrame. Se omite.\n",
      "Aviso: la columna 'fecha_venta' no existe en el DataFrame. Se omite.\n",
      "Aviso: la columna 'fecha_inicio' no existe en el DataFrame. Se omite.\n",
      "Aviso: la columna 'fecha_fin' no existe en el DataFrame. Se omite.\n",
      "Reportes 'despues' guardados en: D:\\Desktop\\Domingo\\df_caba_y_jupyter\\datasets_salida\\reportes\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ruta_csv_limpios' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 79\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPipeline completado.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# Ejecutar menu\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m \u001b[43mmenu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mmenu\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     59\u001b[39m reglas_columnas = {\n\u001b[32m     60\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mnombre\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     61\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mciudad\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     71\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mfecha_fin\u001b[39m\u001b[33m'\u001b[39m: (\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m, {\u001b[33m'\u001b[39m\u001b[33mformats\u001b[39m\u001b[33m'\u001b[39m: [\u001b[33m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m/\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm/\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY\u001b[39m\u001b[33m'\u001b[39m], \u001b[33m'\u001b[39m\u001b[33mdayfirst\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m})\n\u001b[32m     72\u001b[39m }\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Ejecutar procesamiento\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[43mprocesar_carpeta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mruta_entrada\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mruta_salida\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreglas_columnas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreglas_columnas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPipeline completado.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mprocesar_carpeta\u001b[39m\u001b[34m(ruta_entrada, ruta_salida, reglas_columnas)\u001b[39m\n\u001b[32m     65\u001b[39m     ruta_csv_limpio = os.path.join(carpeta_csv_limpios, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnombre_archivo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_limpio.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     66\u001b[39m     df_limpio.to_csv(ruta_csv_limpio, index=\u001b[38;5;28;01mFalse\u001b[39;00m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCSV limpio guardado en:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mruta_csv_limpios\u001b[49m)\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# Crear ZIP de reportes\u001b[39;00m\n\u001b[32m     70\u001b[39m zip_path = os.path.join(ruta_salida, \u001b[33m\"\u001b[39m\u001b[33mreportes_dataset.zip\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'ruta_csv_limpios' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def crear_datasets_ejemplo(ruta_entrada: str):\n",
    "    \"\"\"\n",
    "    Crea 3 CSV de ejemplo si no existen en la carpeta de entrada.\n",
    "    Estos archivos representan: ventas, clientes y marketing.\n",
    "    \"\"\"\n",
    "    # Ejemplo simple de ventas\n",
    "    df_ventas = pd.DataFrame({\n",
    "        'id_venta': [1,2,3,3],\n",
    "        'fecha_venta': ['01/01/2021','02/01/2021','03/01/2021', '03/01/2021'],\n",
    "        'producto': ['camisa','pantalon','camisa','camisa'],\n",
    "        'cantidad': ['1','2','1','1'],\n",
    "        'precio': ['100.0','200.0','100','100']\n",
    "    })\n",
    "    df_clientes = pd.DataFrame({\n",
    "        'id_cliente': [10,11,12],\n",
    "        'nombre': ['Juan Pérez','María López','Carlos Gómez'],\n",
    "        'ciudad': ['Buenos Aires','Cordoba','Rosario']\n",
    "    })\n",
    "    df_marketing = pd.DataFrame({\n",
    "        'campana': ['fb_enero','google_febrero'],\n",
    "        'gasto': ['1000','1500'],\n",
    "        'canal': ['Facebook','Google']\n",
    "    })\n",
    "    # Guardar si no existen\n",
    "    df_ventas.to_csv(os.path.join(ruta_entrada, \"ventas.csv\"), index=False, encoding='utf-8')\n",
    "    df_clientes.to_csv(os.path.join(ruta_entrada, \"clientes.csv\"), index=False, encoding='utf-8')\n",
    "    df_marketing.to_csv(os.path.join(ruta_entrada, \"marketing.csv\"), index=False, encoding='utf-8')\n",
    "    print(\"Datasets de ejemplo creados en:\", ruta_entrada)\n",
    "\n",
    "def menu():\n",
    "    \"\"\"\n",
    "    Función principal de ejecución del pipeline de limpieza.\n",
    "\n",
    "    OBJETIVO EDUCATIVO:\n",
    "    Muestra cómo integrar todas las piezas en un flujo automatizado,\n",
    "    desde la carga de datos hasta la generación de reportes finales.\n",
    "    \"\"\"\n",
    "    print(\"Iniciando pipeline de limpieza de datos...\")\n",
    "    print(f\"Directorio base: {ruta_base}\")\n",
    "    print(f\"Ruta entrada: {ruta_entrada}\")\n",
    "    print(f\"Ruta salida: {ruta_salida}\")\n",
    "\n",
    "    # Si no hay CSV, crear ejemplos\n",
    "    csvs = [f for f in os.listdir(ruta_entrada) if f.lower().endswith('.csv')]\n",
    "    if not csvs:\n",
    "        crear_datasets_ejemplo(ruta_entrada)\n",
    "\n",
    "    # Mostrar preview de los datos originales (solo primeros archivos)\n",
    "    archivos = [f for f in os.listdir(ruta_entrada) if f.lower().endswith('.csv')]\n",
    "    for archivo in archivos:\n",
    "        ruta = os.path.join(ruta_entrada, archivo)\n",
    "        df_temp = pd.read_csv(ruta, encoding='utf-8', low_memory=False)\n",
    "        df_temp = normalizar_nombres_columnas(df_temp)\n",
    "        print(\"\\n--- PREVIEW:\", archivo, \"---\")\n",
    "        print(df_temp.head(3))\n",
    "        print(\"Dimensiones:\", df_temp.shape)\n",
    "\n",
    "    # Definir reglas de limpieza de ejemplo (puede personalizarse)\n",
    "    reglas_columnas = {\n",
    "        'nombre': 'title',\n",
    "        'ciudad': 'title',\n",
    "        'producto': 'title',\n",
    "        'categoria': 'title',\n",
    "        'canal': 'lower',\n",
    "        'precio': ('numeric', {'remove_non_digits': False}),\n",
    "        'cantidad': ('numeric', {'as_int': True}),\n",
    "        'ingresos': ('numeric', {}),\n",
    "        'costo': ('numeric', {}),\n",
    "        'fecha_venta': ('date', {'formats': ['%d/%m/%Y'], 'dayfirst': True}),\n",
    "        'fecha_inicio': ('date', {'formats': ['%d/%m/%Y'], 'dayfirst': True}),\n",
    "        'fecha_fin': ('date', {'formats': ['%d/%m/%Y'], 'dayfirst': True})\n",
    "    }\n",
    "\n",
    "    # Ejecutar procesamiento\n",
    "    procesar_carpeta(ruta_entrada, ruta_salida, reglas_columnas=reglas_columnas)\n",
    "    print(\"Pipeline completado.\")\n",
    "\n",
    "# Ejecutar menu\n",
    "menu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d6f6e7",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"color:#0b61a4;\">El notebook ha sido generado con funciones completas. Si deseas, puedes descargar el archivo .ipynb generado en /mnt/data al ejecutar la celda que crea y guarda el notebook.</p>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
