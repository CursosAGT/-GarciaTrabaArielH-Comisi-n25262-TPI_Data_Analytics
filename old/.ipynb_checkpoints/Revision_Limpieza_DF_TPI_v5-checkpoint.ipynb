{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aacc3485",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color:#CCCCCC; padding:12px; border-radius:8px;\">\n",
    "<h1 style=\"color:#003366; text-align:center; margin:8px 0;\">Revisión y limpieza completa de 3 DataFrames</h1>\n",
    "<p style=\"text-align:center; color:#003366; margin:0;\"><em>Notebook docente en castellano — funciones en snake_case — flujo: leer → revisar → limpiar → salvar</em></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207566e3",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color:#CCCCCC; padding:8px; border-radius:6px;\">\n",
    "<h2 style=\"color:black; text-align:center;\">Resumen y objetivos</h2>\n",
    "<ul style=\"color:black;\">\n",
    "<li>Leer los 3 CSV: <code>marketing.csv</code>, <code>ventas.csv</code>, <code>clientes.csv</code> (busca en <code>./data_in/</code> y en <code>/mnt/data/</code>).</li>\n",
    "<li>Aplicar limpieza automática y parametrizable: quitar acentos, normalizar mayúsculas/minúsculas, convertir numéricos, parsear fechas, eliminar duplicados, marcar y guardar reportes antes/después.</li>\n",
    "<li>Guardar los datasets limpios en <code>./reportes/limpios/</code> y empaquetar reportes.</li>\n",
    "</ul>\n",
    "<p style=\"color:black;\">Pega y ejecuta las celdas en orden. Las funciones están documentadas y listas para usar en clases o trabajos prácticos.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d544d19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports y configuración de rutas\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import unicodedata\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Rutas: prioriza ./data_in, luego /mnt/data\n",
    "carpeta_entrada_local = Path('./data_in')\n",
    "carpeta_entrada_mnt = Path('/mnt/data')\n",
    "carpeta_reportes = Path('./reportes')\n",
    "carpeta_limpios = carpeta_reportes / 'limpios'\n",
    "carpeta_reportes.mkdir(parents=True, exist_ok=True)\n",
    "carpeta_limpios.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Nombres esperados\n",
    "archivo_marketing = 'marketing.csv'\n",
    "archivo_ventas = 'ventas.csv'\n",
    "archivo_clientes = 'clientes.csv'\n",
    "\n",
    "def ruta_entrada(nombre_archivo):\n",
    "    p_local = carpeta_entrada_local / nombre_archivo\n",
    "    if p_local.exists():\n",
    "        return p_local\n",
    "    p_mnt = carpeta_entrada_mnt / nombre_archivo\n",
    "    if p_mnt.exists():\n",
    "        return p_mnt\n",
    "    return None\n",
    "\n",
    "print('Rutas configuradas. Comprueba ./data_in/ y /mnt/data/ para los CSV.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2838330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Utilidades básicas ----------\n",
    "def sacar_acentos(texto):\n",
    "    \"\"\"Elimina acentos (tildes) y diacríticos de un texto. Mantiene NaN intactos.\"\"\"\n",
    "    if pd.isna(texto):\n",
    "        return texto\n",
    "    texto = str(texto)\n",
    "    nk = unicodedata.normalize('NFKD', texto)\n",
    "    return ''.join([c for c in nk if not unicodedata.combining(c)])\n",
    "\n",
    "TOKENS_VALOR_FALTANTE = {\n",
    "    '', 'na', 'n/a', 'null', 'none', 'sin dato', 's/d', 'nd', '-', '--', '?', 'sin_dato', 'n/d', 'n.d.'\n",
    "}\n",
    "\n",
    "def es_valor_faltante(valor):\n",
    "    \"\"\"Devuelve True si el valor representa un faltante (NaN) según tokens o es NaN.\"\"\"\n",
    "    if pd.isna(valor):\n",
    "        return True\n",
    "    s = str(valor).strip().lower()\n",
    "    s = sacar_acentos(s)\n",
    "    return s in TOKENS_VALOR_FALTANTE\n",
    "\n",
    "def guardar_csv(df, ruta):\n",
    "    \"\"\"Guarda un DataFrame en ruta (Path o str). Crea directorio padre si hace falta.\"\"\"\n",
    "    ruta = Path(ruta)\n",
    "    ruta.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(ruta, index=False, encoding='utf-8')\n",
    "    return ruta\n",
    "\n",
    "def mostrar_resumen_pequeno(df, n=5):\n",
    "    \"\"\"Muestra un pequeño resumen para inspección rápida.\"\"\"\n",
    "    print('Dimensiones:', df.shape)\n",
    "    display(df.head(n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726fa4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Detección de outliers ----------\n",
    "def mascara_valores_atipicos_rango_intercuartil(serie_datos):\n",
    "    \"\"\"Devuelve máscara booleana (True = outlier) según IQR.\"\"\"\n",
    "    serie_limpia = serie_datos.dropna().astype(float)\n",
    "    if serie_limpia.shape[0] < 4:\n",
    "        return pd.Series([False] * len(serie_datos), index=serie_datos.index)\n",
    "    q1 = serie_limpia.quantile(0.25)\n",
    "    q3 = serie_limpia.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    limite_inferior = q1 - 1.5 * iqr\n",
    "    limite_superior = q3 + 1.5 * iqr\n",
    "    return (serie_datos < limite_inferior) | (serie_datos > limite_superior)\n",
    "\n",
    "def mascara_valores_atipicos_zscore(serie_datos, umbral=3.0):\n",
    "    \"\"\"Devuelve máscara booleana (True = outlier) según Z-score.\"\"\"\n",
    "    serie_limpia = serie_datos.dropna().astype(float)\n",
    "    if serie_limpia.shape[0] < 4 or serie_limpia.std() == 0:\n",
    "        return pd.Series([False] * len(serie_datos), index=serie_datos.index)\n",
    "    puntaje_z = (serie_datos - serie_limpia.mean()) / serie_limpia.std()\n",
    "    return puntaje_z.abs() > umbral\n",
    "\n",
    "print('Funciones de outliers cargadas.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0df157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Detección de problemas en DataFrame ----------\n",
    "def detectar_problemas_en_dataframe(df: pd.DataFrame):\n",
    "    \"\"\"Detecta nulos, duplicados, problemas textuales, outliers y parseabilidad de fechas.\"\"\"\n",
    "    resumen = {}\n",
    "    resumen['filas'] = df.shape[0]\n",
    "    resumen['columnas'] = df.shape[1]\n",
    "    resumen['nulos_por_columna'] = df.isna().sum().to_dict()\n",
    "    dup_mask = df.duplicated(keep=False)\n",
    "    resumen['duplicados_exactos'] = int(dup_mask.sum())\n",
    "\n",
    "    chequeos_por_columna = {}\n",
    "    for col in df.columns:\n",
    "        ser = df[col]\n",
    "        info = {'dtype': str(ser.dtype), 'nulos': int(ser.isna().sum())}\n",
    "        if ser.dtype == object or pd.api.types.is_string_dtype(ser):\n",
    "            s = ser.astype(str)\n",
    "            info['espacios_inicio'] = int(s.str.match(r'^\\s+').sum())\n",
    "            info['espacios_final'] = int(s.str.match(r'\\s+$').sum())\n",
    "            try:\n",
    "                unique_original = set(s.dropna().unique())\n",
    "                unique_lower = set(s.dropna().str.lower().unique())\n",
    "                info['unique_original'] = len(unique_original)\n",
    "                info['unique_lower'] = len(unique_lower)\n",
    "                info['variantes_mayusculas'] = len(unique_lower) < len(unique_original)\n",
    "            except Exception:\n",
    "                info['unique_original'] = ser.nunique(dropna=True)\n",
    "                info['unique_lower'] = None\n",
    "                info['variantes_mayusculas'] = None\n",
    "            try:\n",
    "                unaccented = s.dropna().map(lambda x: sacar_acentos(x).lower())\n",
    "                groups = unaccented.groupby(unaccented).size()\n",
    "                conflicts = groups[groups > 1]\n",
    "                info['grupos_variantes_acentos'] = int(conflicts.shape[0])\n",
    "                ejemplos = {}\n",
    "                if not conflicts.empty:\n",
    "                    for val in conflicts.index[:5]:\n",
    "                        originales = sorted(list(s[unaccented == val].unique())[:10])\n",
    "                        ejemplos[val] = originales\n",
    "                info['ejemplos_variantes_acentos'] = ejemplos\n",
    "            except Exception:\n",
    "                info['grupos_variantes_acentos'] = None\n",
    "                info['ejemplos_variantes_acentos'] = {}\n",
    "            info['tokens_aparente_faltante'] = int(s.map(lambda x: str(x).strip().lower()).map(lambda v: sacar_acentos(v) in TOKENS_VALOR_FALTANTE).sum())\n",
    "            info['muestras'] = list(s.dropna().unique()[:10])\n",
    "        else:\n",
    "            if pd.api.types.is_numeric_dtype(ser):\n",
    "                s_f = ser.dropna().astype(float)\n",
    "                info['media'] = float(s_f.mean()) if not s_f.empty else None\n",
    "                info['std'] = float(s_f.std()) if not s_f.empty else None\n",
    "                info['min'] = float(s_f.min()) if not s_f.empty else None\n",
    "                info['max'] = float(s_f.max()) if not s_f.empty else None\n",
    "                if len(s_f) >= 4:\n",
    "                    q1 = s_f.quantile(0.25)\n",
    "                    q3 = s_f.quantile(0.75)\n",
    "                    iqr = q3 - q1\n",
    "                    lb = q1 - 1.5 * iqr\n",
    "                    ub = q3 + 1.5 * iqr\n",
    "                    out_iqr = (s_f < lb) | (s_f > ub)\n",
    "                    info['outliers_iqr'] = int(out_iqr.sum())\n",
    "                    info['limites_iqr'] = (float(lb), float(ub))\n",
    "                else:\n",
    "                    info['outliers_iqr'] = None\n",
    "                    info['limites_iqr'] = None\n",
    "                if len(s_f) >= 4 and s_f.std() != 0:\n",
    "                    z = (s_f - s_f.mean()) / s_f.std()\n",
    "                    info['outliers_z'] = int((z.abs() > 3).sum())\n",
    "                else:\n",
    "                    info['outliers_z'] = None\n",
    "            else:\n",
    "                parsed = pd.to_datetime(ser, errors='coerce', dayfirst=True)\n",
    "                info['fechas_parseables'] = int(parsed.notna().sum())\n",
    "                info['muestras'] = list(ser.dropna().unique()[:10])\n",
    "        chequeos_por_columna[col] = info\n",
    "\n",
    "    filas_problemas = []\n",
    "    for idx, fila in df.iterrows():\n",
    "        lista_problemas = []\n",
    "        if dup_mask.loc[idx]:\n",
    "            lista_problemas.append('duplicado_exacto')\n",
    "        for col in df.columns:\n",
    "            val = fila[col]\n",
    "            # heurísticas textuales\n",
    "            if pd.api.types.is_string_dtype(type(val)) or isinstance(val, str) or (not pd.isna(val) and not pd.api.types.is_numeric_dtype(type(val)) and str(chequeos_por_columna[col].get('dtype','')).startswith('object')):\n",
    "                s = str(val)\n",
    "                if s != s.strip():\n",
    "                    lista_problemas.append(f'espacios_en_columna_{col}')\n",
    "                if chequeos_por_columna[col].get('variantes_mayusculas'):\n",
    "                    if s and s != s.lower() and s.lower() in [str(x).lower() for x in df[col].dropna().unique()]:\n",
    "                        lista_problemas.append(f'inconsistencia_mayusculas_columna_{col}')\n",
    "                if chequeos_por_columna[col].get('grupos_variantes_acentos') and chequeos_por_columna[col]['grupos_variantes_acentos'] > 0:\n",
    "                    try:\n",
    "                        un = sacar_acentos(s).lower()\n",
    "                        group_vals = [x for x in chequeos_por_columna[col].get('muestras', []) if sacar_acentos(str(x)).lower() == un]\n",
    "                        if group_vals and any(sacar_acentos(str(x)).lower() != sacar_acentos(s).lower() for x in group_vals):\n",
    "                            lista_problemas.append(f'variantes_acentos_columna_{col}')\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                if es_valor_faltante(s):\n",
    "                    lista_problemas.append(f'token_faltante_columna_{col}')\n",
    "            else:\n",
    "                # heurísticas numéricas\n",
    "                try:\n",
    "                    fval = float(val)\n",
    "                    info_col = chequeos_por_columna[col]\n",
    "                    limites = info_col.get('limites_iqr')\n",
    "                    if limites and (fval < limites[0] or fval > limites[1]):\n",
    "                        lista_problemas.append(f'outlier_iqr_columna_{col}')\n",
    "                    if info_col.get('std') not in (None, 0):\n",
    "                        mean = info_col.get('media')\n",
    "                        std = info_col.get('std')\n",
    "                        if std and abs((fval - mean) / std) > 3:\n",
    "                            lista_problemas.append(f'outlier_z_columna_{col}')\n",
    "                except Exception:\n",
    "                    pass\n",
    "        if lista_problemas:\n",
    "            filas_problemas.append({'row_index': idx, 'problemas': ';'.join(sorted(set(lista_problemas))), 'muestra': json.dumps({str(c): str(fila[c]) for c in df.columns[:8]})})\n",
    "    df_problemas = pd.DataFrame(filas_problemas)\n",
    "    return resumen, chequeos_por_columna, df_problemas\n",
    "\n",
    "print('Función detectar_problemas_en_dataframe cargada.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ebfe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Aplicar reglas por columna y limpieza principal ----------\n",
    "def aplicar_regla_columna(serie, regla):\n",
    "    \"\"\"Aplica regla a la serie. Mantiene firma esperada.\"\"\"\n",
    "    tipo, opts = regla if isinstance(regla, tuple) else (regla, {})\n",
    "    opts = opts or {}\n",
    "    s = serie.copy()\n",
    "    if tipo == 'strip':\n",
    "        s = s.map(lambda x: str(x).strip() if not pd.isna(x) else x)\n",
    "    elif tipo == 'lower':\n",
    "        s = s.map(lambda x: str(x).strip().lower() if not pd.isna(x) else x)\n",
    "        if opts.get('normalizar_acentos'):\n",
    "            s = s.map(lambda x: sacar_acentos(x) if not pd.isna(x) else x)\n",
    "    elif tipo == 'upper':\n",
    "        s = s.map(lambda x: str(x).strip().upper() if not pd.isna(x) else x)\n",
    "        if opts.get('normalizar_acentos'):\n",
    "            s = s.map(lambda x: sacar_acentos(x) if not pd.isna(x) else x)\n",
    "    elif tipo == 'title':\n",
    "        s = s.map(lambda x: str(x).strip().title() if not pd.isna(x) else x)\n",
    "        if opts.get('normalizar_acentos'):\n",
    "            s = s.map(lambda x: sacar_acentos(x) if not pd.isna(x) else x)\n",
    "    elif tipo == 'quitar_acentos':\n",
    "        s = s.map(lambda x: sacar_acentos(x).strip() if not pd.isna(x) else x)\n",
    "    elif tipo == 'numeric':\n",
    "        def to_num(v):\n",
    "            if pd.isna(v):\n",
    "                return np.nan\n",
    "            t = str(v).strip()\n",
    "            if opts.get('remove_non_digits', False):\n",
    "                t = ''.join([c for c in t if c.isdigit() or c in '.-'])\n",
    "            if opts.get('remove_thousands', False):\n",
    "                sep = opts.get('thousands_separator', ',')\n",
    "                if sep == ',':\n",
    "                    t = t.replace('.', '').replace(',', '')\n",
    "                else:\n",
    "                    t = t.replace(',', '').replace('.', '')\n",
    "            try:\n",
    "                val = pd.to_numeric(t, errors='coerce')\n",
    "                if opts.get('as_int', False):\n",
    "                    if pd.isna(val):\n",
    "                        return pd.NA\n",
    "                    try:\n",
    "                        return int(val)\n",
    "                    except Exception:\n",
    "                        return pd.NA\n",
    "                return float(val) if not pd.isna(val) else np.nan\n",
    "            except Exception:\n",
    "                return np.nan\n",
    "        s = s.map(to_num)\n",
    "    elif tipo == 'date':\n",
    "        formatos = opts.get('formats', [])\n",
    "        dayfirst = opts.get('dayfirst', True)\n",
    "        def to_date(v):\n",
    "            if pd.isna(v):\n",
    "                return pd.NaT\n",
    "            t = str(v).strip()\n",
    "            for fmt in formatos:\n",
    "                try:\n",
    "                    return pd.to_datetime(datetime.strptime(t, fmt))\n",
    "                except Exception:\n",
    "                    continue\n",
    "            try:\n",
    "                return pd.to_datetime(t, dayfirst=dayfirst, errors='coerce')\n",
    "            except Exception:\n",
    "                return pd.NaT\n",
    "        s = s.map(to_date)\n",
    "    else:\n",
    "        if s.dtype == object or pd.api.types.is_string_dtype(s):\n",
    "            s = s.map(lambda x: np.nan if es_valor_faltante(x) else (str(x).strip() if not pd.isna(x) else x))\n",
    "    return s\n",
    "\n",
    "def limpiar_dataframe(df, reglas_por_columna=None):\n",
    "    \"\"\"Limpieza principal: aplica reglas, trim por defecto, elimina duplicados y postprocesa tipos.\"\"\"\n",
    "    reglas_por_columna = reglas_por_columna or {}\n",
    "    df2 = df.copy()\n",
    "    df2.columns = [str(c).strip() for c in df2.columns]\n",
    "    for col in df2.columns:\n",
    "        if col in reglas_por_columna:\n",
    "            df2[col] = aplicar_regla_columna(df2[col], reglas_por_columna[col])\n",
    "        else:\n",
    "            if df2[col].dtype == object or pd.api.types.is_string_dtype(df2[col]):\n",
    "                df2[col] = df2[col].map(lambda x: np.nan if es_valor_faltante(x) else (str(x).strip() if not pd.isna(x) else x))\n",
    "    df2 = df2.drop_duplicates(keep='first').reset_index(drop=True)\n",
    "    # post-conversion simple\n",
    "    for col, regla in (reglas_por_columna or {}).items():\n",
    "        tipo = regla if not isinstance(regla, tuple) else regla[0]\n",
    "        opts = {} if not isinstance(regla, tuple) else regla[1] or {}\n",
    "        if tipo == 'numeric':\n",
    "            df2[col] = pd.to_numeric(df2[col], errors='coerce')\n",
    "            if opts.get('as_int', False):\n",
    "                try:\n",
    "                    df2[col] = df2[col].astype('Int64')\n",
    "                except Exception:\n",
    "                    pass\n",
    "        elif tipo == 'date':\n",
    "            formatos = opts.get('formats', [])\n",
    "            dayfirst = opts.get('dayfirst', True)\n",
    "            parsed = pd.Series(pd.NaT, index=df2.index)\n",
    "            for fmt in formatos:\n",
    "                try:\n",
    "                    mask = parsed.isna()\n",
    "                    parsed.loc[mask] = pd.to_datetime(df2.loc[mask, col].astype(str), format=fmt, errors='coerce')\n",
    "                except Exception:\n",
    "                    pass\n",
    "            still_na = parsed.isna()\n",
    "            if still_na.any():\n",
    "                parsed.loc[still_na] = pd.to_datetime(df2.loc[still_na, col].astype(str), dayfirst=dayfirst, errors='coerce')\n",
    "            if opts.get('format_output') == 'YYYY/MM/DD':\n",
    "                df2[col] = parsed.dt.strftime('%Y/%m/%d')\n",
    "            else:\n",
    "                df2[col] = parsed\n",
    "    return df2\n",
    "\n",
    "print('Funciones de limpieza cargadas.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fe7f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Bucle/menu principal para procesar diccionario de DataFrames ----------\n",
    "def menu_procesar_diccionario(dic_frames, reglas_por_archivo):\n",
    "    \"\"\"Recorre dic_frames (nombre_archivo -> DataFrame), detecta problemas antes/después, limpia y guarda.\"\"\"\n",
    "    for nombre_archivo, df_actual in dic_frames.items():\n",
    "        resumen_antes, chequeos_antes, problemas_antes = detectar_problemas_en_dataframe(df_actual)\n",
    "        print(f\"\\n=== RESUMEN ANTES: {nombre_archivo} ===\\n\")\n",
    "        print(resumen_antes)\n",
    "        print('\\nMuestras problemas antes:') \n",
    "        display(problemas_antes.head(5) if not problemas_antes.empty else 'No se detectaron filas con problemas.')\n",
    "\n",
    "        reglas = reglas_por_archivo.get(nombre_archivo, {})\n",
    "        df_limpio = limpiar_dataframe(df_actual, reglas_por_columna=reglas)\n",
    "\n",
    "        resumen_despues, chequeos_despues, problemas_despues = detectar_problemas_en_dataframe(df_limpio)\n",
    "        print(f\"\\n=== RESUMEN DESPUÉS: {nombre_archivo} ===\\n\")\n",
    "        print(resumen_despues)\n",
    "        print('\\nMuestras problemas después:')\n",
    "        display(problemas_despues.head(5) if not problemas_despues.empty else 'No se detectaron filas con problemas tras la limpieza.')\n",
    "\n",
    "        print('\\n' + '-'*100)\n",
    "        print(f'nombre_archivo = {nombre_archivo}')\n",
    "        print(f'type(nombre_archivo) = {type(nombre_archivo)}')\n",
    "        print('-'*100 + '\\n')\n",
    "\n",
    "        ruta_nombre_limpio = nombre_archivo[:-4] + ' limpio.csv' if nombre_archivo.lower().endswith('.csv') else nombre_archivo + ' limpio.csv'\n",
    "        ruta_guardado = carpeta_limpios / ruta_nombre_limpio\n",
    "        guardar_csv(df_limpio, ruta_guardado)\n",
    "        print(f'Guardado cleaned en: {ruta_guardado}')\n",
    "\n",
    "        # sobrescribir variables globales por conveniencia\n",
    "        if 'marketing' in nombre_archivo.lower():\n",
    "            globals()['df_marketing'] = df_limpio\n",
    "        elif 'ventas' in nombre_archivo.lower():\n",
    "            globals()['df_ventas'] = df_limpio\n",
    "        elif 'clientes' in nombre_archivo.lower():\n",
    "            globals()['df_clientes'] = df_limpio\n",
    "\n",
    "    print('\\nProceso completo del diccionario de DataFrames.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76a2036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Funciones adicionales útiles (reportes, empaquetado) ----------\n",
    "def generar_resumen_global(reportes_creados, zip_salida=None):\n",
    "    \"\"\"Empaqueta los reportes listados en reportes_creados en un ZIP (opcional ruta).\"\"\"\n",
    "    zip_salida = zip_salida or (carpeta_reportes / 'reports_dataset.zip')\n",
    "    with zipfile.ZipFile(zip_salida, 'w', compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "        for p in reportes_creados:\n",
    "            p = Path(p)\n",
    "            if p.exists():\n",
    "                zf.write(p, arcname=os.path.join('reports', p.name))\n",
    "        for p in carpeta_limpios.glob('cleaned_*.csv'):\n",
    "            zf.write(p, arcname=os.path.join('limpios', p.name))\n",
    "    return zip_salida\n",
    "\n",
    "def crear_diccionario_frames():\n",
    "    \"\"\"Lee los 3 CSV esperados y devuelve un diccionario nombre->DataFrame (solo los que existen).\"\"\"\n",
    "    dic = {}\n",
    "    for nombre in [archivo_marketing, archivo_ventas, archivo_clientes]:\n",
    "        r = ruta_entrada(nombre)\n",
    "        if r is not None:\n",
    "            try:\n",
    "                df = pd.read_csv(r, dtype=str, keep_default_na=False, na_values=[''])\n",
    "            except Exception:\n",
    "                df = pd.read_csv(r, encoding='latin1', dtype=str, keep_default_na=False, na_values=[''])\n",
    "            dic[nombre] = df\n",
    "        else:\n",
    "            print(f'ATENCION: no se encontró {nombre} en ./data_in/ ni en /mnt/data/.')\n",
    "    return dic\n",
    "\n",
    "print('Funciones extras cargadas.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f16115f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Ejemplo de uso (ejecuta esta celda para procesar los 3 CSV) ----------\n",
    "reglas_ejemplo_marketing = {\n",
    "    'nombre': ('title', {'normalizar_acentos': True}),\n",
    "    'email': ('lower', {}),\n",
    "    'fecha_registro': ('date', {'dayfirst': True, 'formats': ['%d/%m/%Y', '%Y-%m-%d'], 'format_output': 'YYYY/MM/DD'})\n",
    "}\n",
    "reglas_ejemplo_ventas = {\n",
    "    'monto': ('numeric', {'remove_thousands': True, 'as_int': False}),\n",
    "    'fecha_venta': ('date', {'dayfirst': True, 'formats': ['%d/%m/%Y', '%Y-%m-%d'], 'format_output': 'YYYY/MM/DD'})\n",
    "}\n",
    "reglas_ejemplo_clientes = {\n",
    "    'apellido': ('title', {'normalizar_acentos': True}),\n",
    "    'dni': ('numeric', {'remove_non_digits': True, 'as_int': True})\n",
    "}\n",
    "reglas_por_archivo = {\n",
    "    archivo_marketing: reglas_ejemplo_marketing,\n",
    "    archivo_ventas: reglas_ejemplo_ventas,\n",
    "    archivo_clientes: reglas_ejemplo_clientes\n",
    "}\n",
    "\n",
    "dic_frames = crear_diccionario_frames()\n",
    "print('\\nDataFrames leídos: ', list(dic_frames.keys()))\n",
    "\n",
    "menu_procesar_diccionario(dic_frames, reglas_por_archivo)\n",
    "\n",
    "zip_generado = generar_resumen_global([])\n",
    "print('ZIP generado (si corresponde):', zip_generado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a25c03",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color:#CCCCCC; padding:10px; border-radius:6px;\">\n",
    "<h2 style=\"color:black; text-align:center;\">Notas finales</h2>\n",
    "<p style=\"color:black;\">Si alguna columna no quedó con el tipo deseado, ajustá <code>reglas_por_archivo</code> y volvé a ejecutar la celda de ejemplo.</p>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
