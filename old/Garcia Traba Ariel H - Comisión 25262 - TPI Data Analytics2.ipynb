{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cedbb1b4",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color:#CCCCCC; padding:12px; border-radius:8px;\">\n",
    "<h1 style=\"color:#003366; text-align:center; margin:8px 0;\">Revisión y limpieza de 3 DataFrames (TPI - Data Analytics)</h1>\n",
    "<p style=\"text-align:center; color:#003366; margin:0;\"><em>Notebook docente en castellano — nombres descriptivos en snake_case — código y documentación</em></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c723b86a",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color:#CCCCCC; padding:10px; border-radius:6px;\">\n",
    "<h2 style=\"color:black; text-align:center; margin-top:6px;\">Resumen</h2>\n",
    "\n",
    "<p style=\"color:black;\">\n",
    "Este notebook está diseñado con finalidades pedagógicas. Revisa, normaliza y valida tres datasets contenidos en CSV:\n",
    "</p>\n",
    "\n",
    "<ul style=\"color:black;\">\n",
    "<li><code>marketing.csv</code> → variable: <code>df_marketing</code></li>\n",
    "<li><code>ventas.csv</code>    → variable: <code>df_ventas</code></li>\n",
    "<li><code>clientes.csv</code>  → variable: <code>df_clientes</code></li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"color:black;\">\n",
    "Coloca los CSV en <code>./data_in/</code> o en <code>/mnt/data/</code>. El notebook busca primero en <code>./data_in/</code> y si no encuentra, usa <code>/mnt/data/</code> (útil para entornos donde los archivos están pre-subidos).\n",
    "</p>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be46fb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports y configuración inicial (nombres en castellano)\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import unicodedata\n",
    "from colorama import *\n",
    "import re\n",
    "#!pip install gdown\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Any, Optional\n",
    "import zipfile\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from math import isnan\n",
    "ruta_base = \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63edff5b-828e-4254-a4d7-4b0ef05ecea6",
   "metadata": {},
   "source": [
    "## 1. Crear un documento en Google Colaboratory y cargar los sets de datos como DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfa535f-ef52-4f49-9d1b-e70dd8d5ad27",
   "metadata": {},
   "source": [
    "si se usa en disco local comentarla celda de debajo (JuPyteR , VSC, ATOM, Spider, Geany, etc)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "00de9cf8-da0d-4d31-bed7-31a704a7274d",
   "metadata": {},
   "source": [
    "# --- Paso 1: Montar Google Drive ---\n",
    "# Montar tu Google Drive\n",
    "!pip install google\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!ls \"/content/drive/MyDrive/CABA/Garcia Traba Ariel H - Comisión 25262 - TPI Data Analytics/\"\n",
    "# Ruta del archivo (ajústala a la carpeta real en tu Drive)\n",
    "ruta_base = \"/content/drive/MyDrive/CABA/Garcia Traba Ariel H - Comisión 25262 - TPI Data Analytics/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144444b3-1dc1-4f3f-a621-025d1cdb6465",
   "metadata": {},
   "source": [
    "# 1ra parte Definición de ETL\n",
    "ETL es un conjunto de procedimientos que permiten mover datos desde sistemas de origen, que pueden ser bases de datos, archivos o fuentes en la nube, hasta un sistema de destino como un data warehouse o data lake, realizando previamente procesos de limpieza, estructuración y organización de los datos para hacerlos aptos para análisis.​\n",
    "\n",
    "## Fases del proceso ETL\n",
    "Extracción: Consiste en recopilar datos relevantes de diferentes fuentes, asegurando que el impacto en los sistemas origen sea mínimo. Los datos pueden extraerse mediante diversos métodos como consultas SQL o servicios web.​\n",
    "\n",
    "Transformación: En esta etapa, los datos se limpian y se ajustan para garantizar coherencia y calidad, incluyendo la eliminación de valores nulos, normalización y conversión a formatos consistentes, además de aplicar reglas específicas de negocio.\n",
    "\n",
    "Carga: Finalmente, los datos transformados se cargan en el sistema de destino, donde estarán disponibles para análisis, informes o modelado de datos.\n",
    "\n",
    "## Importancia del ETL\n",
    "Es crucial en la minería de datos porque preparar los datos brutos para que puedan ser utilizados en análisis estadísticos, modelados predictivos o técnicas de aprendizaje automático, asegurando la calidad, coherencia y accesibilidad de la información."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4225f2e7-fccf-4a13-86a6-92acee434aa7",
   "metadata": {},
   "source": [
    "## desde python sin librerias pandas / polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86754f0-a78b-453c-9127-5abd20e1877d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876facae-5d32-47aa-8b55-d9b04deadb85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c8f6200-1ad0-4fa8-9cf1-aa1df22c4b3b",
   "metadata": {},
   "source": [
    "### 1.1 Crear estructura de directorios segun modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "605182b8-9652-410e-8c79-13b10a33e692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rutas: \n",
    "\n",
    "carpeta_entrada    = Path(ruta_base)\n",
    "#carpeta_entrada_mnt   = Path('/mnt/data')\n",
    "carpeta_datasets_entrada   = carpeta_entrada / 'datasets_entrada'\n",
    "carpeta_datasets_entrada.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "carpeta_datasets_salida   = carpeta_entrada / 'datasets_salida'\n",
    "carpeta_datasets_salida.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "carpeta_reportes   = carpeta_datasets_salida / 'reportes'\n",
    "carpeta_reportes.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "carpeta_limpios    = carpeta_datasets_salida / 'limpios'\n",
    "carpeta_limpios.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Nombres esperados de archivos\n",
    "archivo_ventas     = 'ventas.csv'\n",
    "archivo_clientes   = 'clientes.csv'\n",
    "archivo_marketing  = 'marketing.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b65281a-0cd0-43a6-8a84-ed2692ae60b2",
   "metadata": {},
   "source": [
    "### 1.2 rutas y carga de los dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "052e6d76-d227-4749-af2c-f065eabf82f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datasets del curso...\n",
      "...Arhivos cargados\n"
     ]
    }
   ],
   "source": [
    "# --- Paso 3: Cargar archivos del curso ---\n",
    "print(\"Cargando datasets del curso...\")\n",
    "try:\n",
    "    ruta_ventas     = os.path.join(carpeta_datasets_entrada, archivo_ventas)\n",
    "    ruta_clientes   = os.path.join(carpeta_datasets_entrada, archivo_clientes)\n",
    "    ruta_marketing  = os.path.join(carpeta_datasets_entrada, archivo_marketing)\n",
    "    \n",
    "    df_ventas       = pd.read_csv(f\"{ruta_ventas}\")\n",
    "    df_clientes     = pd.read_csv(f\"{ruta_clientes}\")\n",
    "    df_marketing    = pd.read_csv(f\"{ruta_marketing}\")\n",
    "    dic_dfs = { \"df_ventas\"   : df_ventas,\n",
    "                \"df_clientes\" : df_clientes,\n",
    "                \"df_marketing\": df_marketing}\n",
    "    print (\"...Arhivos cargados\")\n",
    "except:\n",
    "    print (\"Arhivos no encontrados\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e91d4ff-4465-4ec3-b5e1-29f5d4cc0277",
   "metadata": {},
   "source": [
    "### 1.3 Estructura de parámetros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "895c63d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Proceso principal para los 3 CSV ----------\n",
    "desviacion_margen     = 1.5\n",
    "desviacion_umbral     = 3.0\n",
    "cantidad_duplicados   = 0\n",
    "reportes_creados      = []\n",
    "ruta_excel            = carpeta_reportes / 'reporte_limpieza.xlsx'\n",
    "guardado_ok           = False\n",
    "mensajes              = []\n",
    "TOKENS_VALOR_FALTANTE = {'na', 'n/a', 'null', 'none', 'sin dato', 's/d', 'nd', '-', '--', '?', 'sin_dato', 'n/d'}\n",
    "reglas= {\n",
    "            # id_campanha, producto, canal, costo, fecha_inicio, fecha_fin\n",
    "            \"df_marketing\" : {\n",
    "                                'producto':     ('string',  {'tipo': 'lower', 'normalizar_acentos': True}),\n",
    "                                'canal':        ('string',  {'tipo': 'upper', 'normalizar_acentos': True}),\n",
    "                                'costo':        ('numeric', {'as_int': False}),\n",
    "                                'fecha_inicio': ('date',    {'dayfirst': True, 'formats': ['%d/%m/%Y', '%Y-%m-%d']}),\n",
    "                                'fecha_fin':    ('date',    {'dayfirst': True, 'formats': ['%d/%m/%Y', '%Y-%m-%d']})\n",
    "                                },\n",
    "            # id_venta, producto, precio, cantidad, fecha_venta, categoria\n",
    "            \"df_ventas\" : {\n",
    "                                'producto':     ('string',  {'tipo': 'lower', 'normalizar_acentos': True}),\n",
    "                                'precio':       ('numeric', {'as_int': False}),\n",
    "                                'cantidad':     ('numeric', {'as_int': False}),\n",
    "                                'fecha_venta':  ('date',    {'dayfirst': True, 'formats': ['%d/%m/%Y', '%Y-%m-%d']}),\n",
    "                                'categoria':    ('string',  {'tipo': 'lower', 'normalizar_acentos': True})\n",
    "                            },\n",
    "            # id_cliente, nombre, edad, ciudad, ingresos\n",
    "            \"df_clientes\" : {\n",
    "                                'nombre':       ('string',  {'tipo': 'title', 'normalizar_acentos': True}),\n",
    "                                'edad':         ('numeric', {'as_int': True}),\n",
    "                                'ciudad':       ('string',  {'tipo': 'title', 'normalizar_acentos': True}),\n",
    "                                'ingresos':     ('numeric', {'as_int': False})\n",
    "                            }\n",
    "    }\n",
    "reglas_por_archivo = {\n",
    "    'ventas.csv':    reglas_ventas,\n",
    "    'clientes.csv':  reglas_clientes,\n",
    "    'marketing.csv': reglas_marketing\n",
    "}\n",
    "\n",
    "#zip_path = carpeta_reportes.parent / 'reports_dataset_tpi_v2.zip'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a06a6528-ba7e-4f2c-889a-5d428f5a1cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    df_ventas\n",
      "        Descripción preliminar:\n",
      "                  id_venta     cantidad\n",
      "count  3035.000000  3033.000000\n",
      "mean   1499.851400     6.496538\n",
      "std     866.465379     3.457250\n",
      "min       1.000000     1.000000\n",
      "25%     748.500000     3.000000\n",
      "50%    1502.000000     7.000000\n",
      "75%    2249.500000     9.000000\n",
      "max    3000.000000    12.000000\n",
      "        Dimensiones:2\n",
      "        Forma:(3035, 6)    \n",
      "        Número de elementos:18210\n",
      "        Nombres de columnas:Index(['id_venta', 'producto', 'precio', 'cantidad', 'fecha_venta',\n",
      "       'categoria'],\n",
      "      dtype='object')\n",
      "        Nombres de filas:RangeIndex(start=0, stop=3035, step=1)\n",
      "        Tipos de datos:\n",
      "id_venta         int64\n",
      "producto        object\n",
      "precio          object\n",
      "cantidad       float64\n",
      "fecha_venta     object\n",
      "categoria       object\n",
      "dtype: object\n",
      "        Primeras 10 filas:\n",
      "   id_venta           producto   precio  cantidad fecha_venta  \\\n",
      "0       792  Cuadro decorativo   $69.94       5.0  02/01/2024   \n",
      "1       811    Lámpara de mesa  $105.10       5.0  02/01/2024   \n",
      "2      1156           Secadora   $97.96       3.0  02/01/2024   \n",
      "3      1372           Heladera  $114.35       8.0  02/01/2024   \n",
      "4      1546           Secadora  $106.21       4.0  02/01/2024   \n",
      "5      1697    Horno eléctrico   $35.35       9.0  02/01/2024   \n",
      "6      1710   Plancha de vapor   $65.43       2.0  02/01/2024   \n",
      "7      2959          Proyector   $88.17       9.0  02/01/2024   \n",
      "8       318  Rincón de plantas   $79.86      11.0  03/01/2024   \n",
      "9       419         Candelabro   $66.11       8.0  03/01/2024   \n",
      "\n",
      "           categoria  \n",
      "0         Decoración  \n",
      "1         Decoración  \n",
      "2  Electrodomésticos  \n",
      "3  Electrodomésticos  \n",
      "4  Electrodomésticos  \n",
      "5  Electrodomésticos  \n",
      "6  Electrodomésticos  \n",
      "7        Electrónica  \n",
      "8         Decoración  \n",
      "9         Decoración  \n",
      "        Últimas 3 filas:\n",
      "      id_venta                producto   precio  cantidad fecha_venta  \\\n",
      "3032      2696                  Laptop  $107.81       4.0  30/12/2024   \n",
      "3033      2913              Smartphone   $99.85       7.0  30/12/2024   \n",
      "3034      2930  Consola de videojuegos   $55.47       6.0  30/12/2024   \n",
      "\n",
      "        categoria  \n",
      "3032  Electrónica  \n",
      "3033  Electrónica  \n",
      "3034  Electrónica  \n",
      "    **************************************************\n",
      "    \n",
      "\n",
      "    df_clientes\n",
      "        Descripción preliminar:\n",
      "               id_cliente        edad      ingresos\n",
      "count  578.000000  578.000000    578.000000\n",
      "mean   289.500000   37.968858  34755.977266\n",
      "std    166.998503   10.253244  12989.576812\n",
      "min      1.000000   20.000000    170.290000\n",
      "25%    145.250000   30.000000  26119.060000\n",
      "50%    289.500000   37.000000  35102.285000\n",
      "75%    433.750000   43.000000  42600.435000\n",
      "max    578.000000   81.000000  88053.010000\n",
      "        Dimensiones:2\n",
      "        Forma:(578, 5)    \n",
      "        Número de elementos:2890\n",
      "        Nombres de columnas:Index(['id_cliente', 'nombre', 'edad', 'ciudad', 'ingresos'], dtype='object')\n",
      "        Nombres de filas:RangeIndex(start=0, stop=578, step=1)\n",
      "        Tipos de datos:\n",
      "id_cliente      int64\n",
      "nombre         object\n",
      "edad            int64\n",
      "ciudad         object\n",
      "ingresos      float64\n",
      "dtype: object\n",
      "        Primeras 10 filas:\n",
      "   id_cliente               nombre  edad                 ciudad  ingresos\n",
      "0           1      Aloysia Screase    44          Mar del Plata  42294.68\n",
      "1           2  Kristina Scaplehorn    25                Posadas  24735.04\n",
      "2           3       Filip Castagne    50            Resistencia  35744.85\n",
      "3           4          Liuka Luard    39           Bahía Blanca  27647.96\n",
      "4           5        Dore Cockshtt    28                Rosario  28245.65\n",
      "5           6        Patrick Earle    34  San Miguel de Tucumán  62763.31\n",
      "6           7           Etan Deeth    35            Resistencia  37489.71\n",
      "7           8       Booth Bielfelt    40                Córdoba  35255.94\n",
      "8           9         Shirl Labone    29                Rosario  27592.08\n",
      "9          10      Andy Mendenhall    52                Rosario  37153.94\n",
      "        Últimas 3 filas:\n",
      "     id_cliente           nombre  edad         ciudad  ingresos\n",
      "575         576  Cari Marzellano    38  Mar del Plata  53114.05\n",
      "576         577   Magdalene Pegg    34        Córdoba  49849.42\n",
      "577         578    Lorry Santori    47     Corrientes  42000.81\n",
      "    **************************************************\n",
      "    \n",
      "\n",
      "    df_marketing\n",
      "        Descripción preliminar:\n",
      "               id_campanha      costo\n",
      "count    90.000000  90.000000\n",
      "mean     45.500000   4.928667\n",
      "std      26.124701   0.947750\n",
      "min       1.000000   2.950000\n",
      "25%      23.250000   4.372500\n",
      "50%      45.500000   4.900000\n",
      "75%      67.750000   5.562500\n",
      "max      90.000000   7.390000\n",
      "        Dimensiones:2\n",
      "        Forma:(90, 6)    \n",
      "        Número de elementos:540\n",
      "        Nombres de columnas:Index(['id_campanha', 'producto', 'canal', 'costo', 'fecha_inicio',\n",
      "       'fecha_fin'],\n",
      "      dtype='object')\n",
      "        Nombres de filas:RangeIndex(start=0, stop=90, step=1)\n",
      "        Tipos de datos:\n",
      "id_campanha       int64\n",
      "producto         object\n",
      "canal            object\n",
      "costo           float64\n",
      "fecha_inicio     object\n",
      "fecha_fin        object\n",
      "dtype: object\n",
      "        Primeras 10 filas:\n",
      "   id_campanha             producto  canal  costo fecha_inicio   fecha_fin\n",
      "0           74      Adorno de pared     TV   4.81   20/03/2024  03/05/2024\n",
      "1           12               Tablet   RRSS   3.40   26/03/2024  13/05/2024\n",
      "2           32      Lámpara de mesa  Email   5.54   28/03/2024  20/04/2024\n",
      "3           21           Smartphone   RRSS   6.37   29/03/2024  16/05/2024\n",
      "4           58             Alfombra  Email   4.25   31/03/2024  05/05/2024\n",
      "5           85           SmartWatch     TV   5.07   01/04/2024  05/05/2024\n",
      "6           36     Plancha de vapor  Email   5.41   02/04/2024  01/06/2024\n",
      "7           57             Batidora  Email   4.48   10/04/2024  08/06/2024\n",
      "8           44      Adorno de pared  Email   5.08   13/04/2024  10/05/2024\n",
      "9           84  Parlantes Bluetooth     TV   4.42   17/04/2024  05/05/2024\n",
      "        Últimas 3 filas:\n",
      "    id_campanha            producto  canal  costo fecha_inicio  fecha_fin\n",
      "87           68   Rincón de plantas     TV   5.81   17/12/2024  14/2/2025\n",
      "88           33            Secadora  Email   3.80   20/12/2024   7/1/2025\n",
      "89           11  Freidora eléctrica   RRSS   5.27   29/12/2024  21/1/2025\n",
      "    **************************************************\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Mostrar el DataFrame\n",
    "def ver():\n",
    "    for nombre,df in dic_dfs.items():\n",
    "        print(f\"\"\"\n",
    "    {nombre}\n",
    "        Descripción preliminar:\n",
    "        {df.describe()}\n",
    "        Dimensiones:{ df.ndim}\n",
    "        Forma:{ df.shape}    \n",
    "        Número de elementos:{ df.size}\n",
    "        Nombres de columnas:{ df.columns}\n",
    "        Nombres de filas:{ df.index}\n",
    "        Tipos de datos:\\n{ df.dtypes}\n",
    "        Primeras 10 filas:\\n{ df.head(10)}\n",
    "        Últimas 3 filas:\\n{ df.tail(3)}\n",
    "    {\"*\"*50}\n",
    "    \"\"\")\n",
    "ver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "21030142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Funciones utilitarias en castellano (snake_case) ----------\n",
    "def quitar_acentos(entrada):\n",
    "    \"\"\"\n",
    "    Elimina acentos (tildes/diacríticos) de un texto.\n",
    "    Mantiene NaN intactos.\n",
    "    \"\"\"\n",
    "    if pd.isna(entrada):\n",
    "        return entrada\n",
    "    if isinstance(entrada, str):\n",
    "        #nk = unicodedata.normalize('NFKD', entrada)\n",
    "        #salida = ''.join([c for c in nk if not unicodedata.combining(c)])\n",
    "        texto_n = unicodedata.normalize(\"NFD\", entrada)\n",
    "        return \"\".join(c for c in texto_n if unicodedata.category(c) != \"Mn\")\n",
    "    return entrada\n",
    "    '''\n",
    "    if salida != entrada:\n",
    "        print(\"\"\"\\033[1;37;44m\\n\n",
    "╔═════════════════════════════════════════════════════════════════════════════╗\n",
    "║                                sacar_acentos                                ║\n",
    "╚═════════════════════════════════════════════════════════════════════════════╝\\033[0;m\"\"\")\n",
    "    print(f\"    {texto} -->  {salida}\")\n",
    "    '''\n",
    "    return salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e8de492f-bd35-465f-bbe3-97f1455adf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_num(serie):\n",
    "    \"\"\"\n",
    "    Convierte una serie a tipo numérico.\n",
    "    - Elimina símbolos como $, % y comas.\n",
    "    - Reemplaza coma decimal por punto.\n",
    "    - Si no puede convertir, deja NaN.\n",
    "    \"\"\"\n",
    "    salida =  (serie.astype(str)\n",
    "                  .str.replace(r\"[^0-9,.\\-]\", \"\", regex=True)\n",
    "                  .str.replace(\",\", \".\", regex=False)\n",
    "                  .replace(\"\", np.nan)\n",
    "                  .astype(float))\n",
    "\n",
    "    \n",
    "    return salida\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3a7982-b970-401c-94bf-ee301fd62c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_ventas\n",
      "              id_venta                producto   precio  cantidad fecha_venta  \\\n",
      "0          792       Cuadro decorativo   $69.94       5.0  02/01/2024   \n",
      "1          811         Lámpara de mesa  $105.10       5.0  02/01/2024   \n",
      "2         1156                Secadora   $97.96       3.0  02/01/2024   \n",
      "3         1372                Heladera  $114.35       8.0  02/01/2024   \n",
      "4         1546                Secadora  $106.21       4.0  02/01/2024   \n",
      "...        ...                     ...      ...       ...         ...   \n",
      "3030      1837         Horno eléctrico  $104.12       9.0  30/12/2024   \n",
      "3031      2276                  Laptop   $85.27       9.0  30/12/2024   \n",
      "3032      2696                  Laptop  $107.81       4.0  30/12/2024   \n",
      "3033      2913              Smartphone   $99.85       7.0  30/12/2024   \n",
      "3034      2930  Consola de videojuegos   $55.47       6.0  30/12/2024   \n",
      "\n",
      "              categoria  \n",
      "0            Decoración  \n",
      "1            Decoración  \n",
      "2     Electrodomésticos  \n",
      "3     Electrodomésticos  \n",
      "4     Electrodomésticos  \n",
      "...                 ...  \n",
      "3030  Electrodomésticos  \n",
      "3031        Electrónica  \n",
      "3032        Electrónica  \n",
      "3033        Electrónica  \n",
      "3034        Electrónica  \n",
      "\n",
      "[3035 rows x 6 columns]\n",
      "        \n",
      "\n",
      "            nombre_columna='producto'\n",
      "            serie=0            Cuadro decorativo\n",
      "1              Lámpara de mesa\n",
      "2                     Secadora\n",
      "3                     Heladera\n",
      "4                     Secadora\n",
      "                 ...          \n",
      "3030           Horno eléctrico\n",
      "3031                    Laptop\n",
      "3032                    Laptop\n",
      "3033                Smartphone\n",
      "3034    Consola de videojuegos\n",
      "Name: producto, Length: 3035, dtype: object\n",
      "            regla=('string', {'tipo': 'lower', 'normalizar_acentos': True})\n",
      "            \n",
      "\n",
      "            **************************************************\n",
      "            regla=('string', {'tipo': 'lower', 'normalizar_acentos': True})\n",
      "\n",
      "            nombre_columna='precio'\n",
      "            serie=0        $69.94\n",
      "1       $105.10\n",
      "2        $97.96\n",
      "3       $114.35\n",
      "4       $106.21\n",
      "         ...   \n",
      "3030    $104.12\n",
      "3031     $85.27\n",
      "3032    $107.81\n",
      "3033     $99.85\n",
      "3034     $55.47\n",
      "Name: precio, Length: 3035, dtype: object\n",
      "            regla=('numeric', {'as_int': False})\n",
      "            \n",
      "\n",
      "            **************************************************\n",
      "            regla=('numeric', {'as_int': False})\n",
      "\n",
      "            nombre_columna='cantidad'\n",
      "            serie=0       5.0\n",
      "1       5.0\n",
      "2       3.0\n",
      "3       8.0\n",
      "4       4.0\n",
      "       ... \n",
      "3030    9.0\n",
      "3031    9.0\n",
      "3032    4.0\n",
      "3033    7.0\n",
      "3034    6.0\n",
      "Name: cantidad, Length: 3035, dtype: float64\n",
      "            regla=('numeric', {'as_int': False})\n",
      "            \n",
      "\n",
      "            **************************************************\n",
      "            regla=('numeric', {'as_int': False})\n",
      "\n",
      "            nombre_columna='fecha_venta'\n",
      "            serie=0       02/01/2024\n",
      "1       02/01/2024\n",
      "2       02/01/2024\n",
      "3       02/01/2024\n",
      "4       02/01/2024\n",
      "           ...    \n",
      "3030    30/12/2024\n",
      "3031    30/12/2024\n",
      "3032    30/12/2024\n",
      "3033    30/12/2024\n",
      "3034    30/12/2024\n",
      "Name: fecha_venta, Length: 3035, dtype: object\n",
      "            regla=('date', {'dayfirst': True, 'formats': ['%d/%m/%Y', '%Y-%m-%d']})\n",
      "            \n",
      "\n",
      "            **************************************************\n",
      "            regla=('date', {'dayfirst': True, 'formats': ['%d/%m/%Y', '%Y-%m-%d']})\n",
      "\n",
      "            nombre_columna='categoria'\n",
      "            serie=0              Decoración\n",
      "1              Decoración\n",
      "2       Electrodomésticos\n",
      "3       Electrodomésticos\n",
      "4       Electrodomésticos\n",
      "              ...        \n",
      "3030    Electrodomésticos\n",
      "3031          Electrónica\n",
      "3032          Electrónica\n",
      "3033          Electrónica\n",
      "3034          Electrónica\n",
      "Name: categoria, Length: 3035, dtype: object\n",
      "            regla=('string', {'tipo': 'lower', 'normalizar_acentos': True})\n",
      "            \n",
      "\n",
      "            **************************************************\n",
      "            regla=('string', {'tipo': 'lower', 'normalizar_acentos': True})\n",
      "    id_venta                producto   precio  cantidad fecha_venta  \\\n",
      "0        792       Cuadro decorativo   $69.94       5.0  02/01/2024   \n",
      "1        811         Lámpara de mesa  $105.10       5.0  02/01/2024   \n",
      "2       1156                Secadora   $97.96       3.0  02/01/2024   \n",
      "3       1372                Heladera  $114.35       8.0  02/01/2024   \n",
      "4       1546                Secadora  $106.21       4.0  02/01/2024   \n",
      "5       1697         Horno eléctrico   $35.35       9.0  02/01/2024   \n",
      "6       1710        Plancha de vapor   $65.43       2.0  02/01/2024   \n",
      "7       2959               Proyector   $88.17       9.0  02/01/2024   \n",
      "8        318       Rincón de plantas   $79.86      11.0  03/01/2024   \n",
      "9        419              Candelabro   $66.11       8.0  03/01/2024   \n",
      "10      1374              Aspiradora   $95.90       5.0  03/01/2024   \n",
      "11      1571      Freidora eléctrica  $111.18       1.0  03/01/2024   \n",
      "12      1814              Aspiradora   $70.91       2.0  03/01/2024   \n",
      "13      2769               Proyector   $43.62      11.0  03/01/2024   \n",
      "14      2794                  Tablet   $67.97       9.0  03/01/2024   \n",
      "15      2824                  Tablet   $74.09       5.0  03/01/2024   \n",
      "16       446                Cortinas   $62.10       4.0  04/01/2024   \n",
      "17       630         Adorno de pared   $71.99       9.0  04/01/2024   \n",
      "18       794       Jarrón decorativo   $96.97       7.0  04/01/2024   \n",
      "19       882       Cuadro decorativo  $101.92       9.0  04/01/2024   \n",
      "20       969              Candelabro   $90.70      12.0  04/01/2024   \n",
      "21      1445                Lavadora   $28.41      10.0  04/01/2024   \n",
      "22      1971      Freidora eléctrica  $122.25      11.0  04/01/2024   \n",
      "23      2209               Proyector   $32.15       1.0  04/01/2024   \n",
      "24       256                Cortinas   $86.82       8.0  05/01/2024   \n",
      "25       606                Cortinas  $106.66      11.0  05/01/2024   \n",
      "26       704       Jarrón decorativo   $57.22       9.0  05/01/2024   \n",
      "27      1362                Heladera  $109.49      11.0  05/01/2024   \n",
      "28      2155             Auriculares   $84.80      10.0  05/01/2024   \n",
      "29      2947          Cámara digital  $107.78       3.0  05/01/2024   \n",
      "30      1495                Lavadora   $38.99       6.0  06/01/2024   \n",
      "31      1572                Cafetera   $61.40       9.0  06/01/2024   \n",
      "32      2180  Consola de videojuegos  $102.57       9.0  06/01/2024   \n",
      "33      2487          Cámara digital   $50.76       3.0  06/01/2024   \n",
      "34      2603              Smartphone   $87.05       5.0  06/01/2024   \n",
      "35      2885             Auriculares  $100.40       6.0  06/01/2024   \n",
      "36        17   Elementos de cerámica  $108.48      11.0  07/01/2024   \n",
      "37       183       Espejo decorativo   $82.76       8.0  07/01/2024   \n",
      "38       560         Adorno de pared   $57.62       6.0  07/01/2024   \n",
      "39       632       Cuadro decorativo   $30.49      12.0  07/01/2024   \n",
      "\n",
      "            categoria  \n",
      "0          Decoración  \n",
      "1          Decoración  \n",
      "2   Electrodomésticos  \n",
      "3   Electrodomésticos  \n",
      "4   Electrodomésticos  \n",
      "5   Electrodomésticos  \n",
      "6   Electrodomésticos  \n",
      "7         Electrónica  \n",
      "8          Decoración  \n",
      "9          Decoración  \n",
      "10  Electrodomésticos  \n",
      "11  Electrodomésticos  \n",
      "12  Electrodomésticos  \n",
      "13        Electrónica  \n",
      "14        Electrónica  \n",
      "15        Electrónica  \n",
      "16         Decoración  \n",
      "17         Decoración  \n",
      "18         Decoración  \n",
      "19         Decoración  \n",
      "20         Decoración  \n",
      "21  Electrodomésticos  \n",
      "22  Electrodomésticos  \n",
      "23        Electrónica  \n",
      "24         Decoración  \n",
      "25         Decoración  \n",
      "26         Decoración  \n",
      "27  Electrodomésticos  \n",
      "28        Electrónica  \n",
      "29        Electrónica  \n",
      "30  Electrodomésticos  \n",
      "31  Electrodomésticos  \n",
      "32        Electrónica  \n",
      "33        Electrónica  \n",
      "34        Electrónica  \n",
      "35        Electrónica  \n",
      "36         Decoración  \n",
      "37         Decoración  \n",
      "38         Decoración  \n",
      "39         Decoración  \n"
     ]
    }
   ],
   "source": [
    "def limpiar_dataframes() :\n",
    "    \"\"\"\n",
    "    Limpia el DataFrame aplicando reglas_por_columna = {\"col\": (\"regla\", parametros)...}\n",
    "    Las reglas se asignan automáticamente según el tipo o formato:\n",
    "      - Columnas numéricas o con símbolos ($, %, dígitos) → 'numeric'\n",
    "      - Columnas que parecen fechas → 'date'\n",
    "      - Otras columnas → 'string'\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    for nombre_df, df_cada in dic_dfs.items():\n",
    "        print (f\"\"\"{nombre_df}\n",
    "        {dic_dfs[nombre_df]}\n",
    "        \"\"\")\n",
    "        \n",
    "        for nombre_columna, serie in df_cada.items():\n",
    "            nombre_columna= nombre_columna.lower().replace(\" \",\"_\")\n",
    "            if nombre_columna.startswith(\"id_\"):\n",
    "                continue\n",
    "            regla, params = reglas[nombre_df][nombre_columna]]\n",
    "            print (f\"\"\"\n",
    "            {nombre_columna=}\n",
    "            {serie=}\n",
    "            {regla=}\n",
    "            \"\"\")\n",
    "            s = serie.copy()\n",
    "            if regla == \"numeric\":\n",
    "                s = s.strip()\n",
    "                s = s.replace('$', '')\n",
    "                s = pd.to_numeric(s, errors=\"coerce\")\n",
    "                if regla[\"numeric\"][\"as_int\"] :\n",
    "                    s = s.replace('.', '').replace(',', '')\n",
    "                    if not s.isna().any():\n",
    "                        s = s.astype(int)\n",
    "            elif regla == \"string\":\n",
    "                s = s.astype(str).str.string()\n",
    "                match  regla[\"numeric\"][\"tipo\"] :\n",
    "                    case \"upper\":\n",
    "                        s = s.str.upper()\n",
    "                    case \"title\":\n",
    "                        s = s.str.title()\n",
    "                    case \"lower\":\n",
    "                        s = s.str.lower()\n",
    "            elif regla == \"date\":\n",
    "                if regla[\"date\"][\"dayfirst\"]:\n",
    "                    salida = serie.dt.strftime('%Y/%m/%d', errors=\"coerce\", dayfirst=True)\n",
    "                    #'formats': ['%d/%m/%Y', '%Y-%m-%d']}),\n",
    "                else:\n",
    "                    salida = serie.dt.strftime('%Y/%m/%d', errors=\"coerce\", dayfirst=False)                \n",
    "            elif regla == \"fillna\":\n",
    "                s = s.fillna(0)\n",
    "            dic_dfs[nombre_df][nombre_columna] = s\n",
    "    \n",
    "            \n",
    "            #params = regla_info[1] if len(regla_info) > 1 else None\n",
    "            #df[col] = aplicar_regla_columna(serie, regla, formatos=params)\n",
    "            print (f\"\"\"\n",
    "            {\"*\"*50}\n",
    "            {regla=}\"\"\")\n",
    "        print (dic_dfs[nombre_df].head(40))\n",
    "limpiar_dataframes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4aefb8b-3e14-4917-9986-8b7f27033cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_valor_faltante(valor):\n",
    "    \"\"\"\n",
    "    Determina si un valor debe considerarse faltante (True) usando tokens y NaN.\n",
    "    \"\"\"\n",
    "    if pd.isna(valor):\n",
    "        return True\n",
    "    s = str(valor).strip().lower()\n",
    "    s = sacar_acentos(s)\n",
    "    return s in TOKENS_VALOR_FALTANTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4634aeae-c21a-47dc-9bb9-cffdeafcbe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Detección de outliers (IQR) - función corregida y robusta\n",
    "def mascara_valores_atipicos_rango_intercuartil(serie_datos):\n",
    "    \"\"\"\n",
    "    Devuelve una tupla: (mascara_bool_series, cantidad_outliers, (limite_inferior, limite_superior))\n",
    "    - serie_datos: pd.Series (acepta valores no numéricos, se intentará convertir)\n",
    "    - La máscara tiene la misma indexación que la serie original (NaNs -> False)\n",
    "    \"\"\"\n",
    "    # Intentar convertir a numérico (coerce -> NaN para no numéricos)\n",
    "    serie_numerica = pd.to_numeric(serie_datos, errors='coerce')\n",
    "    # Serie limpia para cálculos de cuartiles (sin NaN)\n",
    "    serie_limpia = serie_numerica.dropna().astype(float)\n",
    "    if serie_limpia.shape[0] < 4:\n",
    "        # No hay suficientes datos para IQR: devolver máscara False de la misma longitud\n",
    "        mascara = pd.Series([False] * len(serie_datos), index=serie_datos.index)\n",
    "        return mascara, int(mascara.sum()), (None, None)\n",
    "    cuartil_1 = float(serie_limpia.quantile(0.25))\n",
    "    cuartil_3 = float(serie_limpia.quantile(0.75))\n",
    "    rango_intercuartil = cuartil_3 - cuartil_1\n",
    "    limite_inferior = cuartil_1 - desviacion_margen * rango_intercuartil\n",
    "    limite_superior = cuartil_3 + desviacion_margen * rango_intercuartil\n",
    "    # Crear máscara sobre la serie numérica original (alineada con el index original)\n",
    "    mascara = (serie_numerica < limite_inferior) | (serie_numerica > limite_superior)\n",
    "    mascara = mascara.fillna(False).astype(bool)\n",
    "    return mascara, int(mascara.sum()), (limite_inferior, limite_superior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c15444c-3df6-409e-a932-c08afaebfd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funciones utilitarias definidas.\n"
     ]
    }
   ],
   "source": [
    "# Detección de outliers (Z-score)\n",
    "def mascara_valores_atipicos_zscore(serie_datos, desviacion_umbral=3.0):\n",
    "    \"\"\"\n",
    "    Devuelve máscara booleana (True = outlier) según Z-score.\n",
    "    \"\"\"\n",
    "    serie_limpia = serie_datos.dropna().astype(float)\n",
    "    if serie_limpia.shape[0] < 4 or serie_limpia.std() == 0:\n",
    "        return pd.Series([False] * len(serie_datos), index=serie_datos.index)\n",
    "    puntaje_z = (serie_datos - serie_limpia.mean()) / serie_limpia.std()\n",
    "    return puntaje_z.abs() > desviacion_umbral  \n",
    "print('Funciones utilitarias definidas.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f411cb90-17af-45f2-9dfb-9fe7534bacbb",
   "metadata": {},
   "source": [
    "# limpieza y normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe131403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Función detectar_problemas_en_dataframe cargada.\n"
     ]
    }
   ],
   "source": [
    "# ---------- Detección de problemas en un DataFrame ----------\n",
    "def detectar_problemas_en_dataframe(df: pd.DataFrame):\n",
    " \n",
    "    resumen                       = {}\n",
    "    resumen['filas']              = df.shape[0]\n",
    "    resumen['columnas']           = df.shape[1]\n",
    "    resumen['nulos_por_columna']  = df.isna().sum().to_dict()\n",
    "    dup_mask                      = df.duplicated(keep=False)\n",
    "    resumen['duplicados_exactos'] = int(dup_mask.sum())\n",
    "    chequeos_por_columna          = {}\n",
    "    print(f\"\"\"\\033[1;37;44m\\n\n",
    "╔═════════════════════════════════════════════════════════════════════════════╗\n",
    "║                              Valores a eliminar                             ║\n",
    "╠═════════════════════════════════════════════════════════════════════════════╣\n",
    "║     Afecta                                                                  ║\n",
    "║         Elimina espacios iniciales y finales.                               ║\n",
    "║         Borra Na                                                            ║\n",
    "║         Borra duplicados                                                    ║\n",
    "╚═════════════════════════════════════════════════════════════════════════════╝\\033[0;m\"\"\") \n",
    "    print(f\"\"\"\\033[1;37;44m\\n\n",
    "╔═════════════════════════════════════════════════════════════════════════════╗\n",
    "║                               Valores atípicos                              ║\n",
    "╠═════════════════════════════════════════════════════════════════════════════╣\n",
    "║     Afecta                                                                  ║\n",
    "║         NO Modifica datos.                                                  ║\n",
    "║         Se guarda la información en archivo excel para referencias futuras  ║\n",
    "║         Se evalua es mediante dos formas                                    ║\n",
    "║            1) limites intercuartiles 25 y 75 % * desviacion_margen {desviacion_margen}      ║\n",
    "║            2) Z-score mayor a desviacion_umbral {desviacion_umbral}                         ║\n",
    "╚═════════════════════════════════════════════════════════════════════════════╝\\033[0;m\"\"\")\n",
    "    for col in df.columns:\n",
    "        serie = df[col]\n",
    "        info = {'dtype': str(serie.dtype), 'nulos': int(serie.isna().sum())}\n",
    "        if serie.dtype == object or pd.api.types.is_string_dtype(serie):\n",
    "            s = serie.astype(str)\n",
    "            info['espacios_inicio']          = int(s.str.match(r'^\\s+').sum())\n",
    "            info['espacios_final']           = int(s.str.match(r'\\s+$').sum())\n",
    "            try:\n",
    "                unique_original              = set(s.dropna().unique())\n",
    "                unique_lower                 = set(s.dropna().str.lower().unique())\n",
    "                info['unique_original']      = len(unique_original)\n",
    "                info['unique_lower']         = len(unique_lower)\n",
    "                info['variantes_mayusculas'] = len(unique_lower) < len(unique_original)\n",
    "            except Exception:\n",
    "                info['unique_original']      = serie.nunique(dropna=True)\n",
    "                info['unique_lower']         = None\n",
    "                info['variantes_mayusculas'] = None\n",
    "            try:\n",
    "                unaccented                   = s.dropna().map(lambda x: sacar_acentos(x).lower())\n",
    "                groups                       = unaccented.groupby(unaccented).size()\n",
    "                conflicts                    = groups[groups > 1]\n",
    "                info['grupos_var_acentos']   = int(conflicts.shape[0])\n",
    "                ejemplos = {}\n",
    "                if not conflicts.empty:\n",
    "                    for val in conflicts.index[:5]:\n",
    "                        originales           = sorted(list(s[unaccented == val].unique())[:10])\n",
    "                        ejemplos[val]        = originales\n",
    "                info['ejemplos_var_acentos'] = ejemplos\n",
    "            except Exception:\n",
    "                info['grupos_var_acentos']   = None\n",
    "                info['ejemplos_var_acentos'] = {}\n",
    "            info['tokens_aparente_faltante'] = int(\n",
    "                s.map(lambda x: str(x).strip().lower()).map(lambda v: sacar_acentos(v) in TOKENS_VALOR_FALTANTE).sum()\n",
    "            )\n",
    "            info['muestras'] = list(s.dropna().unique()[:10])\n",
    "        else:\n",
    "            # numeric\n",
    "            if pd.api.types.is_numeric_dtype(serie) or (serie.dropna().astype(str).str.replace('.','',1).str.isnumeric().all() if len(serie.dropna())>0 else False):\n",
    "                try:\n",
    "                    serie_numerica = serie.dropna().astype(float)\n",
    "                except Exception:\n",
    "                    serie_numerica = pd.to_numeric(serie, errors='coerce').dropna().astype(float)\n",
    "                info['media'] = float(serie_numerica.mean()) if not serie_numerica.empty else None\n",
    "                info['std']   = float(serie_numerica.std()) if not serie_numerica.empty else None\n",
    "                info['min']   = float(serie_numerica.min()) if not serie_numerica.empty else None\n",
    "                info['max']   = float(serie_numerica.max()) if not serie_numerica.empty else None\n",
    "                if len(serie_numerica) >= 4:\n",
    "                    mascara_outliers_iqr, cant,(info['outliers_iqr'] ,info['limites_iqr']) =mascara_valores_atipicos_rango_intercuartil(serie)\n",
    "                    #mascara, int(mascara.sum()), (limite_inferior, limite_superior)\n",
    "                    '''\n",
    "                    print (f\"\"\"\n",
    "                    {mascara_outliers_iqr=}\n",
    "                    {cant=}\n",
    "                    {info['outliers_iqr']=}\n",
    "                    {info['limites_iqr']=}\n",
    "                    {\"-\"*100}\n",
    "                    \"\"\")\n",
    "                    '''\n",
    "                    \n",
    "                else:\n",
    "                    info['outliers_iqr'] = None\n",
    "                    info['limites_iqr'] = None\n",
    "                if len(serie_numerica) >= 4 and serie_numerica.std() != 0:\n",
    "                    z = (serie_numerica - serie_numerica.mean()) / serie_numerica.std()\n",
    "                    info['outliers_z'] = int((z.abs() > 3).sum())\n",
    "                else:\n",
    "                    info['outliers_z'] = None\n",
    "            else:\n",
    "                # fechas intento parseo\n",
    "                parsed = pd.to_datetime(serie, errors='coerce', dayfirst=True)\n",
    "                info['fechas_parseables'] = int(parsed.notna().sum())\n",
    "                info['muestras'] = list(serie.dropna().unique()[:10])\n",
    "        chequeos_por_columna[col] = info\n",
    "\n",
    "    filas_problemas = []\n",
    "    df.columns = [c.strip().lower().replace(' ', '_') for c in df.columns]\n",
    "    for idx, fila in df.iterrows():\n",
    "        lista_problemas = []\n",
    "        if dup_mask.loc[idx]:\n",
    "            lista_problemas.append('duplicado_exacto')\n",
    "        for col in df.columns:\n",
    "            val = fila[col]\n",
    "            \n",
    "            # heurísticas textuales\n",
    "            if pd.api.types.is_string_dtype(type(val)) or isinstance(val, str) or (\n",
    "                not pd.isna(val) and not pd.api.types.is_numeric_dtype(type(val)) and str(chequeos_por_columna[col].get('dtype','')).startswith('object')\n",
    "            ):\n",
    "                s = str(val)\n",
    "                if s != s.strip():\n",
    "                    lista_problemas.append(f'espacios_en_columna_{col}')\n",
    "                if chequeos_por_columna[col].get('variantes_mayusculas'):\n",
    "                    if s and s != s.lower() and s.lower() in [str(x).lower() for x in df[col].dropna().unique()]:\n",
    "                        lista_problemas.append(f'inconsistencia_mayusculas_columna_{col}')\n",
    "                if chequeos_por_columna[col].get('grupos_var_acentos') and chequeos_por_columna[col]['grupos_var_acentos'] > 0:\n",
    "                    try:\n",
    "                        #un = sacar_acentos(s).lower()\n",
    "                        #group_vals = [x for x in chequeos_por_columna[col].get('muestras', []) if sacar_acentos(str(x)).lower() == un]\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        df['col'] = df['col'].apply(sacar_acentos)\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        if group_vals and any(sacar_acentos(str(x)).lower() != sacar_acentos(s).lower() for x in group_vals):\n",
    "                            lista_problemas.append(f'variantes_acentos_columna_{col}')\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                if es_valor_faltante(s):\n",
    "                    lista_problemas.append(f'token_faltante_columna_{col}')\n",
    "            else:\n",
    "                # heurísticas numéricas\n",
    "                try:\n",
    "                    fval = float(val)\n",
    "                    info_col = chequeos_por_columna[col]\n",
    "                    limites = info_col.get('limites_iqr')\n",
    "                    if limites and (fval < limites[0] or fval > limites[1]):\n",
    "                        lista_problemas.append(f'outlier_iqr_columna_{col}')\n",
    "                    if info_col.get('std') not in (None, 0):\n",
    "                        mean = info_col.get('media')\n",
    "                        std = info_col.get('std')\n",
    "                        if std and abs((fval - mean) / std) > 3:\n",
    "                            lista_problemas.append(f'outlier_z_columna_{col}')\n",
    "                except Exception:\n",
    "                    pass\n",
    "        if lista_problemas:\n",
    "            filas_problemas.append({\n",
    "                'row_index': idx,\n",
    "                'problemas': ';'.join(sorted(set(lista_problemas))),\n",
    "                'muestra': json.dumps({str(c): str(fila[c]) for c in df.columns[:8]})\n",
    "            })\n",
    "\n",
    "    df_problemas = pd.DataFrame(filas_problemas)\n",
    "    return resumen, chequeos_por_columna, df_problemas\n",
    "\n",
    "print('Función detectar_problemas_en_dataframe cargada.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e42599a-abe2-4447-b8e6-6c1024f0d651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función auxiliar para guardar CSVs\n",
    "def guardar_csv(df, ruta):\n",
    "    \"\"\"\n",
    "    Guarda df en ruta (string o Path). Crea directorio padre si no existe.\n",
    "    \"\"\"\n",
    "    ruta = Path(ruta)\n",
    "    ruta.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(ruta, index=False, encoding='utf-8')\n",
    "    return ruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccc0f0c3-0e31-4f27-9661-3215c9185f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_frames = {\n",
    "    ruta_ventas     : df_ventas,\n",
    "    ruta_clientes   : df_clientes,\n",
    "    ruta_marketing  : df_marketing\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32235fb9-1e09-422e-931b-3471d6fbf123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37;44m\n",
      "\n",
      "╔═════════════════════════════════════════════════════════════════════════════╗\n",
      "║                     Aplico reglas según columna específica                  ║\n",
      "╠═════════════════════════════════════════════════════════════════════════════╣\n",
      "║     Afecta                                                                  ║\n",
      "║             Numéricos (int/float)                                           ║\n",
      "║             fechas --> YYYY,MM,DD                                           ║\n",
      "╚═════════════════════════════════════════════════════════════════════════════╝\u001b[0;m\n",
      "\u001b[1;37;44m\n",
      "\n",
      "╔═════════════════════════════════════════════════════════════════════════════╗\n",
      "║                              Valores a eliminar                             ║\n",
      "╠═════════════════════════════════════════════════════════════════════════════╣\n",
      "║     Afecta                                                                  ║\n",
      "║         Elimina espacios iniciales y finales.                               ║\n",
      "║         Borra Na                                                            ║\n",
      "║         Borra duplicados                                                    ║\n",
      "╚═════════════════════════════════════════════════════════════════════════════╝\u001b[0;m\n",
      "\u001b[1;37;44m\n",
      "\n",
      "╔═════════════════════════════════════════════════════════════════════════════╗\n",
      "║                               Valores atípicos                              ║\n",
      "╠═════════════════════════════════════════════════════════════════════════════╣\n",
      "║     Afecta                                                                  ║\n",
      "║         NO Modifica datos.                                                  ║\n",
      "║         Se guarda la información en archivo excel para referencias futuras  ║\n",
      "║         Se evalua es mediante dos formas                                    ║\n",
      "║            1) limites intercuartiles 25 y 75 % * desviacion_margen 1.5      ║\n",
      "║            2) Z-score mayor a desviacion_umbral 3.0                         ║\n",
      "╚═════════════════════════════════════════════════════════════════════════════╝\u001b[0;m\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sacar_acentos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 68\u001b[39m\n\u001b[32m     66\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mProceso completo del diccionario de DataFrames.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m errores_df\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m errores_df = \u001b[43mmenu_procesar_diccionario\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdic_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreglas_por_archivo\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mmenu_procesar_diccionario\u001b[39m\u001b[34m(dic_frames, reglas_por_archivo)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# trabajamos sobre una copia para evitar modificar dict original por error\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m [path_archivo, df_actual],[nombre_archivo,_] \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m( dic_frames.items() , reglas_por_archivo.items() ):\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     resumen_antes, chequeos_antes, problemas_antes = \u001b[43mdetectar_problemas_en_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_actual\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m--- RESUMEN ANTES: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnombre_archivo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m     \u001b[38;5;66;03m# Para no volcar objetos muy grandes, mostramos el head del DataFrame de problemas (si existe)\u001b[39;00m\n\u001b[32m     23\u001b[39m     \u001b[38;5;66;03m#print('Muestras de problemas antes (primeras 5 filas):')\u001b[39;00m\n\u001b[32m     24\u001b[39m     \u001b[38;5;66;03m#print( display(problemas_antes.head(5) if not problemas_antes.empty else 'No se detectaron filas con problemas.') )\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mdetectar_problemas_en_dataframe\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m     60\u001b[39m         info[\u001b[33m'\u001b[39m\u001b[33mgrupos_var_acentos\u001b[39m\u001b[33m'\u001b[39m]   = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     61\u001b[39m         info[\u001b[33m'\u001b[39m\u001b[33mejemplos_var_acentos\u001b[39m\u001b[33m'\u001b[39m] = {}\n\u001b[32m     62\u001b[39m     info[\u001b[33m'\u001b[39m\u001b[33mtokens_aparente_faltante\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mint\u001b[39m(\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43ms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msacar_acentos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mTOKENS_VALOR_FALTANTE\u001b[49m\u001b[43m)\u001b[49m.sum()\n\u001b[32m     64\u001b[39m     )\n\u001b[32m     65\u001b[39m     info[\u001b[33m'\u001b[39m\u001b[33mmuestras\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(s.dropna().unique()[:\u001b[32m10\u001b[39m])\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     67\u001b[39m     \u001b[38;5;66;03m# numeric\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:4711\u001b[39m, in \u001b[36mSeries.map\u001b[39m\u001b[34m(self, arg, na_action)\u001b[39m\n\u001b[32m   4631\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmap\u001b[39m(\n\u001b[32m   4632\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4633\u001b[39m     arg: Callable | Mapping | Series,\n\u001b[32m   4634\u001b[39m     na_action: Literal[\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   4635\u001b[39m ) -> Series:\n\u001b[32m   4636\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4637\u001b[39m \u001b[33;03m    Map values of Series according to an input mapping or function.\u001b[39;00m\n\u001b[32m   4638\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4709\u001b[39m \u001b[33;03m    dtype: object\u001b[39;00m\n\u001b[32m   4710\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4711\u001b[39m     new_values = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4712\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._constructor(new_values, index=\u001b[38;5;28mself\u001b[39m.index, copy=\u001b[38;5;28;01mFalse\u001b[39;00m).__finalize__(\n\u001b[32m   4713\u001b[39m         \u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mmap\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4714\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\site-packages\\pandas\\core\\base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mdetectar_problemas_en_dataframe.<locals>.<lambda>\u001b[39m\u001b[34m(v)\u001b[39m\n\u001b[32m     60\u001b[39m         info[\u001b[33m'\u001b[39m\u001b[33mgrupos_var_acentos\u001b[39m\u001b[33m'\u001b[39m]   = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     61\u001b[39m         info[\u001b[33m'\u001b[39m\u001b[33mejemplos_var_acentos\u001b[39m\u001b[33m'\u001b[39m] = {}\n\u001b[32m     62\u001b[39m     info[\u001b[33m'\u001b[39m\u001b[33mtokens_aparente_faltante\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mint\u001b[39m(\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         s.map(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mstr\u001b[39m(x).strip().lower()).map(\u001b[38;5;28;01mlambda\u001b[39;00m v: \u001b[43msacar_acentos\u001b[49m(v) \u001b[38;5;129;01min\u001b[39;00m TOKENS_VALOR_FALTANTE).sum()\n\u001b[32m     64\u001b[39m     )\n\u001b[32m     65\u001b[39m     info[\u001b[33m'\u001b[39m\u001b[33mmuestras\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(s.dropna().unique()[:\u001b[32m10\u001b[39m])\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     67\u001b[39m     \u001b[38;5;66;03m# numeric\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'sacar_acentos' is not defined"
     ]
    }
   ],
   "source": [
    "# ---------- Bucle/menu principal (usa dic_frames) ----------\n",
    "def menu_procesar_diccionario(dic_frames, reglas_por_archivo):\n",
    "    \"\"\"\n",
    "    Recorre dic_frames: clave = nombre_archivo (ej. 'marketing.csv'), valor = DataFrame.\n",
    "    Ejecuta: detectar_problemas_en_dataframe antes, limpiar_dataframe, detectar_problemas_en_dataframe despues,\n",
    "    imprime resúmenes y guarda cleaned en carpeta_limpios con sufijo ' limpio.csv'.\n",
    "    También sobreescribe variables en RAM (df_marketing, df_ventas, df_clientes) si se encuentran en el nombre.\n",
    "    \"\"\"\n",
    "    print(\"\"\"\\033[1;37;44m\\n\n",
    "╔═════════════════════════════════════════════════════════════════════════════╗\n",
    "║                     Aplico reglas según columna específica                  ║\n",
    "╠═════════════════════════════════════════════════════════════════════════════╣\n",
    "║     Afecta                                                                  ║\n",
    "║             Numéricos (int/float)                                           ║\n",
    "║             fechas --> YYYY,MM,DD                                           ║\n",
    "╚═════════════════════════════════════════════════════════════════════════════╝\\033[0;m\"\"\")\n",
    "    errores_df=pd.DataFrame()\n",
    "    # trabajamos sobre una copia para evitar modificar dict original por error\n",
    "    for [path_archivo, df_actual],[nombre_archivo,_] in zip( dic_frames.items() , reglas_por_archivo.items() ):\n",
    "        resumen_antes, chequeos_antes, problemas_antes = detectar_problemas_en_dataframe(df_actual)\n",
    "        print(f\"--- RESUMEN ANTES: {nombre_archivo} ---\")\n",
    "        # Para no volcar objetos muy grandes, mostramos el head del DataFrame de problemas (si existe)\n",
    "        #print('Muestras de problemas antes (primeras 5 filas):')\n",
    "        #print( display(problemas_antes.head(5) if not problemas_antes.empty else 'No se detectaron filas con problemas.') )\n",
    "\n",
    "        errores_df  = pd.concat ([errores_df,problemas_antes])\n",
    "\n",
    "        #df_actual[df_actual.duplicated(keep=False)].copy()\n",
    "        reglas    = reglas_por_archivo.get(nombre_archivo, {})\n",
    "        df_limpio = limpiar_dataframe(df_actual, reglas_por_columna=reglas)\n",
    "\n",
    "        resumen_despues, chequeos_despues, problemas_despues = detectar_problemas_en_dataframe(df_limpio)\n",
    "        '''\n",
    "        print(f\"n--- RESUMEN DESPUÉS: {nombre_archivo} ---\")\n",
    "        print(resumen_despues)\n",
    "        print('Muestras de problemas después (primeras 5 filas):')\n",
    "        print( display(problemas_despues.head(5) if not problemas_despues.empty else 'No se detectaron filas con problemas tras la limpieza.'))\n",
    "        '''\n",
    "        # mostrar separadores y tipo-nombre\n",
    "\n",
    "        #print('-'*100)\n",
    "        #print(f'nombre_archivo = {nombre_archivo}')\n",
    "        #print(f'type(nombre_archivo) = {type(nombre_archivo)}')\n",
    "        #print('-'*100)\n",
    "\n",
    "        # Guardo el archivo en limpios  ruta_base carpeta_reportes\n",
    "        nombre_limpio = nombre_archivo[:-4] + '_limpio.csv' if nombre_archivo.lower().endswith('.csv') else nombre_archivo + ' limpio.csv'\n",
    "        print(f'nombre_limpio = {nombre_limpio}')\n",
    "    \n",
    "        # guardar cleaned\n",
    "        ruta_guardado = carpeta_limpios / nombre_limpio\n",
    "        guardar_csv(df_limpio, ruta_guardado)\n",
    "        print(f'Guardado cleaned en: {ruta_guardado}')\n",
    "\n",
    "        # sobreescribir en RAM según el nombre\n",
    "        # (nota: usar globals() para actualizar variables en el entorno global del notebook)\n",
    "        dic_dfs[nombre_archivo.lower()] = df_limpio\n",
    "        '''\n",
    "        if 'marketing' in nombre_archivo.lower():\n",
    "            globals()['df_marketing'] = df_limpio\n",
    "        elif 'ventas' in nombre_archivo.lower():\n",
    "            globals()['df_ventas'] = df_limpio\n",
    "        elif 'clientes' in nombre_archivo.lower():\n",
    "            globals()['df_clientes'] = df_limpio\n",
    "        '''\n",
    "    print('Proceso completo del diccionario de DataFrames.')\n",
    "    return errores_df\n",
    "errores_df = menu_procesar_diccionario(dic_frames, reglas_por_archivo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f4ffe5-0f8d-465e-be9e-6d180fa42789",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#CCCCCC; padding:10px; border-radius:6px;\">\n",
    "<h2 style=\"color:black; text-align:center;\">Resultados de limpieza</h2>\n",
    "<p style=\"color:black;\">- Revisados los 3 csv  pasados a DataFrames.</p>\n",
    "<p style=\"color:blue;\">- DataFrames filtrados.</p>\n",
    "<p style=\"color:black;\">- Filtrado de Nulos.</p>\n",
    "<p style=\"color:black;\">- Filtrado de duplicados.</p>\n",
    "<p style=\"color:black;\">- Sin '', 'na', 'n/a', 'null', 'none', 'sin dato', 's/d', 'nd', '-', '--', '?', 'sin_dato', 'n/d'</p>    \n",
    "<p style=\"color:black;\">- Normalisados Strings segun reglas. Estilo (lower,string.upper) unicodedata.normalize('NFKD')</p>\n",
    "<p style=\"color:black;\">- Normalisados precios a float sin signo ($)</p>\n",
    "<p style=\"color:black;\">- Normalisados Numericos a int o float segun regla</p>    \n",
    "<p style=\"color:black;\">- Normalisados Fechas segun regla YYYY/MM/DD</p>    \n",
    "<p style=\"color:black;\">- Resguardo <code>datasets_salida/limpios/clientes_limpio.csv</code>.</p>\n",
    "<p style=\"color:black;\">- Resguardo <code>datasets_salida/limpios/marketing_limpio.csv</code>.</p>\n",
    "<p style=\"color:black;\">- Resguardo <code>datasets_salida/limpios/ventas_limpio.csv</code>.</p>\n",
    "<p style=\"color:blue;\">- Registros filtrados eliminados</p>\n",
    "<p style=\"color:black;\">- Resguardo <code>datasets_salida/reportes/reporte_limpieza.xlsx</code> con hojas (duplicados borrados, outliers, totales)</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1ab45f-4494-4b83-8470-20c6aab64b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Crear df_duplicados_total\n",
    "print (f\"\"\"\n",
    "{cantidad_duplicados=}\n",
    "\"\"\")\n",
    "# Filtra las filas donde la columna 'problemas' contiene la subcadena 'duplicado_exacto'\n",
    "df_duplicados_total = errores_df[errores_df['problemas'].str.contains('duplicado_exacto', case=False, na=False)].copy()\n",
    "\n",
    "# 2. Crear df_outliers_total\n",
    "# Filtra las filas donde la columna 'problemas' contiene la subcadena 'outlier_'\n",
    "df_outliers_total = errores_df[errores_df['problemas'].str.contains('outlier_', case=False, na=False)].copy()\n",
    "\n",
    "# 3. Crear df_resumen (Combinación y Ordenamiento)\n",
    "# Concatena los dos DataFrames creados\n",
    "df_resumen = pd.concat([df_duplicados_total, df_outliers_total])\n",
    "\n",
    "# Ordena el DataFrame resultante por la columna 'problemas'\n",
    "df_resumen = df_resumen.sort_values(by=\"problemas\").reset_index(drop=True)\n",
    "'''\n",
    "print (f\"\"\"\n",
    "df_duplicados_total\n",
    "{df_duplicados_total}\n",
    "\n",
    "{\"-\"*100}\n",
    "\n",
    "df_outliers_total\n",
    "{df_outliers_total}\n",
    "\n",
    "{\"-\"*100}\n",
    "df_resumen\n",
    "{df_resumen}\n",
    "{\"-\"*100}\n",
    "\"\"\")\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3a213a-5293-46f6-ad63-4cabc9e5b920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar en Excel con manejo de fallo si no existe el engine\n",
    "try:\n",
    "    with pd.ExcelWriter(ruta_excel, engine='openpyxl') as writer:\n",
    "        try:\n",
    "            df_duplicados_total.to_excel(writer, index=False, sheet_name='duplicados')\n",
    "        except Exception:\n",
    "            pd.DataFrame().to_excel(writer, index=False, sheet_name='duplicados')\n",
    "        try:\n",
    "            df_outliers_total.to_excel(writer, index=False, sheet_name='outliers')\n",
    "        except Exception:\n",
    "            pd.DataFrame().to_excel(writer, index=False, sheet_name='outliers')\n",
    "        try:\n",
    "            df_resumen.to_excel(writer, index=False, sheet_name='resumen_filtros')\n",
    "        except Exception:\n",
    "            pd.DataFrame(lista_resumen).to_excel(writer, index=False, sheet_name='resumen_filtros')\n",
    "    guardado_ok = True\n",
    "except Exception as e_openpyxl:\n",
    "    mensajes.append('Error usando openpyxl: ' + str(e_openpyxl))\n",
    "    try:\n",
    "        with pd.ExcelWriter(ruta_excel) as writer:\n",
    "            df_duplicados_total.to_excel(writer, index=False, sheet_name='duplicados')\n",
    "            df_outliers_total.to_excel(writer, index=False, sheet_name='outliers')\n",
    "            df_resumen.to_excel(writer, index=False, sheet_name='resumen_filtros')\n",
    "        guardado_ok = True\n",
    "    except Exception as e_default:\n",
    "        mensajes.append('Error sin engine: ' + str(e_default))\n",
    "        try:\n",
    "            df_duplicados_total.to_csv(carpeta_reportes / 'duplicados.csv', index=False, encoding='utf-8')\n",
    "            df_outliers_total.to_csv(carpeta_reportes / 'outliers.csv', index=False, encoding='utf-8')\n",
    "            df_resumen.to_csv(carpeta_reportes / 'resumen_filtros.csv', index=False, encoding='utf-8')\n",
    "            mensajes.append('Se guardaron CSVs separados como fallback.')\n",
    "            guardado_ok = True\n",
    "        except Exception as e_csv:\n",
    "            mensajes.append('Error guardando CSV fallback: ' + str(e_csv))\n",
    "            guardado_ok = False\n",
    "\n",
    "print('Guardado OK:', guardado_ok)\n",
    "if mensajes:\n",
    "    print('Mensajes/Errores durante guardado:')\n",
    "    for m in mensajes:\n",
    "        print('-', m)\n",
    "print('Ruta final esperada del Excel (si guardado):', ruta_excel)\n",
    "print('Resumen por dataset:')\n",
    "print(df_resumen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f43150b-eb7b-4ae8-a0a6-233f4b6ca5f1",
   "metadata": {},
   "source": [
    "ventas.csv  análisis de ventas, limpieza de datos y estadísticas descriptivas.\n",
    " \t\n",
    "clientes.csv  unirse a las ventas mediante el uso de funciones de combinación para analizar características de los clientes relacionados con sus \tcompras.\n",
    " \t\n",
    "marketing.csv analizar la efectividad de las campañas de marketing en las ventas y buscar correlaciones.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f5fcf9c-6140-4160-9ca0-ebf688f8c921",
   "metadata": {},
   "source": [
    "2) Claves candidatas para merge o concat\n",
    "\n",
    "Intersecciones observadas:\n",
    "\n",
    "ventas ∩ marketing → producto (clave natural para unir campañas con ventas por producto)\n",
    "\n",
    "ventas ∩ clientes → ninguna columna en común\n",
    "\n",
    "clientes ∩ marketing → ninguna columna en común\n",
    "\n",
    "Interpretación pedagógica:\n",
    "\n",
    "Puedes unir ventas con marketing por producto (p. ej. para ver qué canal promocionó qué producto y coste).\n",
    "\n",
    "No puedes unir directamente ventas con clientes porque ventas_limpio no contiene id_cliente ni email ni nombre_cliente. Para unir ventas↔clientes necesitas:\n",
    "\n",
    "que ventas_limpio tenga una columna id_cliente (recomendado), o\n",
    "\n",
    "un fichero/mapeo que vincule id_venta → id_cliente, o\n",
    "\n",
    "usar correspondencia por email/nombre (menos fiable) si esos campos existieran.\n",
    "\n",
    "3) Ejemplos de combinaciones útiles (casos prácticos)\n",
    "\n",
    "Ventas por canal de marketing (recomendado)\n",
    "\n",
    "Merge ventas_limpio ⟵ marketing_limpio por producto (left join): asignás a cada venta el canal y id_campanha. Luego agrupás por canal para métricas.\n",
    "\n",
    "Análisis de ticket promedio por categoría y canal\n",
    "\n",
    "Después del merge anterior, crear monto = precio * cantidad y agrupar por categoria y canal.\n",
    "\n",
    "Concatenación (vertical)\n",
    "\n",
    "Si tuvieras varios archivos de ventas de distintos periodos: pd.concat([ventas_periodo1, ventas_periodo2], axis=0).\n",
    "\n",
    "Unir clientes (si se dispone de id_cliente en ventas)\n",
    "\n",
    "ventas_limpio.merge(clientes_limpio, on='id_cliente', how='left') → permite segmentar ventas por edad, ciudad, ingresos.\n",
    "\n",
    "Si no hay id_cliente en ventas\n",
    "\n",
    "Crear un mapping table (archivo) que contenga id_venta → id_cliente y hacer merge por esa tabla.\n",
    "\n",
    "4) Código limpio en pandas (listo para ejecutar — adaptá nombres de columnas si quieres otro comportamiento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93885b49-e04a-401c-9c03-78439696cdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3) Tipos y limpiezas básicas\n",
    "ventas['fecha_venta'] = pd.to_datetime(ventas['fecha_venta'], errors='coerce')\n",
    "marketing['fecha_inicio'] = pd.to_datetime(marketing['fecha_inicio'], errors='coerce')\n",
    "marketing['fecha_fin'] = pd.to_datetime(marketing['fecha_fin'], errors='coerce')\n",
    "\n",
    "# Asegurar numéricos\n",
    "ventas['precio'] = pd.to_numeric(ventas['precio'], errors='coerce')\n",
    "ventas['cantidad'] = pd.to_numeric(ventas['cantidad'], errors='coerce')\n",
    "clientes['ingresos'] = pd.to_numeric(clientes['ingresos'], errors='coerce')\n",
    "marketing['costo'] = pd.to_numeric(marketing['costo'], errors='coerce')\n",
    "\n",
    "# 4) Crear columnas útiles\n",
    "ventas['monto'] = ventas['precio'] * ventas['cantidad']\n",
    "\n",
    "# 5) Merge ejemplo: ventas + marketing por 'producto' (asignar canal a cada venta)\n",
    "ventas_marketing = pd.merge(\n",
    "    ventas,\n",
    "    marketing[['producto', 'id_campanha', 'canal', 'costo', 'fecha_inicio', 'fecha_fin']],\n",
    "    on='producto',\n",
    "    how='left',   # left para conservar todas las ventas aunque no tengan campana asociada\n",
    "    validate='m:1'  # opcional: espera muchos registros ventas para 1 campaña por producto\n",
    ")\n",
    "\n",
    "# 6) Agregados: ventas por canal\n",
    "ventas_por_canal = (\n",
    "    ventas_marketing\n",
    "    .groupby('canal', dropna=False)\n",
    "    .agg(\n",
    "        total_monto=('monto', 'sum'),\n",
    "        cantidad_transacciones=('monto', 'count'),\n",
    "        ticket_promedio=('monto', 'mean')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# 7) Agregado: ventas por categoria y canal\n",
    "ventas_categoria_canal = (\n",
    "    ventas_marketing\n",
    "    .groupby(['categoria', 'canal'], dropna=False)\n",
    "    .agg(\n",
    "        total_monto     = ('monto', 'sum'),\n",
    "        transacciones   = ('monto', 'count'),\n",
    "        ticket_promedio = ('monto', 'mean')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# 8) Guardar resultados (opcional)\n",
    "ventas_por_canal.to_csv('/mnt/data/ventas_por_canal.csv', index=False)\n",
    "ventas_categoria_canal.to_csv('/mnt/data/ventas_categoria_canal.csv', index=False)\n",
    "\n",
    "# 9) ¿Y clientes? Si tienes id_cliente en ventas:\n",
    "# ventas_con_clientes = ventas.merge(clientes, on='id_cliente', how='left')\n",
    "\n",
    "# 10) Checks útiles\n",
    "# - Ver duplicados en claves: ventas['id_venta'].duplicated().sum()\n",
    "# - Ver clientes sin ventas: clientes[~clientes['id_cliente'].isin(ventas.get('id_cliente', []))]\n",
    "\n",
    "Recomendaciones prácticas (breves, accionables)\n",
    "\n",
    "Si querés unir ventas con clientes agregá id_cliente a ventas_limpio (registro en punto de venta o mapeo).\n",
    "\n",
    "Revisá duplicados en producto dentro de marketing (puede haber varias campañas por producto: decidir estrategia — por ejemplo filtrar la campaña activa por fecha).\n",
    "\n",
    "Elegí tipo de join con criterio pedagógico:\n",
    "\n",
    "left join para preservar todas las ventas (evitar perder datos).\n",
    "\n",
    "inner join si sólo te interesa el subset con campaña asociada.\n",
    "\n",
    "Creá fecha de periodo (día/semana/mes) para series temporales: ventas['mes'] = ventas['fecha_venta'].dt.to_period('M').\n",
    "\n",
    "Documentá supuestos: por qué usás how='left', cómo tratás ventas sin campaña, cómo imputás nulos en precio/cantidad.\n",
    "\n",
    "Ejemplo aplicado (interpretación cotidiana)\n",
    "\n",
    "Imaginá a Juan, vendedor en una pyme familiar con 2 hijos. Quiere saber si la campaña en Instagram está trayendo ventas: con el merge ventas + marketing por producto obtiene canal asignado a cada venta. Luego agrupa por canal y ve: “Instagram” tiene muchas visitas pero ticket promedio bajo — decisión: ajustar oferta o dirigir una campaña de cross-sell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40448d5e-62bf-4d58-9c5c-28da021964b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a mano\n",
    "ver que es lo que falta en drop na\n",
    "precios == 0 buscar en categoria el producto o promedo si no hay otro dato\n",
    "duplicate si el id es =\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21ab2c8-5a03-4fe6-9f02-60d2dc73d966",
   "metadata": {},
   "outputs": [],
   "source": [
    "productos mas vendidos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0737e69-7b61-4fa9-b678-3a178e09b62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ventas por mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d1d542-093a-49ad-b788-5e1b23af562d",
   "metadata": {},
   "outputs": [],
   "source": [
    "git remote set-url origin https://github.com/CursosAGT/nombre-nuevo.git\n",
    "git push -u origin main\n",
    "https://github.com/CursosAGT/-GarciaTrabaArielH-Comisi-n25262-TPI_Data_Analytics/blob/main/Garcia%20Traba%20Ariel%20H%20-%20Comisi%C3%B3n%2025262%20-%20TPI%20Data%20Analytics.ipynb\n",
    "Garcia Traba Ariel H - Comisión 25262 - TPI Data Analytics.ipynb\n",
    "https://github.com/CursosAGT/-GarciaTrabaArielH-Comisi-n25262-TPI_Data_Analytics/blob/main/datasets_entrada/clientes.csv\n",
    "https://github.com/CursosAGT/-GarciaTrabaArielH-Comisi-n25262-TPI_Data_Analytics/blob/main/datasets_entrada/marketing.csv\n",
    "https://github.com/CursosAGT/-GarciaTrabaArielH-Comisi-n25262-TPI_Data_Analytics/blob/main/datasets_entrada/ventas.csv.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
