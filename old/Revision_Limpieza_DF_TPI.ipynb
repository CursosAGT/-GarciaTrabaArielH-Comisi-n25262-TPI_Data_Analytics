{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2076de1",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color:#CCCCCC; padding:12px; border-radius:8px;\">\n",
    "<h1 style=\"color:blue; text-align:center;\">Revisión y limpieza de 3 DataFrames (TPI - Data Analytics)</h1>\n",
    "<p style=\"text-align:center;\"><em>Notebook docente en castellano — nombres descriptivos en snake_case — instrucciones y código listo para ejecutar</em></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f6277b",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color:#CCCCCC; padding:10px; border-radius:6px;\">\n",
    "<h2 style=\"color:blue; text-align:center;\">Resumen</h2>\n",
    "\n",
    "Este notebook está diseñado con **fines pedagógicos** para enseñar a revisar, normalizar y validar tres datasets almacenados en CSV:\n",
    "- `df_marketing.csv`  \n",
    "- `df_ventas.csv`  \n",
    "- `df_clientes.csv`  \n",
    "\n",
    "**Objetivos principales:**\n",
    "1. Revisar línea a línea cada DataFrame y detectar problemas (duplicados, espacios, acentos, tipos, nulos, outliers).  \n",
    "2. Aplicar reglas de limpieza reproducibles y parametrizables por columna.  \n",
    "3. Generar informes antes y después de la limpieza, y guardar los archivos limpios.  \n",
    "\n",
    "> Coloca los CSV en la carpeta `./data_in/`. El notebook usa rutas locales por defecto. Se conserva la celda comentada para Google Drive como referencia pedagógica.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "729c9856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rutas configuradas. Coloca los CSV en: D:\\Desktop\\Domingo\\df_caba_y_jupyter\\data_in\n"
     ]
    }
   ],
   "source": [
    "# Imports y configuración inicial\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import unicodedata\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "\n",
    "# Rutas locales\n",
    "carpeta_entrada = Path('./data_in')\n",
    "carpeta_entrada.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "carpeta_reportes = Path('./salida_reports/reports')\n",
    "carpeta_limpios = Path('./salida_reports/limpios')\n",
    "carpeta_reportes.mkdir(parents=True, exist_ok=True)\n",
    "carpeta_limpios.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "archivos_esperados = {\n",
    "    'df_marketing': carpeta_entrada / 'df_marketing.csv',\n",
    "    'df_ventas': carpeta_entrada / 'df_ventas.csv',\n",
    "    'df_clientes': carpeta_entrada / 'df_clientes.csv'\n",
    "}\n",
    "\n",
    "print('Rutas configuradas. Coloca los CSV en:', carpeta_entrada.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7c9b0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funciones utilitarias cargadas.\n"
     ]
    }
   ],
   "source": [
    "# ---------- Funciones utilitarias en castellano (snake_case) ----------\n",
    "\n",
    "def sacar_acentos(texto):\n",
    "    \"\"\"Elimina acentos (tildes) de un texto. Mantiene NaN intactos.\"\"\"\n",
    "    if pd.isna(texto):\n",
    "        return texto\n",
    "    texto = str(texto)\n",
    "    normalizado = unicodedata.normalize('NFKD', texto)\n",
    "    return ''.join([c for c in normalizado if not unicodedata.combining(c)])\n",
    "\n",
    "TOKENS_VALOR_FALTANTE = {\n",
    "    '', 'na', 'n/a', 'null', 'none', 'sin dato', 's/d', 'nd', '-', '--', '?', 'sin_dato', 'n/d'\n",
    "}\n",
    "\n",
    "def es_valor_faltante(valor):\n",
    "    \"\"\"Determina si un valor debe considerarse faltante (NaN).\"\"\"\n",
    "    if pd.isna(valor):\n",
    "        return True\n",
    "    s = str(valor).strip().lower()\n",
    "    s = sacar_acentos(s)\n",
    "    return s in TOKENS_VALOR_FALTANTE\n",
    "\n",
    "# IQR mask\n",
    "def mascara_valores_atipicos_rango_intercuartil(serie_datos):\n",
    "    serie_limpia = serie_datos.dropna().astype(float)\n",
    "    if serie_limpia.shape[0] < 4:\n",
    "        return pd.Series([False] * len(serie_datos), index=serie_datos.index)\n",
    "    q1 = serie_limpia.quantile(0.25)\n",
    "    q3 = serie_limpia.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    limite_inferior = q1 - 1.5 * iqr\n",
    "    limite_superior = q3 + 1.5 * iqr\n",
    "    return (serie_datos < limite_inferior) | (serie_datos > limite_superior)\n",
    "\n",
    "# Z-score mask\n",
    "def mascara_valores_atipicos_zscore(serie_datos, umbral=3.0):\n",
    "    serie_limpia = serie_datos.dropna().astype(float)\n",
    "    if serie_limpia.shape[0] < 4 or serie_limpia.std() == 0:\n",
    "        return pd.Series([False] * len(serie_datos), index=serie_datos.index)\n",
    "    puntaje_z = (serie_datos - serie_limpia.mean()) / serie_limpia.std()\n",
    "    return puntaje_z.abs() > umbral\n",
    "\n",
    "print('Funciones utilitarias cargadas.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bd88d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Función detectar_problemas_dataframe cargada.\n"
     ]
    }
   ],
   "source": [
    "# ---------- Detección de problemas en un DataFrame ----------\n",
    "def detectar_problemas_dataframe(df: pd.DataFrame):\n",
    "    resumen = {}\n",
    "    resumen['filas'] = df.shape[0]\n",
    "    resumen['columnas'] = df.shape[1]\n",
    "    resumen['nulos_por_columna'] = df.isna().sum().to_dict()\n",
    "    dup_mask = df.duplicated(keep=False)\n",
    "    resumen['duplicados_exactos'] = int(dup_mask.sum())\n",
    "\n",
    "    chequeos_por_columna = {}\n",
    "    for col in df.columns:\n",
    "        ser = df[col]\n",
    "        info = {'dtype': str(ser.dtype), 'nulos': int(ser.isna().sum())}\n",
    "        if ser.dtype == object or pd.api.types.is_string_dtype(ser):\n",
    "            s = ser.astype(str)\n",
    "            info['espacios_inicio'] = int(s.str.match(r'^\\s+').sum())\n",
    "            info['espacios_final'] = int(s.str.match(r'\\s+$').sum())\n",
    "            try:\n",
    "                unique_original = set(s.dropna().unique())\n",
    "                unique_lower = set(s.dropna().str.lower().unique())\n",
    "                info['unique_original'] = len(unique_original)\n",
    "                info['unique_lower'] = len(unique_lower)\n",
    "                info['variantes_mayusculas'] = len(unique_lower) < len(unique_original)\n",
    "            except Exception:\n",
    "                info['unique_original'] = ser.nunique(dropna=True)\n",
    "                info['unique_lower'] = None\n",
    "                info['variantes_mayusculas'] = None\n",
    "            try:\n",
    "                unaccented = s.dropna().map(lambda x: sacar_acentos(x).lower())\n",
    "                groups = unaccented.groupby(unaccented).size()\n",
    "                conflicts = groups[groups > 1]\n",
    "                info['grupos_variantes_acentos'] = int(conflicts.shape[0])\n",
    "                ejemplos = {}\n",
    "                if not conflicts.empty:\n",
    "                    for val in conflicts.index[:5]:\n",
    "                        originales = sorted(list(s[unaccented == val].unique())[:10])\n",
    "                        ejemplos[val] = originales\n",
    "                info['ejemplos_variantes_acentos'] = ejemplos\n",
    "            except Exception:\n",
    "                info['grupos_variantes_acentos'] = None\n",
    "                info['ejemplos_variantes_acentos'] = {}\n",
    "            info['tokens_aparente_faltante'] = int(s.map(lambda x: str(x).strip().lower()).map(lambda v: sacar_acentos(v) in TOKENS_VALOR_FALTANTE).sum())\n",
    "            info['muestras'] = list(s.dropna().unique()[:10])\n",
    "        else:\n",
    "            if pd.api.types.is_numeric_dtype(ser):\n",
    "                s_f = ser.dropna().astype(float)\n",
    "                info['media'] = float(s_f.mean()) if not s_f.empty else None\n",
    "                info['std'] = float(s_f.std()) if not s_f.empty else None\n",
    "                info['min'] = float(s_f.min()) if not s_f.empty else None\n",
    "                info['max'] = float(s_f.max()) if not s_f.empty else None\n",
    "                if len(s_f) >= 4:\n",
    "                    q1 = s_f.quantile(0.25)\n",
    "                    q3 = s_f.quantile(0.75)\n",
    "                    iqr = q3 - q1\n",
    "                    lb = q1 - 1.5 * iqr\n",
    "                    ub = q3 + 1.5 * iqr\n",
    "                    out_iqr = (s_f < lb) | (s_f > ub)\n",
    "                    info['outliers_iqr'] = int(out_iqr.sum())\n",
    "                    info['limites_iqr'] = (float(lb), float(ub))\n",
    "                else:\n",
    "                    info['outliers_iqr'] = None\n",
    "                    info['limites_iqr'] = None\n",
    "                if len(s_f) >= 4 and s_f.std() != 0:\n",
    "                    z = (s_f - s_f.mean()) / s_f.std()\n",
    "                    info['outliers_z'] = int((z.abs() > 3).sum())\n",
    "                else:\n",
    "                    info['outliers_z'] = None\n",
    "            else:\n",
    "                parsed = pd.to_datetime(ser, errors='coerce', dayfirst=True)\n",
    "                info['fechas_parseables'] = int(parsed.notna().sum())\n",
    "                info['muestras'] = list(ser.dropna().unique()[:10])\n",
    "        chequeos_por_columna[col] = info\n",
    "\n",
    "    filas_problemas = []\n",
    "    for idx, fila in df.iterrows():\n",
    "        lista_problemas = []\n",
    "        if dup_mask.loc[idx]:\n",
    "            lista_problemas.append('duplicado_exacto')\n",
    "        for col in df.columns:\n",
    "            val = fila[col]\n",
    "            # heuristicas textuales\n",
    "            if pd.api.types.is_string_dtype(type(val)) or isinstance(val, str) or (not pd.isna(val) and not pd.api.types.is_numeric_dtype(type(val)) and str(chequeos_por_columna[col].get('dtype','')).startswith('object')):\n",
    "                s = str(val)\n",
    "                if s != s.strip():\n",
    "                    lista_problemas.append(f'espacios_en_columna_{col}')\n",
    "                if chequeos_por_columna[col].get('variantes_mayusculas'):\n",
    "                    if s and s != s.lower() and s.lower() in [str(x).lower() for x in df[col].dropna().unique()]:\n",
    "                        lista_problemas.append(f'inconsistencia_mayusculas_columna_{col}')\n",
    "                if chequeos_por_columna[col].get('grupos_variantes_acentos') and chequeos_por_columna[col]['grupos_variantes_acentos'] > 0:\n",
    "                    try:\n",
    "                        un = sacar_acentos(s).lower()\n",
    "                        group_vals = [x for x in chequeos_por_columna[col].get('muestras', []) if sacar_acentos(str(x)).lower() == un]\n",
    "                        if group_vals and any(sacar_acentos(str(x)).lower() != sacar_acentos(s).lower() for x in group_vals):\n",
    "                            lista_problemas.append(f'variantes_acentos_columna_{col}')\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                if es_valor_faltante(s):\n",
    "                    lista_problemas.append(f'token_faltante_columna_{col}')\n",
    "            else:\n",
    "                try:\n",
    "                    fval = float(val)\n",
    "                    info_col = chequeos_por_columna[col]\n",
    "                    limites = info_col.get('limites_iqr')\n",
    "                    if limites and (fval < limites[0] or fval > limites[1]):\n",
    "                        lista_problemas.append(f'outlier_iqr_columna_{col}')\n",
    "                    if info_col.get('std') not in (None, 0):\n",
    "                        mean = info_col.get('media')\n",
    "                        std = info_col.get('std')\n",
    "                        if std and abs((fval - mean) / std) > 3:\n",
    "                            lista_problemas.append(f'outlier_z_columna_{col}')\n",
    "                except Exception:\n",
    "                    pass\n",
    "        if lista_problemas:\n",
    "            filas_problemas.append({'row_index': idx, 'problemas': ';'.join(sorted(set(lista_problemas))), 'muestra': json.dumps({str(c): str(fila[c]) for c in df.columns[:8]})})\n",
    "    df_problemas = pd.DataFrame(filas_problemas)\n",
    "    return resumen, chequeos_por_columna, df_problemas\n",
    "\n",
    "print('Función detectar_problemas_dataframe cargada.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47823495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funciones de limpieza cargadas.\n"
     ]
    }
   ],
   "source": [
    "# ---------- Funciones de limpieza ----------\n",
    "def aplicar_regla_columna(serie, regla):\n",
    "    tipo, opts = regla if isinstance(regla, tuple) else (regla, {})\n",
    "    s = serie.copy()\n",
    "    if tipo == 'strip':\n",
    "        s = s.map(lambda x: str(x).strip() if not pd.isna(x) else x)\n",
    "    elif tipo == 'lower':\n",
    "        s = s.map(lambda x: str(x).strip().lower() if not pd.isna(x) else x)\n",
    "    elif tipo == 'upper':\n",
    "        s = s.map(lambda x: str(x).strip().upper() if not pd.isna(x) else x)\n",
    "    elif tipo == 'title':\n",
    "        s = s.map(lambda x: str(x).strip().title() if not pd.isna(x) else x)\n",
    "    elif tipo == 'quitar_acentos':\n",
    "        s = s.map(lambda x: sacar_acentos(x).strip() if not pd.isna(x) else x)\n",
    "    elif tipo == 'numeric':\n",
    "        def to_num(v):\n",
    "            if pd.isna(v):\n",
    "                return np.nan\n",
    "            t = str(v).strip()\n",
    "            if opts.get('remove_non_digits', False):\n",
    "                t = ''.join([c for c in t if c.isdigit() or c in '.-'])\n",
    "            if opts.get('remove_thousands', False):\n",
    "                t = t.replace(',', '')\n",
    "            try:\n",
    "                return int(float(t)) if opts.get('as_int', False) else float(t)\n",
    "            except Exception:\n",
    "                return np.nan\n",
    "        s = s.map(to_num)\n",
    "    elif tipo == 'date':\n",
    "        def to_date(v):\n",
    "            if pd.isna(v):\n",
    "                return pd.NaT\n",
    "            t = str(v).strip()\n",
    "            if 'formats' in opts and opts['formats']:\n",
    "                for fmt in opts['formats']:\n",
    "                    try:\n",
    "                        return pd.to_datetime(pd.to_datetime(t, format=fmt, errors='coerce'))\n",
    "                    except Exception:\n",
    "                        continue\n",
    "            return pd.to_datetime(t, dayfirst=opts.get('dayfirst', True), errors='coerce')\n",
    "        s = s.map(to_date)\n",
    "    return s\n",
    "\n",
    "def limpiar_dataframe(df, reglas_por_columna=None):\n",
    "    reglas_por_columna = reglas_por_columna or {}\n",
    "    df2 = df.copy()\n",
    "    df2.columns = [str(c).strip() for c in df2.columns]\n",
    "    for col in df2.columns:\n",
    "        if col in reglas_por_columna:\n",
    "            df2[col] = aplicar_regla_columna(df2[col], reglas_por_columna[col])\n",
    "        else:\n",
    "            if df2[col].dtype == object or pd.api.types.is_string_dtype(df2[col]):\n",
    "                df2[col] = df2[col].map(lambda x: np.nan if es_valor_faltante(x) else (str(x).strip() if not pd.isna(x) else x))\n",
    "    df2 = df2.drop_duplicates(keep='first').reset_index(drop=True)\n",
    "    return df2\n",
    "\n",
    "print('Funciones de limpieza cargadas.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08b0250a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando: df_marketing.csv\n",
      "  - No encontrado: data_in\\df_marketing.csv. Coloca el CSV en D:\\Desktop\\Domingo\\df_caba_y_jupyter\\data_in y vuelve a ejecutar.\n",
      "Procesando: df_ventas.csv\n",
      "  - No encontrado: data_in\\df_ventas.csv. Coloca el CSV en D:\\Desktop\\Domingo\\df_caba_y_jupyter\\data_in y vuelve a ejecutar.\n",
      "Procesando: df_clientes.csv\n",
      "  - No encontrado: data_in\\df_clientes.csv. Coloca el CSV en D:\\Desktop\\Domingo\\df_caba_y_jupyter\\data_in y vuelve a ejecutar.\n",
      "\n",
      "Proceso finalizado. Revisa salida_reports para informes y archivos limpios.\n"
     ]
    }
   ],
   "source": [
    "# ---------- Proceso principal para los 3 CSV ----------\n",
    "reglas_ejemplo_marketing = {\n",
    "    'nombre': ('title', {}),\n",
    "    'email': ('lower', {}),\n",
    "    'fecha_registro': ('date', {'dayfirst': True, 'formats': ['%d/%m/%Y', '%Y-%m-%d']})\n",
    "}\n",
    "reglas_ejemplo_ventas = {\n",
    "    'monto': ('numeric', {'remove_thousands': True, 'as_int': False}),\n",
    "    'fecha_venta': ('date', {'dayfirst': True, 'formats': ['%d/%m/%Y', '%Y-%m-%d']})\n",
    "}\n",
    "reglas_ejemplo_clientes = {\n",
    "    'apellido': ('title', {}),\n",
    "    'dni': ('numeric', {'remove_non_digits': True, 'as_int': True})\n",
    "}\n",
    "reglas_por_archivo = {\n",
    "    'df_marketing.csv': reglas_ejemplo_marketing,\n",
    "    'df_ventas.csv': reglas_ejemplo_ventas,\n",
    "    'df_clientes.csv': reglas_ejemplo_clientes\n",
    "}\n",
    "\n",
    "reportes_creados = []\n",
    "zip_path = carpeta_limpios.parent / 'reports_dataset_tpi.zip'\n",
    "\n",
    "for nombre_logico, ruta_archivo in archivos_esperados.items():\n",
    "    nombre_archivo = ruta_archivo.name\n",
    "    print(f\"Procesando: {nombre_archivo}\")\n",
    "    if not ruta_archivo.exists():\n",
    "        print(f\"  - No encontrado: {ruta_archivo}. Coloca el CSV en {carpeta_entrada.resolve()} y vuelve a ejecutar.\")\n",
    "        continue\n",
    "    try:\n",
    "        df_original = pd.read_csv(ruta_archivo, dtype=str, keep_default_na=False, na_values=[''])\n",
    "    except Exception:\n",
    "        df_original = pd.read_csv(ruta_archivo, encoding='latin1', dtype=str, keep_default_na=False, na_values=[''])\n",
    "    resumen_antes, chequeos_antes, problemas_antes = detectar_problemas_dataframe(df_original)\n",
    "    ruta_reporte_antes = carpeta_reportes / f\"reporte_antes_limpieza_{nombre_archivo}.csv\"\n",
    "    problemas_antes.to_csv(ruta_reporte_antes, index=False, encoding='utf-8')\n",
    "    reportes_creados.append(ruta_reporte_antes)\n",
    "\n",
    "    reglas = reglas_por_archivo.get(nombre_archivo, {})\n",
    "    df_limpio = limpiar_dataframe(df_original, reglas_por_columna=reglas)\n",
    "\n",
    "    resumen_despues, chequeos_despues, problemas_despues = detectar_problemas_dataframe(df_limpio)\n",
    "    ruta_reporte_despues = carpeta_reportes / f\"reporte_despues_limpieza_{nombre_archivo}.csv\"\n",
    "    problemas_despues.to_csv(ruta_reporte_despues, index=False, encoding='utf-8')\n",
    "    reportes_creados.append(ruta_reporte_despues)\n",
    "\n",
    "    ruta_chequeos_antes = carpeta_reportes / f\"chequeos_antes_{nombre_archivo}.json\"\n",
    "    ruta_chequeos_despues = carpeta_reportes / f\"chequeos_despues_{nombre_archivo}.json\"\n",
    "    with open(ruta_chequeos_antes, 'w', encoding='utf-8') as fh:\n",
    "        json.dump(chequeos_antes, fh, ensure_ascii=False, indent=2)\n",
    "    with open(ruta_chequeos_despues, 'w', encoding='utf-8') as fh:\n",
    "        json.dump(chequeos_despues, fh, ensure_ascii=False, indent=2)\n",
    "    reportes_creados.extend([ruta_chequeos_antes, ruta_chequeos_despues])\n",
    "\n",
    "    resumen_path = carpeta_reportes / f\"summary_{nombre_archivo}.json\"\n",
    "    resumen_guardar = {\n",
    "        'archivo': nombre_archivo,\n",
    "        'filas_antes': resumen_antes.get('filas'),\n",
    "        'filas_despues': resumen_despues.get('filas'),\n",
    "        'duplicados_antes': resumen_antes.get('duplicados_exactos'),\n",
    "        'nulos_antes': resumen_antes.get('nulos_por_columna'),\n",
    "        'filas_con_problemas_antes': len(problemas_antes),\n",
    "        'filas_con_problemas_despues': len(problemas_despues)\n",
    "    }\n",
    "    with open(resumen_path, 'w', encoding='utf-8') as fh:\n",
    "        json.dump(resumen_guardar, fh, ensure_ascii=False, indent=2)\n",
    "    reportes_creados.append(resumen_path)\n",
    "\n",
    "    ruta_cleaned = carpeta_limpios / f\"cleaned_{nombre_archivo}\"\n",
    "    df_limpio.to_csv(ruta_cleaned, index=False, encoding='utf-8')\n",
    "\n",
    "# Empaquetar\n",
    "with zipfile.ZipFile(zip_path, 'w', compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    for p in reportes_creados:\n",
    "        zf.write(p, arcname=os.path.join('reports', p.name))\n",
    "    for p in carpeta_limpios.glob('cleaned_*.csv'):\n",
    "        zf.write(p, arcname=os.path.join('limpios', p.name))\n",
    "\n",
    "print('\\nProceso finalizado. Revisa salida_reports para informes y archivos limpios.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e449e1b",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color:#f0f8ff; padding:10px; border-radius:6px;\">\n",
    "<h2 style=\"color:blue; text-align:center;\">Notas pedagógicas y siguientes pasos</h2>\n",
    "\n",
    "- Revisa los archivos `reporte_antes_limpieza_<archivo>.csv` para ver ejemplos por fila y decidir reglas adicionales.  \n",
    "- Modifica `reglas_por_archivo` según los nombres reales de las columnas en tus CSV.  \n",
    "- Si quieres que ejecute el notebook aquí, sube los 3 CSV a `./data_in/` y te muestro los resultados.  \n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
