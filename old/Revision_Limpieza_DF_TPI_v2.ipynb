{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cedbb1b4",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color:#CCCCCC; padding:12px; border-radius:8px;\">\n",
    "<h1 style=\"color:#003366; text-align:center; margin:8px 0;\">Revisión y limpieza de 3 DataFrames (TPI - Data Analytics)</h1>\n",
    "<p style=\"text-align:center; color:#003366; margin:0;\"><em>Notebook docente en castellano — nombres descriptivos en snake_case — código y documentación</em></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c723b86a",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color:#CCCCCC; padding:10px; border-radius:6px;\">\n",
    "<h2 style=\"color:black; text-align:center; margin-top:6px;\">Resumen</h2>\n",
    "\n",
    "<p style=\"color:black;\">\n",
    "Este notebook está diseñado con finalidades pedagógicas. Revisa, normaliza y valida tres datasets contenidos en CSV:\n",
    "</p>\n",
    "\n",
    "<ul style=\"color:black;\">\n",
    "<li><code>marketing.csv</code> → variable: <code>df_marketing</code></li>\n",
    "<li><code>ventas.csv</code> → variable: <code>df_ventas</code></li>\n",
    "<li><code>clientes.csv</code> → variable: <code>df_clientes</code></li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"color:black;\">\n",
    "Coloca los CSV en <code>./data_in/</code> o en <code>/mnt/data/</code>. El notebook busca primero en <code>./data_in/</code> y si no encuentra, usa <code>/mnt/data/</code> (útil para entornos donde los archivos están pre-subidos).\n",
    "</p>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be46fb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuración de rutas lista.\n",
      "Coloca los CSV en ./data_in/ o en /mnt/data/ (ya están comprobadas ambas rutas).\n"
     ]
    }
   ],
   "source": [
    "# Imports y configuración inicial (nombres en castellano)\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import unicodedata\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Rutas: busca en ./data_in primero, si no existe usa /mnt/data\n",
    "\n",
    "# Rutas locales\n",
    "\n",
    "carpeta_entrada_local = Path('./data_in')\n",
    "carpeta_entrada_mnt = Path('/mnt/data')\n",
    "carpeta_reportes = Path('./reportes')\n",
    "carpeta_limpios = carpeta_reportes / 'limpios'\n",
    "carpeta_reportes.mkdir(parents=True, exist_ok=True)\n",
    "carpeta_limpios.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Nombres esperados de archivos\n",
    "archivo_marketing = 'marketing.csv'\n",
    "archivo_ventas = 'ventas.csv'\n",
    "archivo_clientes = 'clientes.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21030142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funciones utilitarias definidas.\n"
     ]
    }
   ],
   "source": [
    "# ---------- Funciones utilitarias en castellano (snake_case) ----------\n",
    "def sacar_acentos(texto):\n",
    "    \"\"\"Elimina acentos (tildes) de un texto. Mantiene NaN intactos.\"\"\"\n",
    "    if pd.isna(texto):\n",
    "        return texto\n",
    "    texto = str(texto)\n",
    "    normalizado = unicodedata.normalize('NFKD', texto)\n",
    "    return ''.join([c for c in normalizado if not unicodedata.combining(c)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aefb8b-3e14-4917-9986-8b7f27033cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENS_VALOR_FALTANTE = {\n",
    "    '', 'na', 'n/a', 'null', 'none', 'sin dato', 's/d', 'nd', '-', '--', '?', 'sin_dato', 'n/d'\n",
    "}\n",
    "\n",
    "def es_valor_faltante(valor):\n",
    "    \"\"\"Determina si un valor debe considerarse faltante (NaN).\"\"\"\n",
    "    if pd.isna(valor):\n",
    "        return True\n",
    "    s = str(valor).strip().lower()\n",
    "    s = sacar_acentos(s)\n",
    "    return s in TOKENS_VALOR_FALTANTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4634aeae-c21a-47dc-9bb9-cffdeafcbe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detección de outliers (IQR)\n",
    "def mascara_valores_atipicos_rango_intercuartil(serie_datos):\n",
    "    serie_limpia = serie_datos.dropna().astype(float)\n",
    "    if serie_limpia.shape[0] < 4:\n",
    "        return pd.Series([False] * len(serie_datos), index=serie_datos.index)\n",
    "    q1 = serie_limpia.quantile(0.25)\n",
    "    q3 = serie_limpia.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    limite_inferior = q1 - 1.5 * iqr\n",
    "    limite_superior = q3 + 1.5 * iqr\n",
    "    return (serie_datos < limite_inferior) | (serie_datos > limite_superior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c15444c-3df6-409e-a932-c08afaebfd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detección de outliers (Z-score)\n",
    "def mascara_valores_atipicos_zscore(serie_datos, umbral=3.0):\n",
    "    serie_limpia = serie_datos.dropna().astype(float)\n",
    "    if serie_limpia.shape[0] < 4 or serie_limpia.std() == 0:\n",
    "        return pd.Series([False] * len(serie_datos), index=serie_datos.index)\n",
    "    puntaje_z = (serie_datos - serie_limpia.mean()) / serie_limpia.std()\n",
    "    return puntaje_z.abs() > umbral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32837ebb-e71d-4c31-bcfc-9e747390519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Funciones utilitarias definidas.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f411cb90-17af-45f2-9dfb-9fe7534bacbb",
   "metadata": {},
   "source": [
    "# limpieza y normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe131403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Función detectar_problemas_en_dataframe cargada.\n"
     ]
    }
   ],
   "source": [
    "# ---------- Detección de problemas en un DataFrame ----------\n",
    "def detectar_problemas_en_dataframe(df: pd.DataFrame):\n",
    "    \"\"\"Detecta problemas comunes en un DataFrame y retorna:\n",
    "    - resumen: diccionario con métricas generales\n",
    "    - chequeos_por_columna: dict con información por columna\n",
    "    - problemas_df: DataFrame con filas problemáticas y descripción\n",
    "    \"\"\"\n",
    "    resumen = {}\n",
    "    resumen['filas'] = df.shape[0]\n",
    "    resumen['columnas'] = df.shape[1]\n",
    "    resumen['nulos_por_columna'] = df.isna().sum().to_dict()\n",
    "    dup_mask = df.duplicated(keep=False)\n",
    "    resumen['duplicados_exactos'] = int(dup_mask.sum())\n",
    "\n",
    "    chequeos_por_columna = {}\n",
    "    for col in df.columns:\n",
    "        ser = df[col]\n",
    "        info = {'dtype': str(ser.dtype), 'nulos': int(ser.isna().sum())}\n",
    "        if ser.dtype == object or pd.api.types.is_string_dtype(ser):\n",
    "            s = ser.astype(str)\n",
    "            info['espacios_inicio'] = int(s.str.match(r'^\\s+').sum())\n",
    "            info['espacios_final'] = int(s.str.match(r'\\s+$').sum())\n",
    "            try:\n",
    "                unique_original = set(s.dropna().unique())\n",
    "                unique_lower = set(s.dropna().str.lower().unique())\n",
    "                info['unique_original'] = len(unique_original)\n",
    "                info['unique_lower'] = len(unique_lower)\n",
    "                info['variantes_mayusculas'] = len(unique_lower) < len(unique_original)\n",
    "            except Exception:\n",
    "                info['unique_original'] = ser.nunique(dropna=True)\n",
    "                info['unique_lower'] = None\n",
    "                info['variantes_mayusculas'] = None\n",
    "            try:\n",
    "                unaccented = s.dropna().map(lambda x: sacar_acentos(x).lower())\n",
    "                groups = unaccented.groupby(unaccented).size()\n",
    "                conflicts = groups[groups > 1]\n",
    "                info['grupos_variantes_acentos'] = int(conflicts.shape[0])\n",
    "                ejemplos = {}\n",
    "                if not conflicts.empty:\n",
    "                    for val in conflicts.index[:5]:\n",
    "                        originales = sorted(list(s[unaccented == val].unique())[:10])\n",
    "                        ejemplos[val] = originales\n",
    "                info['ejemplos_variantes_acentos'] = ejemplos\n",
    "            except Exception:\n",
    "                info['grupos_variantes_acentos'] = None\n",
    "                info['ejemplos_variantes_acentos'] = {}\n",
    "            info['tokens_aparente_faltante'] = int(s.map(lambda x: str(x).strip().lower()).map(lambda v: sacar_acentos(v) in TOKENS_VALOR_FALTANTE).sum())\n",
    "            info['muestras'] = list(s.dropna().unique()[:10])\n",
    "        else:\n",
    "            if pd.api.types.is_numeric_dtype(ser):\n",
    "                s_f = ser.dropna().astype(float)\n",
    "                info['media'] = float(s_f.mean()) if not s_f.empty else None\n",
    "                info['std'] = float(s_f.std()) if not s_f.empty else None\n",
    "                info['min'] = float(s_f.min()) if not s_f.empty else None\n",
    "                info['max'] = float(s_f.max()) if not s_f.empty else None\n",
    "                if len(s_f) >= 4:\n",
    "                    q1 = s_f.quantile(0.25)\n",
    "                    q3 = s_f.quantile(0.75)\n",
    "                    iqr = q3 - q1\n",
    "                    lb = q1 - 1.5 * iqr\n",
    "                    ub = q3 + 1.5 * iqr\n",
    "                    out_iqr = (s_f < lb) | (s_f > ub)\n",
    "                    info['outliers_iqr'] = int(out_iqr.sum())\n",
    "                    info['limites_iqr'] = (float(lb), float(ub))\n",
    "                else:\n",
    "                    info['outliers_iqr'] = None\n",
    "                    info['limites_iqr'] = None\n",
    "                if len(s_f) >= 4 and s_f.std() != 0:\n",
    "                    z = (s_f - s_f.mean()) / s_f.std()\n",
    "                    info['outliers_z'] = int((z.abs() > 3).sum())\n",
    "                else:\n",
    "                    info['outliers_z'] = None\n",
    "            else:\n",
    "                parsed = pd.to_datetime(ser, errors='coerce', dayfirst=True)\n",
    "                info['fechas_parseables'] = int(parsed.notna().sum())\n",
    "                info['muestras'] = list(ser.dropna().unique()[:10])\n",
    "        chequeos_por_columna[col] = info\n",
    "\n",
    "    filas_problemas = []\n",
    "    for idx, fila in df.iterrows():\n",
    "        lista_problemas = []\n",
    "        if dup_mask.loc[idx]:\n",
    "            lista_problemas.append('duplicado_exacto')\n",
    "        for col in df.columns:\n",
    "            val = fila[col]\n",
    "            # heuristicas textuales\n",
    "            if pd.api.types.is_string_dtype(type(val)) or isinstance(val, str) or (not pd.isna(val) and not pd.api.types.is_numeric_dtype(type(val)) and str(chequeos_por_columna[col].get('dtype','')).startswith('object')):\n",
    "                s = str(val)\n",
    "                if s != s.strip():\n",
    "                    lista_problemas.append(f'espacios_en_columna_{col}')\n",
    "                if chequeos_por_columna[col].get('variantes_mayusculas'):\n",
    "                    if s and s != s.lower() and s.lower() in [str(x).lower() for x in df[col].dropna().unique()]:\n",
    "                        lista_problemas.append(f'inconsistencia_mayusculas_columna_{col}')\n",
    "                if chequeos_por_columna[col].get('grupos_variantes_acentos') and chequeos_por_columna[col]['grupos_variantes_acentos'] > 0:\n",
    "                    try:\n",
    "                        un = sacar_acentos(s).lower()\n",
    "                        group_vals = [x for x in chequeos_por_columna[col].get('muestras', []) if sacar_acentos(str(x)).lower() == un]\n",
    "                        if group_vals and any(sacar_acentos(str(x)).lower() != sacar_acentos(s).lower() for x in group_vals):\n",
    "                            lista_problemas.append(f'variantes_acentos_columna_{col}')\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                if es_valor_faltante(s):\n",
    "                    lista_problemas.append(f'token_faltante_columna_{col}')\n",
    "            else:\n",
    "                try:\n",
    "                    fval = float(val)\n",
    "                    info_col = chequeos_por_columna[col]\n",
    "                    limites = info_col.get('limites_iqr')\n",
    "                    if limites and (fval < limites[0] or fval > limites[1]):\n",
    "                        lista_problemas.append(f'outlier_iqr_columna_{col}')\n",
    "                    if info_col.get('std') not in (None, 0):\n",
    "                        mean = info_col.get('media')\n",
    "                        std = info_col.get('std')\n",
    "                        if std and abs((fval - mean) / std) > 3:\n",
    "                            lista_problemas.append(f'outlier_z_columna_{col}')\n",
    "                except Exception:\n",
    "                    pass\n",
    "        if lista_problemas:\n",
    "            filas_problemas.append({'row_index': idx, 'problemas': ';'.join(sorted(set(lista_problemas))), 'muestra': json.dumps({str(c): str(fila[c]) for c in df.columns[:8]})})\n",
    "    df_problemas = pd.DataFrame(filas_problemas)\n",
    "    return resumen, chequeos_por_columna, df_problemas\n",
    "\n",
    "print('Función detectar_problemas_en_dataframe cargada.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddbaa5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funciones de limpieza cargadas.\n"
     ]
    }
   ],
   "source": [
    "# ---------- Funciones de limpieza ----------\n",
    "def aplicar_regla_columna(serie, regla):\n",
    "    tipo, opts = regla if isinstance(regla, tuple) else (regla, {})\n",
    "    s = serie.copy()\n",
    "    if tipo == 'strip':\n",
    "        s = s.map(lambda x: str(x).strip() if not pd.isna(x) else x)\n",
    "    elif tipo == 'lower':\n",
    "        s = s.map(lambda x: str(x).strip().lower() if not pd.isna(x) else x)\n",
    "    elif tipo == 'upper':\n",
    "        s = s.map(lambda x: str(x).strip().upper() if not pd.isna(x) else x)\n",
    "    elif tipo == 'title':\n",
    "        s = s.map(lambda x: str(x).strip().title() if not pd.isna(x) else x)\n",
    "    elif tipo == 'quitar_acentos':\n",
    "        s = s.map(lambda x: sacar_acentos(x).strip() if not pd.isna(x) else x)\n",
    "    elif tipo == 'numeric':\n",
    "        def to_num(v):\n",
    "            if pd.isna(v):\n",
    "                return np.nan\n",
    "            t = str(v).strip()\n",
    "            if opts.get('remove_non_digits', False):\n",
    "                t = ''.join([c for c in t if c.isdigit() or c in '.-'])\n",
    "            if opts.get('remove_thousands', False):\n",
    "                t = t.replace(',', '')\n",
    "            try:\n",
    "                return int(float(t)) if opts.get('as_int', False) else float(t)\n",
    "            except Exception:\n",
    "                return np.nan\n",
    "        s = s.map(to_num)\n",
    "    elif tipo == 'date':\n",
    "        def to_date(v):\n",
    "            if pd.isna(v):\n",
    "                return pd.NaT\n",
    "            t = str(v).strip()\n",
    "            if 'formats' in opts and opts['formats']:\n",
    "                for fmt in opts['formats']:\n",
    "                    try:\n",
    "                        return pd.to_datetime(pd.to_datetime(t, format=fmt, errors='coerce'))\n",
    "                    except Exception:\n",
    "                        continue\n",
    "            return pd.to_datetime(t, dayfirst=opts.get('dayfirst', True), errors='coerce')\n",
    "        s = s.map(to_date)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24848b21-01d9-4de3-903f-9137a292eef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_dataframe(df, reglas_por_columna=None):\n",
    "    reglas_por_columna = reglas_por_columna or {}\n",
    "    df2 = df.copy()\n",
    "    df2.columns = [str(c).strip() for c in df2.columns]\n",
    "    for col in df2.columns:\n",
    "        if col in reglas_por_columna:\n",
    "            df2[col] = aplicar_regla_columna(df2[col], reglas_por_columna[col])\n",
    "        else:\n",
    "            if df2[col].dtype == object or pd.api.types.is_string_dtype(df2[col]):\n",
    "                df2[col] = df2[col].map(lambda x: np.nan if es_valor_faltante(x) else (str(x).strip() if not pd.isna(x) else x))\n",
    "    df2 = df2.drop_duplicates(keep='first').reset_index(drop=True)\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb47523-22aa-403a-bfd2-82f3ae8fa01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Funciones de limpieza cargadas.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "895c63d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     27\u001b[39m     ruta.parent.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     28\u001b[39m     df.to_csv(ruta, index=\u001b[38;5;28;01mFalse\u001b[39;00m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     30\u001b[39m archivos = {\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmarketing.csv\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mruta_entrada\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmarketing.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[32m     32\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mventas.csv\u001b[39m\u001b[33m'\u001b[39m: ruta_entrada(\u001b[33m'\u001b[39m\u001b[33mventas.csv\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     33\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mclientes.csv\u001b[39m\u001b[33m'\u001b[39m: ruta_entrada(\u001b[33m'\u001b[39m\u001b[33mclientes.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     34\u001b[39m }\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m nombre_archivo, ruta_archivo \u001b[38;5;129;01min\u001b[39;00m archivos.items():\n\u001b[32m     37\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mProcesando: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnombre_archivo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mruta_entrada\u001b[39m\u001b[34m(nombre_archivo)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mruta_entrada\u001b[39m(nombre_archivo):\n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# Prioriza ./data_in, luego /mnt/data\u001b[39;00m\n\u001b[32m     26\u001b[39m     carpeta_entrada_local=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     p_local = \u001b[43mcarpeta_entrada_local\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mnombre_archivo\u001b[49m\n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m#p_local =  nombre_archivo\u001b[39;00m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m p_local.exists():\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for /: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "# ---------- Proceso principal para los 3 CSV ----------\n",
    "reglas_ejemplo_marketing = {\n",
    "    'nombre': ('title', {}),\n",
    "    'email': ('lower', {}),\n",
    "    'fecha_registro': ('date', {'dayfirst': True, 'formats': ['%d/%m/%Y', '%Y-%m-%d']})\n",
    "}\n",
    "reglas_ejemplo_ventas = {\n",
    "    'monto': ('numeric', {'remove_thousands': True, 'as_int': False}),\n",
    "    'fecha_venta': ('date', {'dayfirst': True, 'formats': ['%d/%m/%Y', '%Y-%m-%d']})\n",
    "}\n",
    "reglas_ejemplo_clientes = {\n",
    "    'apellido': ('title', {}),\n",
    "    'dni': ('numeric', {'remove_non_digits': True, 'as_int': True})\n",
    "}\n",
    "reglas_por_archivo = {\n",
    "    'marketing.csv': reglas_ejemplo_marketing,\n",
    "    'ventas.csv': reglas_ejemplo_ventas,\n",
    "    'clientes.csv': reglas_ejemplo_clientes\n",
    "}\n",
    "\n",
    "reportes_creados = []\n",
    "zip_path = carpeta_limpios.parent / 'reports_dataset_tpi_v2.zip'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e42599a-abe2-4447-b8e6-6c1024f0d651",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Función auxiliar para guardar CSVs\n",
    "def guardar_csv(df, ruta):\n",
    "    ruta = Path(ruta)\n",
    "    ruta.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(ruta, index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a416838-290d-41c6-9c25-772836830b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "archivos = {\n",
    "    'marketing.csv': ruta_entrada('marketing.csv'),\n",
    "    'ventas.csv': ruta_entrada('ventas.csv'),\n",
    "    'clientes.csv': ruta_entrada('clientes.csv')\n",
    "}\n",
    "\n",
    "for nombre_archivo, ruta_archivo in archivos.items():\n",
    "    print(f'Procesando: {nombre_archivo}')\n",
    "    if ruta_archivo is None:\n",
    "        print(f'  - No se encontró {nombre_archivo} en ./data_in/ ni en /mnt/data/. Saltando.')\n",
    "        continue\n",
    "    try:\n",
    "        df_original = pd.read_csv(ruta_archivo, dtype=str, keep_default_na=False, na_values=[''])\n",
    "    except Exception:\n",
    "        df_original = pd.read_csv(ruta_archivo, encoding='latin1', dtype=str, keep_default_na=False, na_values=[''])\n",
    "    # Variables lógicas en castellano\n",
    "    if nombre_archivo == 'marketing.csv':\n",
    "        df_marketing = df_original.copy()\n",
    "        df_actual = df_marketing\n",
    "    elif nombre_archivo == 'ventas.csv':\n",
    "        df_ventas = df_original.copy()\n",
    "        df_actual = df_ventas\n",
    "    else:\n",
    "        df_clientes = df_original.copy()\n",
    "        df_actual = df_clientes\n",
    "\n",
    "    resumen_antes, chequeos_antes, problemas_antes = detectar_problemas_en_dataframe(df_actual)\n",
    "    ruta_reporte_antes = carpeta_reportes / f'reporte_antes_limpieza_{nombre_archivo}.csv'\n",
    "    guardar_csv(problemas_antes, ruta_reporte_antes)\n",
    "    reportes_creados.append(ruta_reporte_antes)\n",
    "\n",
    "    reglas = reglas_por_archivo.get(nombre_archivo, {})\n",
    "    df_limpio = limpiar_dataframe(df_actual, reglas_por_columna=reglas)\n",
    "\n",
    "    resumen_despues, chequeos_despues, problemas_despues = detectar_problemas_en_dataframe(df_limpio)\n",
    "    ruta_reporte_despues = carpeta_reportes / f'reporte_despues_limpieza_{nombre_archivo}.csv'\n",
    "    guardar_csv(problemas_despues, ruta_reporte_despues)\n",
    "    reportes_creados.append(ruta_reporte_despues)\n",
    "\n",
    "    ruta_chequeos_antes = carpeta_reportes / f'chequeos_antes_{nombre_archivo}.json'\n",
    "    ruta_chequeos_despues = carpeta_reportes / f'chequeos_despues_{nombre_archivo}.json'\n",
    "    with open(ruta_chequeos_antes, 'w', encoding='utf-8') as fh:\n",
    "        json.dump(chequeos_antes, fh, ensure_ascii=False, indent=2)\n",
    "    with open(ruta_chequeos_despues, 'w', encoding='utf-8') as fh:\n",
    "        json.dump(chequeos_despues, fh, ensure_ascii=False, indent=2)\n",
    "    reportes_creados.extend([ruta_chequeos_antes, ruta_chequeos_despues])\n",
    "\n",
    "    resumen_path = carpeta_reportes / f'summary_{nombre_archivo}.json'\n",
    "    resumen_guardar = {\n",
    "        'archivo': nombre_archivo,\n",
    "        'filas_antes': resumen_antes.get('filas'),\n",
    "        'filas_despues': resumen_despues.get('filas'),\n",
    "        'duplicados_antes': resumen_antes.get('duplicados_exactos'),\n",
    "        'nulos_antes': resumen_antes.get('nulos_por_columna'),\n",
    "        'filas_con_problemas_antes': len(problemas_antes),\n",
    "        'filas_con_problemas_despues': len(problemas_despues)\n",
    "    }\n",
    "    with open(resumen_path, 'w', encoding='utf-8') as fh:\n",
    "        json.dump(resumen_guardar, fh, ensure_ascii=False, indent=2)\n",
    "    reportes_creados.append(resumen_path)\n",
    "\n",
    "    ruta_cleaned = carpeta_limpios / f'cleaned_{nombre_archivo}'\n",
    "    guardar_csv(df_limpio, ruta_cleaned)\n",
    "\n",
    "# Empaquetar reportes y limpios\n",
    "with zipfile.ZipFile(zip_path, 'w', compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    for p in reportes_creados:\n",
    "        if Path(p).exists():\n",
    "            zf.write(p, arcname=os.path.join('reports', Path(p).name))\n",
    "    for p in carpeta_limpios.glob('cleaned_*.csv'):\n",
    "        zf.write(p, arcname=os.path.join('limpios', p.name))\n",
    "\n",
    "print('\\nProceso finalizado. Revisa la carpeta reportes para los CSV y JSON generados.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94f394f",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color:#CCCCCC; padding:10px; border-radius:6px;\">\n",
    "<h2 style=\"color:black; text-align:center;\">Notas pedagógicas y siguientes pasos</h2>\n",
    "\n",
    "<p style=\"color:black;\">- Revisa los archivos <code>reporte_antes_limpieza_<archivo>.csv</code> para ver ejemplos por fila y decidir reglas adicionales.</p>\n",
    "<p style=\"color:black;\">- Ajusta <code>reglas_por_archivo</code> según los nombres reales de las columnas en tus CSV.</p>\n",
    "<p style=\"color:black;\">- Si querés que ejecute el notebook aquí y muestre resultados, subí los 3 CSV a <code>./data_in/</code> o confirma que están en <code>/mnt/data/</code>.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50338261-fcc2-4dda-85b8-d6c12320e8fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f895b5b-5c12-442f-abf1-4b7a9d3ac96d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
