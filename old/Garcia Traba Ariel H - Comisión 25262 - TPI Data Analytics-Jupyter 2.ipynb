\"\"\"\n",
    "Preparacion de archivos CSV en la carpeta de entrada.\n",
    "\n",
    "Proposito:\n",
    "    Guardar los DataFrames cargados en memoria como archivos CSV\n",
    "    en la carpeta de entrada para que puedan ser procesados\n",
    "    por el pipeline automatizado.\n",
    "\n",
    "Proceso:\n",
    "    1. Verificar que los DataFrames esten cargados\n",
    "    2. Guardar cada DataFrame como CSV en ruta_entrada\n",
    "    3. Confirmar creacion de archivos\n",
    "\n",
    "NOTA: Este paso es necesario para simular un escenario real\n",
    "      donde los archivos ya existen en disco.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Guardando DataFrames como archivos CSV...\")\n",
    "print(f\"Carpeta destino: {ruta_entrada}\")\n",
    "\n",
    "try:\n",
    "    # Guardar cada dataset\n",
    "    df_ventas.to_csv(os.path.join(ruta_entrada, \"ventas.csv\"), index=False)\n",
    "    print(\"  - ventas.csv guardado correctamente\")\n",
    "    \n",
    "    df_clientes.to_csv(os.path.join(ruta_entrada, \"clientes.csv\"), index=False)\n",
    "    print(\"  - clientes.csv guardado correctamente\")\n",
    "    \n",
    "    df_marketing.to_csv(os.path.join(ruta_entrada, \"marketing.csv\"), index=False)\n",
    "    print(\"  - marketing.csv guardado correctamente\")\n",
    "    \n",
    "    print(\"\\nArchivos de entrada preparados exitosamente\")\n",
    "    \n",
    "    # Listar archivos en la carpeta de entrada\n",
    "    archivos_en_entrada = os.listdir(ruta_entrada)\n",
    "    print(f\"\\nArchivos en carpeta de entrada ({len(archivos_en_entrada)}):\")\n",
    "    for archivo in archivos_en_entrada:\n",
    "        print(f\"  * {archivo}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR al guardar archivos: {e}\")\n",
    "    print(\"Verifica que los DataFrames esten cargados correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color: #d3d3d3; padding: 15px; text-align: center; border-radius: 8px;'>\n",
    "    <h3 style='color: #1e3a8a; margin: 0;'>SECCION 13: EJECUCION DEL PIPELINE COMPLETO</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ejecucion del pipeline completo de limpieza de datos.\n",
    "\n",
    "Este es el paso final que integra todas las funciones definidas:\n",
    "    1. Funciones de normalizacion (acentos, strings seguros)\n",
    "    2. Deteccion de valores perdidos\n",
    "    3. Deteccion de valores atipicos (IQR y Z-Score)\n",
    "    4. Analisis completo de DataFrames\n",
    "    5. Aplicacion de reglas de limpieza\n",
    "    6. Procesamiento automatizado de multiples archivos\n",
    "\n",
    "Resultado esperado:\n",
    "    - Carpeta 'datasets_salida' con:\n",
    "        * Subcarpeta 'reportes': analisis antes/despues + resumen global\n",
    "        * Subcarpeta 'limpios': archivos CSV limpios\n",
    "        * Archivo 'reportes_dataset.zip': todo comprimido\n",
    "\n",
    "IMPORTANTE:\n",
    "    Esta celda puede tardar varios segundos/minutos dependiendo\n",
    "    del tama√±o de los datasets.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"INICIANDO PIPELINE DE LIMPIEZA DE DATOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nConfiguracion:\")\n",
    "print(f\"  - Directorio entrada: {ruta_entrada}\")\n",
    "print(f\"  - Directorio salida: {ruta_salida}\")\n",
    "print(f\"  - Reglas definidas: {len(reglas_columnas)} columnas\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Presiona la tecla para ejecutar el procesamiento...\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Ejecutar el pipeline\n",
    "try:\n",
    "    procesar_carpeta(\n",
    "        directorio_entrada=ruta_entrada,\n",
    "        directorio_salida=ruta_salida,\n",
    "        reglas_columnas=reglas_columnas\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"ERROR DURANTE EL PROCESAMIENTO\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nDetalle del error: {e}\")\n",
    "    print(\"\\nVerifica:\")\n",
    "    print(\"  1. Que los archivos CSV existan en la carpeta de entrada\")\n",
    "    print(\"  2. Que tengas permisos de escritura en la carpeta de salida\")\n",
    "    print(\"  3. Que los nombres de columnas en reglas_columnas sean correctos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color: #d3d3d3; padding: 15px; text-align: center; border-radius: 8px;'>\n",
    "    <h3 style='color: #1e3a8a; margin: 0;'>SECCION 14: VERIFICACION DE RESULTADOS</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Verificacion de resultados generados.\n",
    "\n",
    "Esta celda lista todos los archivos creados en la carpeta de salida\n",
    "y carga el resumen global para inspeccion visual.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"VERIFICACION DE RESULTADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Verificar estructura de carpetas\n",
    "print(f\"\\nEstructura de carpeta de salida ({ruta_salida}):\")\n",
    "\n",
    "for root, dirs, files in os.walk(ruta_salida):\n",
    "    nivel = root.replace(ruta_salida, '').count(os.sep)\n",
    "    indent = ' ' * 2 * nivel\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    \n",
    "    sub_indent = ' ' * 2 * (nivel + 1)\n",
    "    for file in files:\n",
    "        print(f\"{sub_indent}{file}\")\n",
    "\n",
    "# Cargar y mostrar resumen global\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"RESUMEN GLOBAL DE LIMPIEZA\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "try:\n",
    "    ruta_resumen = os.path.join(ruta_salida, \"reportes\", \"resumen_global.csv\")\n",
    "    df_resumen = pd.read_csv(ruta_resumen)\n",
    "    \n",
    "    print(\"Metricas por archivo:\")\n",
    "    display(df_resumen)\n",
    "    \n",
    "    # Calcular mejoras totales\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"METRICAS CONSOLIDADAS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    total_filas_antes = df_resumen['filas_antes'].sum()\n",
    "    total_filas_despues = df_resumen['filas_despues'].sum()\n",
    "    total_duplicados_antes = df_resumen['duplicados_antes'].sum()\n",
    "    total_duplicados_despues = df_resumen['duplicados_despues'].sum()\n",
    "    total_problemas_antes = df_resumen['filas_con_problemas_antes'].sum()\n",
    "    total_problemas_despues = df_resumen['filas_con_problemas_despues'].sum()\n",
    "    \n",
    "    print(f\"Total de filas procesadas:\")\n",
    "    print(f\"  - Antes: {total_filas_antes}\")\n",
    "    print(f\"  - Despues: {total_filas_despues}\")\n",
    "    print(f\"  - Diferencia: {total_filas_antes - total_filas_despues} filas eliminadas\\n\")\n",
    "    \n",
    "    print(f\"Duplicados:\")\n",
    "    print(f\"  - Antes: {total_duplicados_antes}\")\n",
    "    print(f\"  - Despues: {total_duplicados_despues}\")\n",
    "    print(f\"  - Reduccion: {total_duplicados_antes - total_duplicados_despues} duplicados eliminados\\n\")\n",
    "    \n",
    "    print(f\"Filas con problemas:\")\n",
    "    print(f\"  - Antes: {total_problemas_antes}\")\n",
    "    print(f\"  - Despues: {total_problemas_despues}\")\n",
    "    if total_problemas_antes > 0:\n",
    "        mejora = ((total_problemas_antes - total_problemas_despues) / total_problemas_antes) * 100\n",
    "        print(f\"  - Mejora: {mejora:.2f}%\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: No se encontro el archivo resumen_global.csv\")\n",
    "    print(\"Verifica que el procesamiento se haya completado correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color: #d3d3d3; padding: 15px; text-align: center; border-radius: 8px;'>\n",
    "    <h3 style='color: #1e3a8a; margin: 0;'>SECCION 15: COMPARACION ANTES/DESPUES</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Comparacion visual de un archivo especifico antes y despues de la limpieza.\n",
    "\n",
    "Esta celda muestra:\n",
    "    1. Primeras filas del archivo original\n",
    "    2. Primeras filas del archivo limpio\n",
    "    3. Diferencias principales detectadas\n",
    "\n",
    "MODIFICAR: Cambia 'nombre_archivo' para analizar otro dataset\n",
    "\"\"\"\n",
    "\n",
    "# Seleccionar archivo a comparar\n",
    "nombre_archivo = \"ventas.csv\"  # Cambiar por \"clientes.csv\" o \"marketing.csv\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"COMPARACION: {nombre_archivo}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # Cargar archivos\n",
    "    df_original = pd.read_csv(os.path.join(ruta_entrada, nombre_archivo))\n",
    "    df_limpio = pd.read_csv(os.path.join(ruta_salida, \"limpios\", f\"limpio_{nombre_archivo}\"))\n",
    "    \n",
    "    print(f\"\\nARCHIVO ORIGINAL\")\n",
    "    print(f\"Dimensiones: {df_original.shape[0]} filas x {df_original.shape[1]} columnas\")\n",
    "    print(f\"\\nPrimeras 5 filas:\")\n",
    "    display(df_original.head())\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ARCHIVO LIMPIO\")\n",
    "    print(f\"Dimensiones: {df_limpio.shape[0]} filas x {df_limpio.shape[1]} columnas\")\n",
    "    print(f\"\\nPrimeras 5 filas:\")\n",
    "    display(df_limpio.head())\n",
    "    \n",
    "    # Comparar tipos de datos\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"COMPARACION DE TIPOS DE DATOS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    comparacion_tipos = pd.DataFrame({\n",
    "        'Columna': df_original.columns,\n",
    "        'Tipo Original': df_original.dtypes.values,\n",
    "        'Tipo Limpio': df_limpio.dtypes.values\n",
    "    })\n",
    "    \n",
    "    display(comparacion_tipos)\n",
    "    \n",
    "    # Comparar valores nulos\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"COMPARACION DE VALORES NULOS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    comparacion_nulos = pd.DataFrame({\n",
    "        'Columna': df_original.columns,\n",
    "        'Nulos Original': df_original.isna().sum().values,\n",
    "        'Nulos Limpio': df_limpio.isna().sum().values\n",
    "    })\n",
    "    \n",
    "    display(comparacion_nulos)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nERROR: No se encontro el archivo {nombre_archivo}\")\n",
    "    print(\"Archivos disponibles:\")\n",
    "    print(\"  - En entrada:\", os.listdir(ruta_entrada))\n",
    "    print(\"  - En limpios:\", os.listdir(os.path.join(ruta_salida, \"limpios\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color: #d3d3d3; padding: 15px; text-align: center; border-radius: 8px;'>\n",
    "    <h3 style='color: #1e3a8a; margin: 0;'>SECCION 16: CONCLUSIONES Y RECOMENDACIONES</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen del Proceso de Limpieza\n",
    "\n",
    "### Objetivos Alcanzados\n",
    "\n",
    "1. **Deteccion Automatica de Problemas**\n",
    "   - Valores nulos y faltantes\n",
    "   - Duplicados exactos\n",
    "   - Inconsistencias de formato (mayusculas, acentos, espacios)\n",
    "   - Valores atipicos numericos (outliers)\n",
    "\n",
    "2. **Limpieza Parametrizada**\n",
    "   - Normalizacion de texto (mayusculas, acentos)\n",
    "   - Conversion de tipos de datos (numericos, fechas)\n",
    "   - Eliminacion de duplicados\n",
    "   - Estandarizacion de formatos\n",
    "\n",
    "3. **Generacion de Reportes**\n",
    "   - Analisis antes/despues de la limpieza\n",
    "   - Metricas consolidadas por archivo\n",
    "   - Resumen global del proceso\n",
    "\n",
    "### Archivos Generados\n",
    "\n",
    "```\n",
    "datasets_salida/\n",
    "‚îú‚îÄ‚îÄ reportes/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ reporte_antes_ventas.csv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ reporte_despues_ventas.csv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ reporte_antes_clientes.csv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ reporte_despues_clientes.csv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ reporte_antes_marketing.csv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ reporte_despues_marketing.csv\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ resumen_global.csv\n",
    "‚îú‚îÄ‚îÄ limpios/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ limpio_ventas.csv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ limpio_clientes.csv\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ limpio_marketing.csv\n",
    "‚îî‚îÄ‚îÄ reportes_dataset.zip\n",
    "```\n",
    "\n",
    "### Proximos Pasos Recomendados\n",
    "\n",
    "1. **Analisis Exploratorio de Datos (EDA)**\n",
    "   - Graficos de distribucion\n",
    "   - Analisis de correlaciones\n",
    "   - Deteccion de patrones\n",
    "\n",
    "2. **Validacion de Negocio**\n",
    "   - Revisar reportes de problemas detectados\n",
    "   - Validar valores atipicos con expertos de dominio\n",
    "   - Confirmar reglas de limpieza aplicadas\n",
    "\n",
    "3. **Documentacion**\n",
    "   - Registrar decisiones de limpieza\n",
    "   - Documentar casos especiales\n",
    "   - Crear manual de procedimientos\n",
    "\n",
    "4. **Automatizacion**\n",
    "   - Programar ejecucion periodica\n",
    "   - Integrar con pipeline de datos\n",
    "   - Implementar alertas de calidad\n",
    "\n",
    "### Buenas Practicas Aplicadas\n",
    "\n",
    "- **Reproducibilidad**: Todo el proceso esta documentado y parametrizado\n",
    "- **Trazabilidad**: Se conservan reportes antes/despues\n",
    "- **Modularidad**: Funciones reutilizables y configurables\n",
    "- **Seguridad**: No se modifican archivos originales\n",
    "- **Escalabilidad**: Funciona con multiples archivos automaticamente\n",
    "\n",
    "### Recursos Adicionales\n",
    "\n",
    "- Documentacion de Pandas: https://pandas.pydata.org/docs/\n",
    "- Buenas practicas de limpieza de datos: https://www.datacamp.com/\n",
    "- Tecnicas de deteccion de outliers: https://scikit-learn.org/\n",
    "\n",
    "---\n",
    "\n",
    "**Autor**: Garcia Traba Ariel H  \n",
    "**Comision**: 25262  \n",
    "**Curso**: Data Analytics - TPI  \n",
    "**Fecha**: 2025"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color: #d3d3d3; padding: 15px; text-align: center; border-radius: 8px;'>\n",
    "    <h3 style='color: #1e3a8a; margin: 0;'>SECCION 11: DEFINICION DE REGLAS DE LIMPIEZA</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Definicion de reglas de limpieza especificas para los datasets del curso.\n",
    "\n",
    "Estructura del diccionario de reglas:\n",
    "    Clave: nombre de la columna\n",
    "    Valor: regla a aplicar (string simple o tupla con opciones)\n",
    "\n",
    "Reglas definidas para cada dataset:\n",
    "\n",
    "DATASET CLIENTES:\n",
    "    - nombre: Formato Titulo (\"Juan Perez\")\n",
    "    - ciudad: Formato Titulo (\"Buenos Aires\")\n",
    "\n",
    "DATASET VENTAS:\n",
    "    - producto: Formato Titulo\n",
    "    - categoria: Formato Titulo\n",
    "    - precio: Convertir a numerico (sin eliminar decimales)\n",
    "    - cantidad: Convertir a entero\n",
    "    - fecha_venta: Convertir a fecha (formato DD/MM/YYYY)\n",
    "\n",
    "DATASET MARKETING:\n",
    "    - producto: Formato Titulo\n",
    "    - canal: Minusculas (\"rrss\", \"email\", \"tv\")\n",
    "    - costo: Convertir a numerico\n",
    "    - fecha_inicio: Convertir a fecha\n",
    "    - fecha_fin: Convertir a fecha\n",
    "\n",
    "NOTA: Las reglas se aplican de forma selectiva.\n",
    "      Las columnas sin regla reciben limpieza basica automatica.\n",
    "\"\"\"\n",
    "\n",
    "reglas_columnas = {\n",
    "    # Columnas comunes en varios datasets\n",
    "    'nombre': 'title',\n",
    "    'ciudad': 'title',\n",
    "    'producto': 'title',\n",
    "    'categoria': 'title',\n",
    "    \n",
    "    # Columnas del dataset MARKETING\n",
    "    'canal': 'lower',\n",
    "    \n",
    "    # Columnas numericas\n",
    "    'precio': ('numeric', {\n",
    "        'remove_non_digits': False,  # Mantener puntos decimales\n",
    "        'remove_thousands': False,   # No hay separadores de miles\n",
    "        'as_int': False              # Mantener como float\n",
    "    }),\n",
    "    \n",
    "    'cantidad': ('numeric', {\n",
    "        'remove_non_digits': True,   # Solo digitos\n",
    "        'as_int': True               # Convertir a entero\n",
    "    }),\n",
    "    \n",
    "    'ingresos': ('numeric', {\n",
    "        'remove_non_digits': False,\n",
    "        'as_int': False\n",
    "    }),\n",
    "    \n",
    "    'costo': ('numeric', {\n",
    "        'remove_non_digits': False,\n",
    "        'as_int': False\n",
    "    }),\n",
    "    \n",
    "    # Columnas de fecha\n",
    "    'fecha_venta': ('date', {\n",
    "        'formats': ['%d/%m/%Y'],     # Formato dia/mes/a√±o\n",
    "        'dayfirst': True             # Dia va primero\n",
    "    }),\n",
    "    \n",
    "    'fecha_inicio': ('date', {\n",
    "        'formats': ['%d/%m/%Y'],\n",
    "        'dayfirst': True\n",
    "    }),\n",
    "    \n",
    "    'fecha_fin': ('date', {\n",
    "        'formats': ['%d/%m/%Y'],\n",
    "        'dayfirst': True\n",
    "    })\n",
    "}\n",
    "\n",
    "print(\"Reglas de limpieza definidas:\")\n",
    "print(f\"\\nTotal de columnas con reglas especificas: {len(reglas_columnas)}\")\n",
    "print(\"\\nDetalle de reglas:\")\n",
    "for columna, regla in reglas_columnas.items():\n",
    "    if isinstance(regla, tuple):\n",
    "        tipo, opciones = regla\n",
    "        print(f\"  - {columna}: {tipo} {opciones}\")\n",
    "    else:\n",
    "        print(f\"  - {columna}: {regla}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color: #d3d3d3; padding: 15px; text-align: center; border-radius: 8px;'>\n",
    "    <h3 style='color: #1e3a8a; margin: 0;'>SECCION 12: PREPARACION DE ARCHIVOS DE ENTRADA</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Preparacion de archivos CSV en la carpeta            texto = str(valor).strip()

            # Si se especificaron formatos personalizados, intentarlos primero
            if 'formats' in opciones and opciones['formats']:
                for formato in opciones['formats']:
                    try:
                        # Intentar conversion con formato especifico
                        return pd.to_datetime(datetime.strptime(texto, formato))
                    except (ValueError, TypeError):
                        # Si falla, probar siguiente formato
                        continue

            # Si no hay formatos especificos o ninguno funciono,
            # intentar conversion automatica de pandas
            try:
                return pd.to_datetime(
                    texto,
                    dayfirst=opciones.get('dayfirst', True),  # Por defecto: dia primero
                    errors='coerce'  # Si falla, devuelve NaT
                )
            except Exception:
                return pd.NaT

        serie_resultado = serie_resultado.map(convertir_a_fecha)

    # Si la regla no se reconoce, devolver la serie sin cambios
    return serie_resultado


print("Funcion aplicar_regla_columna definida correctamente")
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color: #d3d3d3; padding: 15px; text-align: center; border-radius: 8px;'>\n",
    "    <h3 style='color: #1e3a8a; margin: 0;'>SECCION 9: FUNCION PRINCIPAL DE LIMPIEZA</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Funcion: limpiar_dataframe\n",
    "\n",
    "Proposito:\n",
    "    Aplica un proceso completo de limpieza sobre un DataFrame,\n",
    "    combinando reglas especificas por columna con limpieza general.\n",
    "\n",
    "Parametros:\n",
    "    df (pd.DataFrame): DataFrame original a limpiar\n",
    "    reglas_columnas (dict): Diccionario con reglas por columna\n",
    "        Estructura: {'nombre_columna': regla}\n",
    "        Ejemplo: {\n",
    "            'nombre': 'title',\n",
    "            'precio': ('numeric', {'remove_thousands': True}),\n",
    "            'fecha': ('date', {'formats': ['%d/%m/%Y']})\n",
    "        }\n",
    "\n",
    "Retorna:\n",
    "    pd.DataFrame: Nueva copia del DataFrame limpio\n",
    "\n",
    "Proceso de limpieza aplicado:\n",
    "    1. Normalizar nombres de columnas (eliminar espacios)\n",
    "    2. Aplicar reglas especificas definidas por el usuario\n",
    "    3. Aplicar limpieza basica a columnas de texto sin regla:\n",
    "       - Eliminar espacios en blanco\n",
    "       - Convertir tokens de valores faltantes a NaN\n",
    "    4. Eliminar filas duplicadas completas (mantener primera ocurrencia)\n",
    "    5. Resetear indices del DataFrame\n",
    "\n",
    "Ventajas de este enfoque:\n",
    "    - Modular: cada columna puede tener su propia regla\n",
    "    - Reutilizable: mismas reglas para diferentes datasets\n",
    "    - Seguro: no modifica el DataFrame original\n",
    "    - Completo: combina limpieza especifica y general\n",
    "\n",
    "Ejemplo de uso:\n",
    "    reglas = {\n",
    "        'nombre': 'title',\n",
    "        'email': 'lower',\n",
    "        'precio': ('numeric', {'remove_thousands': True, 'as_int': False})\n",
    "    }\n",
    "    df_limpio = limpiar_dataframe(df_original, reglas_columnas=reglas)\n",
    "\"\"\"\n",
    "\n",
    "def limpiar_dataframe(df, reglas_columnas=None):\n",
    "    # Paso 1: Crear copia para no modificar el original\n",
    "    df_limpio = df.copy()\n",
    "    \n",
    "    # Si no se pasaron reglas, inicializar diccionario vacio\n",
    "    reglas_columnas = reglas_columnas or {}\n",
    "    \n",
    "    # Paso 2: Normalizar nombres de columnas\n",
    "    # Elimina espacios en blanco al inicio/final de cada nombre\n",
    "    df_limpio.columns = [str(nombre_columna).strip() for nombre_columna in df_limpio.columns]\n",
    "    \n",
    "    # Paso 3: Aplicar reglas especificas por columna\n",
    "    for nombre_columna in df_limpio.columns:\n",
    "        if nombre_columna in reglas_columnas:\n",
    "            # Si existe regla para esta columna, aplicarla\n",
    "            df_limpio[nombre_columna] = aplicar_regla_columna(\n",
    "                df_limpio[nombre_columna],\n",
    "                reglas_columnas[nombre_columna]\n",
    "            )\n",
    "        else:\n",
    "            # Paso 4: Limpieza basica por defecto para columnas de texto\n",
    "            # Solo se aplica si no hay regla especifica definida\n",
    "            if (\n",
    "                df_limpio[nombre_columna].dtype == object\n",
    "                or pd.api.types.is_string_dtype(df_limpio[nombre_columna])\n",
    "            ):\n",
    "                df_limpio[nombre_columna] = df_limpio[nombre_columna].map(\n",
    "                    lambda valor: (\n",
    "                        np.nan if buscar_valores_perdidos(valor)\n",
    "                        else (safe_str(valor).strip() if not pd.isna(valor) else valor)\n",
    "                    )\n",
    "                )\n",
    "    \n",
    "    # Paso 5: Eliminar filas duplicadas\n",
    "    # keep='first' mantiene la primera ocurrencia de cada fila duplicada\n",
    "    df_limpio = df_limpio.drop_duplicates(keep='first')\n",
    "    \n",
    "    # Paso 6: Resetear indices para tener numeracion consecutiva\n",
    "    # drop=True evita que el indice viejo se agregue como columna\n",
    "    df_limpio = df_limpio.reset_index(drop=True)\n",
    "    \n",
    "    return df_limpio\n",
    "\n",
    "\n",
    "print(\"Funcion limpiar_dataframe definida correctamente\")
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color: #d3d3d3; padding: 15px; text-align: center; border-radius: 8px;'>\n",
    "    <h3 style='color: #1e3a8a; margin: 0;'>SECCION 10: PROCESAMIENTO AUTOMATIZADO DE MULTIPLES ARCHIVOS</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Funcion: procesar_carpeta\n",
    "\n",
    "Proposito:\n",
    "    Automatiza el proceso completo de limpieza para multiples archivos CSV,\n",
    "    generando reportes comparativos (antes/despues) y consolidando resultados.\n",
    "\n",
    "Parametros:\n",
    "    directorio_entrada (str): Carpeta con los CSV originales\n",
    "    directorio_salida (str): Carpeta donde guardar resultados\n",
    "    reglas_columnas (dict): Reglas de limpieza por columna\n",
    "\n",
    "Estructura del proceso:\n",
    "    1. Crear estructura de carpetas de salida\n",
    "    2. Identificar todos los archivos CSV\n",
    "    3. Para cada CSV:\n",
    "       a) Leer archivo original\n",
    "       b) Detectar problemas ANTES de limpiar\n",
    "       c) Aplicar limpieza con reglas definidas\n",
    "       d) Detectar problemas DESPUES de limpiar\n",
    "       e) Guardar reportes y CSV limpio\n",
    "    4. Generar resumen global consolidado\n",
    "    5. Comprimir todo en un archivo ZIP\n",
    "\n",
    "Archivos generados:\n",
    "    - reportes/reporte_antes_[nombre].csv: Problemas detectados antes\n",
    "    - reportes/reporte_despues_[nombre].csv: Problemas despues de limpiar\n",
    "    - limpios/limpio_[nombre].csv: Archivo CSV limpio\n",
    "    - reportes/resumen_global.csv: Metricas consolidadas de todos los archivos\n",
    "    - reportes_dataset.zip: Todos los archivos comprimidos\n",
    "\n",
    "Metricas del resumen global:\n",
    "    - Cantidad de filas antes y despues\n",
    "    - Duplicados detectados\n",
    "    - Valores nulos totales\n",
    "    - Filas con problemas antes y despues\n",
    "\"\"\"\n",
    "\n",
    "def procesar_carpeta(directorio_entrada, directorio_salida, reglas_columnas=None):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"INICIO DEL PROCESAMIENTO AUTOMATIZADO\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # =====================================================\n",
    "    # PASO 1: CREAR ESTRUCTURA DE CARPETAS\n",
    "    # =====================================================\n",
    "    \n",
    "    print(\"\\nPaso 1/5: Creando estructura de carpetas...\")\n",
    "    os.makedirs(directorio_salida, exist_ok=True)\n",
    "    \n",
    "    directorio_reportes = os.path.join(directorio_salida, \"reportes\")\n",
    "    directorio_limpios = os.path.join(directorio_salida, \"limpios\")\n",
    "    \n",
    "    os.makedirs(directorio_reportes, exist_ok=True)\n",
    "    os.makedirs(directorio_limpios, exist_ok=True)\n",
    "    \n",
    "    print(f\"  - Carpeta de reportes: {directorio_reportes}\")\n",
    "    print(f\"  - Carpeta de archivos limpios: {directorio_limpios}\")\n",
    "    \n",
    "    # =====================================================\n",
    "    # PASO 2: IDENTIFICAR ARCHIVOS CSV\n",
    "    # =====================================================\n",
    "    \n",
    "    print(\"\\nPaso 2/5: Identificando archivos CSV...\")\n",
    "    archivos_csv = [\n",
    "        f for f in os.listdir(directorio_entrada) \n",
    "        if f.lower().endswith('.csv')\n",
    "    ]\n",
    "    \n",
    "    if not archivos_csv:\n",
    "        raise RuntimeError(\n",
    "            f\"ERROR: No se encontraron archivos CSV en {directorio_entrada}\"\n",
    "        )\n",
    "    \n",
    "    print(f\"  - Archivos encontrados: {len(archivos_csv)}\")\n",
    "    for archivo in archivos_csv:\n",
    "        print(f\"    * {archivo}\")\n",
    "    \n",
    "    # Lista para almacenar metricas de cada archivo\n",
    "    resumen_archivos = []\n",
    "    \n",
    "    # Ruta del ZIP final\n",
    "    ruta_zip_reportes = os.path.join(directorio_salida, \"reportes_dataset.zip\")\n",
    "    \n",
    "    # =====================================================\n",
    "    # PASO 3: PROCESAMIENTO DE CADA ARCHIVO\n",
    "    # =====================================================\n",
    "    \n",
    "    print(\"\\nPaso 3/5: Procesando archivos...\")\n",
    "    \n",
    "    with zipfile.ZipFile(ruta_zip_reportes, 'w', compression=zipfile.ZIP_DEFLATED) as archivo_zip:\n",
    "        \n",
    "        for numero, nombre_archivo in enumerate(archivos_csv, 1):\n",
    "            print(f\"\\n{'-'*80}\")\n",
    "            print(f\"Archivo {numero}/{len(archivos_csv)}: {nombre_archivo}\")\n",
    "            print(f\"{'-'*80}\")\n",
    "            \n",
    "            ruta_archivo = os.path.join(directorio_entrada, nombre_archivo)\n",
    "            \n",
    "            # --- 3a. Lectura del CSV ---\n",
    "            print(\"  [1/6] Leyendo archivo...\")\n",
    "            try:\n",
    "                # Intento 1: UTF-8 (encoding mas comun)\n",
    "                df_original = pd.read_csv(\n",
    "                    ruta_archivo,\n",
    "                    dtype=str,              # Leer todo como texto\n",
    "                    keep_default_na=False,  # No convertir autom√°ticamente a NaN\n",
    "                    na_values=['']          # Solo strings vacios son NaN\n",
    "                )\n",
    "                print(\"      Encoding: UTF-8\")\n",
    "            except Exception:\n",
    "                # Intento 2: Latin1 (alternativa comun)\n",
    "                df_original = pd.read_csv(\n",
    "                    ruta_archivo,\n",
    "                    encoding='latin1',\n",
    "                    dtype=str,\n",
    "                    keep_default_na=False,\n",
    "                    na_values=['']\n",
    "                )\n",
    "                print(\"      Encoding: Latin1\")\n",
    "            \n",
    "            print(f\"      Dimensiones: {df_original.shape[0]} filas x {df_original.shape[1]} columnas\")\n",
    "            \n",
    "            # --- 3b. Deteccion ANTES de limpiar ---\n",
    "            print(\"  [2/6] Analizando calidad de datos (ANTES)...\")\n",
    "            resumen_antes, chequeos_antes, problemas_antes = detectar_problemas_dataframe(df_original)\n",
    "            \n",
    "            print(f\"      Duplicados: {resumen_antes.get('filas_duplicadas', 0)}\")\n",
    "            print(f\"      Nulos totales: {sum(resumen_antes.get('nulos_por_columna', {}).values())}\")\n",
    "            print(f\"      Filas con problemas: {len(problemas_antes)}\")\n",
    "            \n",
    "            # Guardar reporte ANTES\n",
    "            nombre_base = os.path.splitext(nombre_archivo)[0]\n",
    "            ruta_reporte_antes = os.path.join(\n",
    "                directorio_reportes,\n",
    "                f\"reporte_antes_{nombre_base}.csv\"\n",
    "            )\n",
    "            \n",
    "            problemas_antes.to_csv(ruta_reporte_antes, index=False, encoding='utf-8')\n",
    "            archivo_zip.write(\n",
    "                ruta_reporte_antes,\n",
    "                arcname=os.path.join(\"reportes\", f\"reporte_antes_{nombre_base}.csv\")\n",
    "            )\n",
    "            \n",
    "            # --- 3c. Aplicar limpieza ---\n",
    "            print(\"  [3/6] Aplicando limpieza de datos...\")\n",
    "            df_limpio = limpiar_dataframe(df_original, reglas_columnas=reglas_columnas)\n",
    "            print(f\"      Filas despues de limpieza: {df_limpio.shape[0]}\")\n",
    "            \n",
    "            # --- 3d. Deteccion DESPUES de limpiar ---\n",
    "            print(\"  [4/6] Analizando calidad de datos (DESPUES)...\")\n",
    "            resumen_despues, chequeos_despues, problemas_despues = detectar_problemas_dataframe(df_limpio)\n",
    "            \n",
    "            print(f\"      Duplicados: {resumen_despues.get('filas_duplicadas', 0)}\")\n",
    "            print(f\"      Nulos totales: {sum(resumen_despues.get('nulos_por_columna', {}).values())}\")\n",
    "            print(f\"      Filas con problemas: {len(problemas_despues)}\")\n",
    "            \n",
    "            # Guardar reporte DESPUES\n",
    "            ruta_reporte_despues = os.path.join(\n",
    "                directorio_reportes,\n",
    "                f\"reporte_despues_{nombre_base}.csv\"\n",
    "            )\n",
    "            \n",
    "            problemas_despues.to_csv(ruta_reporte_despues, index=False, encoding='utf-8')\n",
    "            archivo_zip.write(\n",
    "                ruta_reporte_despues,\n",
    "                arcname=os.path.join(\"reportes\", f\"reporte_despues_{nombre_base}.csv\")\n",
    "            )\n",
    "            \n",
    "            # --- 3e. Guardar CSV limpio ---\n",
    "            print(\"  [5/6] Guardando archivo limpio...\")\n",
    "            ruta_csv_limpio = os.path.join(directorio_limpios, f\"limpio_{nombre_archivo}\")\n",
    "            df_limpio.to_csv(ruta_csv_limpio, index=False, encoding='utf-8')\n",
    "            \n",
    "            archivo_zip.write(\n",
    "                ruta_csv_limpio,\n",
    "                arcname=os.path.join(\"limpios\", f\"limpio_{nombre_archivo}\")\n",
    "            )\n",
    "            \n",
    "            # --- 3f. Guardar metricas ---\n",
    "            print(\"  [6/6] Registrando metricas...\")\n",
    "            resumen_archivos.append({\n",
    "                'archivo': nombre_archivo,\n",
    "                'filas_antes': resumen_antes['cantidad_filas'],\n",
    "                'filas_despues': resumen_despues['cantidad_filas'],\n",
    "                'columnas': resumen_antes['cantidad_columnas'],\n",
    "                'duplicados_antes': resumen_antes.get('filas_duplicadas', 0),\n",
    "                'duplicados_despues': resumen_despues.get('filas_duplicadas', 0),\n",
    "                'nulos_totales_antes': sum(resumen_antes.get('nulos_por_columna', {}).values()),\n",
    "                'nulos_totales_despues': sum(resumen_despues.get('nulos_por_columna', {}).values()),\n",
    "                'filas_con_problemas_antes': len(problemas_antes),\n",
    "                'filas_con_problemas_despues': len(problemas_despues)\n",
    "            })\n",
    "            \n",
    "            print(\"      Completado\")\n",
    "    \n",
    "    # =====================================================\n",
    "    # PASO 4: GENERAR RESUMEN GLOBAL\n",
    "    # =====================================================\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"Paso 4/5: Generando resumen global...\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    df_resumen_global = pd.DataFrame(resumen_archivos)\n",
    "    ruta_resumen_global = os.path.join(directorio_reportes, \"resumen_global.csv\")\n",
    "    df_resumen_global.to_csv(ruta_resumen_global, index=False, encoding='utf-8')\n",
    "    \n",
    "    # Agregar resumen al ZIP\n",
    "    with zipfile.ZipFile(ruta_zip_reportes, 'a', compression=zipfile.ZIP_DEFLATED) as archivo_zip:\n",
    "        archivo_zip.write(\n",
    "            ruta_resumen_global,\n",
    "            arcname=\"reportes/resumen_global.csv\"\n",
    "        )\n",
    "    \n",
    "    # =====================================================\n",
    "    # PASO 5: MENSAJES FINALES\n",
    "    # =====================================================\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PROCESAMIENTO COMPLETADO EXITOSAMENTE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nResultados guardados en: {directorio_salida}\")\n",
    "    print(f\"  - Reportes individuales: {directorio_reportes}\")\n",
    "    print(f\"  - Archivos limpios: {directorio_limpios}\")\n",
    "    print(f\"  - ZIP consolidado: {ruta_zip_reportes}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"RESUMEN GLOBAL DE PROCESAMIENTO\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(df_resumen_global.to_string(index=False))\n",
    "    print(f\"\\n{'='*80}\\n\")\n",
    "\n",
    "\n",
    "print(\"Funcion procesar_carpeta definida correctamente\")
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color: #d3d3d3; padding: 15px; text-align: center; border-radius: 8px;'>\n",
    "    <h3 style='color: #1e3a8a; margin: 0;'>SECCION 8: APLICACION DE REGLAS DE LIMPIEZA</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Funcion: aplicar_regla_columna\n",
    "\n",
    "Proposito:\n",
    "    Aplica una regla de transformacion especifica sobre una columna completa\n",
    "    de un DataFrame, permitiendo estandarizar y limpiar datos de forma parametrizada.\n",
    "\n",
    "Parametros:\n",
    "    serie (pd.Series): Columna del DataFrame a transformar\n",
    "    regla (str o tuple): Tipo de transformacion a aplicar\n",
    "        - str: regla simple (ej: 'lower', 'upper', 'strip')\n",
    "        - tuple: (tipo_regla, diccionario_opciones)\n",
    "\n",
    "Retorna:\n",
    "    pd.Series: Nueva serie con la transformacion aplicada\n",
    "\n",
    "Tipos de reglas disponibles:\n",
    "\n",
    "    REGLAS DE TEXTO:\n",
    "    - 'strip': Elimina espacios al inicio y final\n",
    "    - 'lower': Convierte todo a minusculas\n",
    "    - 'upper': Convierte todo a MAYUSCULAS\n",
    "    - 'title': Convierte a Formato Titulo (primera letra mayuscula)\n",
    "    - 'remove_accents': Elimina acentos y diacriticos\n",
    "\n",
    "    REGLAS NUMERICAS:\n",
    "    - ('numeric', opciones):\n",
    "        Opciones disponibles:\n",
    "        * 'remove_non_digits': True/False - Elimina todo excepto numeros\n",
    "        * 'remove_thousands': True/False - Elimina separadores de miles (,)\n",
    "        * 'as_int': True/False - Convierte a entero\n",
    "\n",
    "    REGLAS DE FECHA:\n",
    "    - ('date', opciones):\n",
    "        Opciones disponibles:\n",
    "        * 'formats': lista de formatos a intentar (ej: ['%d/%m/%Y'])\n",
    "        * 'dayfirst': True/False - Si el dia va primero en fechas ambiguas\n",
    "\n",
    "Ejemplos de uso:\n",
    "    # Texto simple\n",
    "    aplicar_regla_columna(df['nombre'], 'title')\n",
    "    \n",
    "    # Numeros con opciones\n",
    "    aplicar_regla_columna(df['precio'], ('numeric', {'remove_thousands': True}))\n",
    "    \n",
    "    # Fechas con formato especifico\n",
    "    aplicar_regla_columna(df['fecha'], ('date', {'formats': ['%d/%m/%Y']}))\n",
    "\"\"\"\n",
    "\n",
    "def aplicar_regla_columna(serie, regla):\n",
    "    # Parsear la regla: puede ser string simple o tupla (tipo, opciones)\n",
    "    if isinstance(regla, tuple):\n",
    "        tipo_regla, opciones = regla\n",
    "    else:\n",
    "        tipo_regla = regla\n",
    "        opciones = {}\n",
    "    \n",
    "    # Crear copia de la serie para no modificar el original\n",
    "    serie_resultado = serie.copy()\n",
    "    \n",
    "    # =====================================================\n",
    "    # REGLAS DE TEXTO BASICAS\n",
    "    # =====================================================\n",
    "    \n",
    "    if tipo_regla == 'strip':\n",
    "        # Eliminar espacios al inicio y final\n",
    "        serie_resultado = serie_resultado.map(\n",
    "            lambda valor: safe_str(valor).strip() if not pd.isna(valor) else valor\n",
    "        )\n",
    "    \n",
    "    elif tipo_regla == 'lower':\n",
    "        # Convertir a minusculas\n",
    "        serie_resultado = serie_resultado.map(\n",
    "            lambda valor: safe_str(valor).strip().lower() if not pd.isna(valor) else valor\n",
    "        )\n",
    "    \n",
    "    elif tipo_regla == 'upper':\n",
    "        # Convertir a MAYUSCULAS\n",
    "        serie_resultado = serie_resultado.map(\n",
    "            lambda valor: safe_str(valor).strip().upper() if not pd.isna(valor) else valor\n",
    "        )\n",
    "    \n",
    "    elif tipo_regla == 'title':\n",
    "        # Convertir a Formato Titulo\n",
    "        # Ejemplo: \"juan perez\" -> \"Juan Perez\"\n",
    "        serie_resultado = serie_resultado.map(\n",
    "            lambda valor: safe_str(valor).strip().title() if not pd.isna(valor) else valor\n",
    "        )\n",
    "    \n",
    "    elif tipo_regla == 'remove_accents':\n",
    "        # Eliminar acentos y diacriticos\n",
    "        serie_resultado = serie_resultado.map(\n",
    "            lambda valor: sacar_acentos(safe_str(valor)).strip() if not pd.isna(valor) else valor\n",
    "        )\n",
    "    \n",
    "    # =====================================================\n",
    "    # REGLAS NUMERICAS\n",
    "    # =====================================================\n",
    "    \n",
    "    elif tipo_regla == 'numeric':\n",
    "        def convertir_a_numero(valor):\n",
    "            # Si es nulo, mantener como NaN\n",
    "            if pd.isna(valor):\n",
    "                return np.nan\n",
    "            \n",
    "            # Convertir a string y limpiar espacios\n",
    "            texto = str(valor).strip()\n",
    "            \n",
    "            # Opcion: eliminar todo excepto digitos y signos\n",
    "            if opciones.get('remove_non_digits', False):\n",
    "                # Mantener solo: digitos, punto decimal, signo negativo\n",
    "                texto = ''.join([c for c in texto if c.isdigit() or c in '.-'])\n",
    "            \n",
    "            # Opcion: eliminar separadores de miles\n",
    "            if opciones.get('remove_thousands', False):\n",
    "                # Ejemplo: \"1,000.50\" -> \"1000.50\"\n",
    "                texto = texto.replace(',', '')\n",
    "            \n",
    "            # Intentar conversion\n",
    "            try:\n",
    "                if opciones.get('as_int', False):\n",
    "                    # Convertir a entero\n",
    "                    return int(float(texto))\n",
    "                else:\n",
    "                    # Convertir a float\n",
    "                    return float(texto)\n",
    "            except (ValueError, TypeError):\n",
    "                # Si falla la conversion, devolver NaN\n",
    "                return np.nan\n",
    "        \n",
    "        serie_resultado = serie_resultado.map(convertir_a_numero)\n",
    "    \n",
    "    # =====================================================\n",
    "    # REGLAS DE FECHA\n",
    "    # =====================================================\n",
    "    \n",
    "    elif tipo_regla == 'date':\n",
    "        def convertir_a_fecha(valor):\n",
    "            # Si es nulo, devolver NaT (Not a Time)\n",
    "            if pd.isna(valor):\n",
    "                return pd.NaT\n",
    "            \n",
    "            texto = str(valor).strip()\n",
    "            \n",{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color: #d3d3d3; padding: 20px; text-align: center; border-radius: 10px;'>\n",
    "    <h1 style='color: #1e3a8a; margin: 0;'>TRABAJO PRACTICO INTEGRADOR</h1>\n",
    "    <h2 style='color: #1e3a8a; margin: 10px 0 0 0;'>Data Analytics - Limpieza y Calidad de Datos</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color: #d3d3d3; padding: 15px; text-align: center; border-radius: 8px;'>\n",
    "    <h3 style='color: #1e3a8a; margin: 0;'>SECCION 0: IMPORTACION DE LIBRERIAS</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Importacion de librerias necesarias para el procesamiento de datos.\n",
    "\n",
    "Librerias utilizadas:\n",
    "    - os: Manejo de rutas y archivos del sistema operativo\n",
    "    - pandas: Manipulacion y analisis de datos estructurados\n",
    "    - numpy: Operaciones numericas y manejo de arrays\n",
    "    - unicodedata: Normalizacion de caracteres Unicode (acentos)\n",
    "    - json: Serializacion de datos para reportes\n",
    "    - zipfile: Compresion de archivos de salida\n",
    "    - datetime: Manejo de fechas y timestamps\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import json\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Librerias importadas correctamente\")\n",
    "print(f\"Version de pandas: {pd.__version__}\")\n",
    "print(f\"Version de numpy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color: #d3d3d3; padding: 15px; text-align: center; border-radius: 8px;'>\n",
    "    <h3 style='color: #1e3a8a; margin: 0;'>SECCION 1: CONFIGURACION DE RUTAS</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Configuracion de rutas del proyecto.\n",
    "\n",
    "Variables globales:\n",
    "    - ruta_base: Directorio raiz donde se encuentran los archivos del proyecto\n",
    "    - ruta_entrada: Carpeta donde se colocan los CSV a procesar\n",
    "    - ruta_salida: Carpeta donde se guardan los resultados\n",
    "\n",
    "IMPORTANTE: Ajustar ruta_base segun la ubicacion de tus archivos.\n",
    "            Para Google Colab, descomentar la seccion correspondiente.\n",
    "\"\"\"\n",
    "\n",
    "# Configuracion para ejecucion local\n",
    "ruta_base = \"\"  # Dejar vacio para usar el directorio actual\n",
    "\n",
    "# DESCOMENTAR PARA GOOGLE COLAB\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# ruta_base = \"/content/drive/MyDrive/CABA/Garcia Traba Ariel H - Comision 25262 - TPI Data Analytics/\"\n",
    "\n",
    "# Crear estructura de carpetas\n",
    "ruta_entrada = os.path.join(ruta_base, \"datasets_entrada\")\n",
    "ruta_salida = os.path.join(ruta_base, \"datasets_salida\")\n",
    "\n",
    "os.makedirs(ruta_entrada, exist_ok=True)\n",
    "os.makedirs(ruta_salida, exist_ok=True)\n",
    "\n",
    "print(\"Configuracion de rutas completada:\")\n",
    "print(f\"  - Ruta base: {ruta_base if ruta_base else 'Directorio actual'}\")\n",
    "print(f\"  - Entrada: {ruta_entrada}\")\n",
    "print(f\"  - Salida: {ruta_salida}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color: #d3d3d3; padding: 15px; text-align: center; border-radius: 8px;'>\n",
    "    <h3 style='color: #1e3a8a; margin: 0;'>SECCION 2: CARGA DE DATASETS</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Carga de los datasets del curso.\n",
    "\n",
    "Archivos esperados:\n",
    "    - ventas.csv: Registro de transacciones comerciales\n",
    "    - clientes.csv: Informacion demografica de clientes\n",
    "    - marketing.csv: Datos de campanas publicitarias\n",
    "\n",
    "NOTA: Los archivos deben estar en el directorio ruta_base.\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    # Cargar datasets\n",
    "    df_ventas = pd.read_csv(os.path.join(ruta_base, \"ventas.csv\"))\n",
    "    df_clientes = pd.read_csv(os.path.join(ruta_base, \"clientes.csv\"))\n",
    "    df_marketing = pd.read_csv(os.path.join(ruta_base, \"marketing.csv\"))\n",
    "    \n",
    "    print(\"Datasets cargados correctamente:\")\n",
    "    print(f\"\\n  - df_ventas: {df_ventas.shape[0]} filas x {df_ventas.shape[1]} columnas\")\n",
    "    print(f\"    Columnas: {list(df_ventas.columns)}\")\n",
    "    \n",
    "    print(f\"\\n  - df_clientes: {df_clientes.shape[0]} filas x {df_clientes.shape[1]} columnas\")\n",
    "    print(f\"    Columnas: {list(df_clientes.columns)}\")\n",
    "    \n",
    "    print(f\"\\n  - df_marketing: {df_marketing.shape[0]} filas x {df_marketing.shape[1]} columnas\")\n",
    "    print(f\"    Columnas: {list(df_marketing.columns)}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERROR: No se encontraron los archivos CSV en {ruta_base}\")\n",
    "    print(f\"Detalle: {e}\")\n",
    "    print(\"\\nAsegurate de que los archivos esten en la ruta correcta.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color: #d3d3d3; padding: 15px; text-align: center; border-radius: 8px;'>\n",
    "    <h3 style='color: #1e3a8a; margin: 0;'>SECCION 3: PREVIEW DE DATOS ORIGINALES</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualizacion inicial de los datos cargados.\n",
    "\n",
    "Se muestran las primeras 5 filas de cada dataset para verificar\n",
    "la estructura y contenido antes de aplicar transformaciones.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATASET: df_ventas\")\n",
    "print(\"=\"*80)\n",
    "display(df_ventas.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATASET: df_clientes\")\n",
    "print(\"=\"*80)\n",
    "display(df_clientes.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATASET: df_marketing\")\n",
    "print(\"=\"*80)\n",
    "display(df_marketing.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color: #d3d3d3; padding: 15px; text-align: center; border-radius: 8px;'>\n",
    "    <h3 style='color: #1e3a8a; margin: 0;'>SECCION 4: FUNCIONES DE NORMALIZACION</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Funcion: sacar_acentos\n",
    "\n",
    "Proposito:\n",
    "    Elimina acentos y caracteres diacriticos de un texto, convirtiendo\n",
    "    caracteres como '√°', '√©', '√±' a sus equivalentes sin acento.\n",
    "\n",
    "Parametros:\n",
    "    texto (str): Cadena de texto a normalizar\n",
    "\n",
    "Retorna:\n",
    "    str: Texto sin acentos ni diacriticos\n",
    "\n",
    "Ejemplo:\n",
    "    sacar_acentos(\"Jos√© Mar√≠a\") -> \"Jose Maria\"\n",
    "    sacar_acentos(\"C√≥rdoba\") -> \"Cordoba\"\n",
    "\n",
    "Tecnica utilizada:\n",
    "    - Normalizacion Unicode NFKD: Separa caracteres base de marcas diacriticas\n",
    "    - Filtracion: Elimina solo las marcas diacriticas, conservando la letra base\n",
    "\"\"\"\n",
    "\n",
    "def sacar_acentos(texto):\n",
    "    # Verificar si el valor es nulo (NaN, None, etc.)\n",
    "    if pd.isna(texto):\n",
    "        return texto\n",
    "    \n",
    "    # Convertir a string por seguridad (por si recibe numeros u otros tipos)\n",
    "    texto = str(texto)\n",
    "    \n",
    "    # Normalizar a formato NFKD: separa caracteres base de acentos\n",
    "    # Ejemplo: \"√°\" se convierte en \"a\" + \"¬¥\" (letra + tilde)\n",
    "    texto_normalizado = unicodedata.normalize('NFKD', texto)\n",
    "    \n",
    "    # Filtrar solo caracteres que NO sean marcas diacriticas\n",
    "    # unicodedata.combining(c) devuelve True si es una tilde, dieresis, etc.\n",
    "    texto_sin_acentos = ''.join([c for c in texto_normalizado if not unicodedata.combining(c)])\n",
    "    \n",
    "    return texto_sin_acentos\n",
    "\n",
    "\n",
    "# Pruebas de la funcion\n",
    "print(\"Pruebas de la funcion sacar_acentos:\")\n",
    "print(f\"  sacar_acentos('Jos√© Mar√≠a') = '{sacar_acentos('Jos√© Mar√≠a')}'\")\n",
    "print(f\"  sacar_acentos('C√≥rdoba') = '{sacar_acentos('C√≥rdoba')}'\")\n",
    "print(f\"  sacar_acentos('√ëo√±o') = '{sacar_acentos('√ëo√±o')}'\")\n",
    "print(f\"  sacar_acentos(None) = {sacar_acentos(None)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Funcion: safe_str\n",
    "\n",
    "Proposito:\n",
    "    Convierte un valor a string de forma segura, manejando valores nulos\n",
    "    sin generar errores.\n",
    "\n",
    "Parametros:\n",
    "    x (any): Valor a convertir a string\n",
    "\n",
    "Retorna:\n",
    "    str: String vacio si el valor es nulo, sino el valor convertido a string\n",
    "\n",
    "Uso:\n",
    "    Esta funcion es util al procesar columnas donde pueden existir valores\n",
    "    NaN, None o similares que causarian errores al aplicar metodos de string.\n",
    "\n",
    "Ejemplo:\n",
    "    safe_str(None) -> \"\"\n",
    "    safe_str(np.nan) -> \"\"\n",
    "    safe_str(123) -> \"123\"\n",
    "    safe_str(\"texto\") -> \"texto\"\n",
    "\"\"\"\n",
    "\n",
    "def safe_str(x):\n",
    "    # Verificar si el valor es nulo usando la funcion de pandas\n",
    "    if pd.isna(x):\n",
    "        return \"\"  # Devolver string vacio en lugar de \"nan\" o error\n",
    "    \n",
    "    # Convertir a string de forma segura\n",
    "    return str(x)\n",
    "\n",
    "\n",
    "# Pruebas de la funcion\n",
    "print(\"Pruebas de la funcion safe_str:\")\n",
    "print(f\"  safe_str(None) = '{safe_str(None)}'\")\n",
    "print(f\"  safe_str(np.nan) = '{safe_str(np.nan)}'\")\n",
    "print(f\"  safe_str(123) = '{safe_str(123)}'\")\n",
    "print(f\"  safe_str('texto') = '{safe_str('texto')}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color: #d3d3d3; padding: 15px; text-align: center; border-radius: 8px;'>\n",
    "    <h3 style='color: #1e3a8a; margin: 0;'>SECCION 5: DETECCION DE VALORES PERDIDOS</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Funcion: buscar_valores_perdidos\n",
    "\n",
    "Proposito:\n",
    "    Identifica si un valor debe considerarse como \"dato faltante\" o \"perdido\",\n",
    "    mas alla de los NaN estandar de pandas.\n",
    "\n",
    "Parametros:\n",
    "    valor (any): Valor a verificar (puede ser string, numero, None, etc.)\n",
    "\n",
    "Retorna:\n",
    "    bool: True si el valor representa un dato perdido, False en caso contrario\n",
    "\n",
    "Logica de deteccion:\n",
    "    1. Verifica si es un NaN reconocido por pandas\n",
    "    2. Convierte a string, elimina espacios y pasa a minusculas\n",
    "    3. Elimina acentos para evitar variaciones ortograficas\n",
    "    4. Compara contra una lista de expresiones comunes de datos faltantes\n",
    "\n",
    "Valores considerados como perdidos:\n",
    "    - Strings vacios: '', '   '\n",
    "    - Variaciones de NA: 'na', 'n/a', 'N/A'\n",
    "    - Palabras: 'null', 'none', 'sin dato', 's/d', 'nd'\n",
    "    - Simbolos: '-', '--', '?'\n",
    "\n",
    "Ejemplo:\n",
    "    buscar_valores_perdidos('N/A') -> True\n",
    "    buscar_valores_perdidos('  ') -> True\n",
    "    buscar_valores_perdidos('Sin dato') -> True\n",
    "    buscar_valores_perdidos('Juan') -> False\n",
    "    buscar_valores_perdidos(np.nan) -> True\n",
    "\"\"\"\n",
    "\n",
    "def buscar_valores_perdidos(valor):\n",
    "    # Paso 1: Verificar si ya es un NaN reconocido por pandas\n",
    "    if pd.isna(valor):\n",
    "        return True\n",
    "    \n",
    "    # Paso 2: Normalizar el valor a texto para comparacion\n",
    "    # - Convertir a string\n",
    "    # - Eliminar espacios al inicio y final\n",
    "    # - Convertir a minusculas\n",
    "    valor_texto = str(valor).strip().lower()\n",
    "    \n",
    "    # Paso 3: Eliminar acentos para evitar problemas con variaciones ortograficas\n",
    "    # Ejemplo: \"Sin Dato\" y \"Sin Dat√≥\" se normalizan a \"sin dato\"\n",
    "    valor_texto = sacar_acentos(valor_texto)\n",
    "    \n",
    "    # Paso 4: Definir conjunto de expresiones que representan datos faltantes\n",
    "    # Se usa un conjunto (set) para busqueda eficiente O(1)\n",
    "    valores_faltantes = {\n",
    "        '',           # String vacio\n",
    "        'na',         # Not Available\n",
    "        'n/a',        # Not Available (con barra)\n",
    "        'null',       # Valor nulo\n",
    "        'none',       # Sin valor\n",
    "        'sin dato',   # Expresion en espa√±ol\n",
    "        's/d',        # Sin datos (abreviado)\n",
    "        'nd',         # No disponible\n",
    "        '-',          # Guion simple\n",
    "        '--',         # Guion doble\n",
    "        '?',          # Signo de interrogacion\n",
    "        'sin_dato'    # Con guion bajo\n",
    "    }\n",
    "    \n",
    "    # Paso 5: Verificar si el valor normalizado esta en el conjunto\n",
    "    return valor_texto in valores_faltantes\n",
    "\n",
    "\n",
    "# Pruebas de la funcion\n",
    "print(\"Pruebas de la funcion buscar_valores_perdidos:\")\n",
    "print(f\"  buscar_valores_perdidos('N/A') = {buscar_valores_perdidos('N/A')}\")\n",
    "print(f\"  buscar_valores_perdidos('  ') = {buscar_valores_perdidos('  ')}\")\n",
    "print(f\"  buscar_valores_perdidos('Sin Dato') = {buscar_valores_perdidos('Sin Dato')}\")\n",
    "print(f\"  buscar_valores_perdidos('Juan') = {buscar_valores_perdidos('Juan')}\")\n",
    "print(f\"  buscar_valores_perdidos(np.nan) = {buscar_valores_perdidos(np.nan)}\")\n",
    "print(f\"  buscar_valores_perdidos('--') = {buscar_valores_perdidos('--')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color: #d3d3d3; padding: 15px; text-align: center; border-radius: 8px;'>\n",
    "    <h3 style='color: #1e3a8a; margin: 0;'>SECCION 6: DETECCION DE VALORES ATIPICOS</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Funcion: mascara_valores_atipicos_rango_intercuartil\n",
    "\n",
    "Proposito:\n",
    "    Identifica valores atipicos (outliers) en una serie numerica utilizando\n",
    "    el metodo del Rango Intercuartilico (IQR - Interquartile Range).\n",
    "\n",
    "Parametros:\n",
    "    serie_datos (pd.Series): Serie numerica a analizar (columna de un DataFrame)\n",
    "\n",
    "Retorna:\n",
    "    pd.Series: Serie booleana del mismo tama√±o que la entrada\n",
    "               - True: el valor es atipico\n",
    "               - False: el valor esta dentro del rango normal\n",
    "\n",
    "Metodo IQR:\n",
    "    1. Calcular Q1 (percentil 25) y Q3 (percentil 75)\n",
    "    2. Calcular IQR = Q3 - Q1\n",
    "    3. Establecer limites:\n",
    "       - Limite inferior = Q1 - 1.5 * IQR\n",
    "       - Limite superior = Q3 + 1.5 * IQR\n",
    "    4. Los valores fuera de estos limites son atipicos\n",
    "\n",
    "Ventajas del metodo IQR:\n",
    "    - Robusto ante distribuciones asimetricas\n",
    "    - No asume normalidad de los datos\n",
    "    - Resistente a valores extremos\n",
    "\n",
    "Consideraciones:\n",
    "    - Se requieren al menos 4 valores no nulos para calcular cuartiles\n",
    "    - Los valores nulos (NaN) son ignorados en el calculo\n",
    "    - Si no hay suficientes datos, devuelve False para todos los valores\n",
    "\n",
    "Ejemplo:\n",
    "    serie = pd.Series([10, 12, 11, 13, 100, 9, 14])\n",
    "    mascara = mascara_valores_atipicos_rango_intercuartil(serie)\n",
    "    # Resultado: [False, False, False, False, True, False, False]\n",
    "    # El valor 100 es detectado como atipico\n",
    "\"\"\"\n",
    "\n",
    "def mascara_valores_atipicos_rango_intercuartil(serie_datos):\n",
    "    # Paso 1: Limpiar la serie de valores nulos y convertir a float\n",
    "    serie_limpia = serie_datos.dropna().astype(float)\n",
    "    \n",
    "    # Paso 2: Verificar si hay suficientes datos para calcular cuartiles\n",
    "    # Se necesitan al menos 4 valores para tener Q1 y Q3 significativos\n",
    "    if serie_limpia.shape[0] < 4:\n",
    "        # Si no hay suficientes datos, ningun valor se considera atipico\n",
    "        return pd.Series([False] * len(serie_datos), index=serie_datos.index)\n",
    "    \n",
    "    # Paso 3: Calcular los cuartiles\n",
    "    # Q1 (percentil 25): El 25% de los datos estan por debajo de este valor\n",
    "    cuartil_1 = serie_limpia.quantile(0.25)\n",
    "    \n",
    "    # Q3 (percentil 75): El 75% de los datos estan por debajo de este valor\n",
    "    cuartil_3 = serie_limpia.quantile(0.75)\n",
    "    \n",
    "    # Paso 4: Calcular el Rango Intercuartilico (IQR)\n",
    "    # El IQR contiene el 50% central de los datos\n",
    "    rango_intercuartil = cuartil_3 - cuartil_1\n",
    "    \n",
    "    # Paso 5: Calcular los limites para considerar valores atipicos\n",
    "    # El factor 1.5 es una convencion estadistica ampliamente aceptada\n",
    "    limite_inferior = cuartil_1 - 1.5 * rango_intercuartil\n",
    "    limite_superior = cuartil_3 + 1.5 * rango_intercuartil\n",
    "    \n",
    "    # Paso 6: Crear mascara booleana\n",
    "    # El operador | es un OR logico (barra vertical)\n",
    "    # Un valor es atipico si esta por debajo del limite inferior\n",
    "    # O (|) si esta por encima del limite superior\n",
    "    mascara_atipicos = (serie_datos < limite_inferior) | (serie_datos > limite_superior)\n",
    "    \n",
    "    return mascara_atipicos\n",
    "\n",
    "\n",
    "# Pruebas de la funcion\n",
    "print(\"Pruebas de la funcion mascara_valores_atipicos_rango_intercuartil:\")\n",
    "serie_prueba = pd.Series([10, 12, 11, 13, 100, 9, 14, 12, 11])\n",
    "mascara_resultado = mascara_valores_atipicos_rango_intercuartil(serie_prueba)\n",
    "print(f\"\\nSerie original: {list(serie_prueba)}\")\n",
    "print(f\"Mascara atipicos: {list(mascara_resultado)}\")\n",
    "print(f\"Valores atipicos detectados: {list(serie_prueba[mascara_resultado])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Funcion: mascara_valores_atipicos_zscore\n",
    "\n",
    "Proposito:\n",
    "    Identifica valores atipicos usando el metodo del puntaje Z (Z-Score),\n",
    "    que mide cuantas desviaciones estandar se aleja cada valor de la media.\n",
    "\n",
    "Parametros:\n",
    "    serie_datos (pd.Series): Serie numerica a analizar\n",
    "    umbral (float): Numero de desviaciones estandar para considerar atipico\n",
    "                    Por defecto: 3.0 (valor estandar en estadistica)\n",
    "\n",
    "Retorna:\n",
    "    pd.Series: Serie booleana indicando valores atipicos\n",
    "\n",
    "Formula del Z-Score:\n",
    "    Z = (X - media) / desviacion_estandar\n",
    "\n",
    "Interpretacion:\n",
    "    - Z entre -3 y 3: valor normal (dentro de 3 desviaciones estandar)\n",
    "    - |Z| > 3: valor atipico (muy alejado de la media)\n",
    "\n",
    "Cuando usar Z-Score vs IQR:\n",
    "    - Z-Score: Mejor para datos con distribucion normal (gaussiana)\n",
    "    - IQR: Mejor para datos asimetricos o con muchos valores extremos\n",
    "\n",
    "Consideraciones:\n",
    "    - Requiere al menos 4 valores para calcular media y desviacion\n",
    "    - Si todos los valores son iguales (desviacion = 0), no hay atipicos
    - Sensible a valores extremos en la media y desviacion

Ejemplo:
    serie = pd.Series([10, 12, 11, 13, 50, 9, 14])
    mascara = mascara_valores_atipicos_zscore(serie, umbral=2.0)
    # El valor 50 probablemente sera detectado como atipico
"""

def mascara_valores_atipicos_zscore(serie_datos, umbral=3.0):
    # Paso 1: Limpiar la serie de valores nulos
    serie_limpia = serie_datos.dropna().astype(float)

    # Paso 2: Verificar condiciones minimas
    # - Al menos 4 valores para tener estadisticas significativas
    # - Desviacion estandar diferente de cero (valores no todos iguales)
    if serie_limpia.shape[0] < 4 or serie_limpia.std() == 0:
        # Si no se cumplen las condiciones, ningun valor es atipico
        return pd.Series([False] * len(serie_datos), index=serie_datos.index)

    # Paso 3: Calcular el puntaje Z para cada valor
    # Formula: Z = (X - media) / desviacion_estandar
    media = serie_limpia.mean()
    desviacion_estandar = serie_limpia.std()

    puntaje_z = (serie_datos - media) / desviacion_estandar

    # Paso 4: Identificar atipicos
    # Un valor es atipico si su puntaje Z (en valor absoluto) supera el umbral
    # abs() convierte valores negativos a positivos para comparar distancia
    mascara_atipicos = puntaje_z.abs() > umbral

    return mascara_atipicos


# Pruebas de la funcion
print("Pruebas de la funcion mascara_valores_atipicos_zscore:")
serie_prueba = pd.Series([10, 12, 11, 13, 50, 9, 14, 12, 11])
mascara_resultado = mascara_valores_atipicos_zscore(serie_prueba, umbral=2.0)
print(f"\nSerie original: {list(serie_prueba)}")
print(f"Mascara atipicos: {list(mascara_resultado)}")
print(f"Valores atipicos detectados: {list(serie_prueba[mascara_resultado])}")
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color: #d3d3d3; padding: 15px; text-align: center; border-radius: 8px;'>\n",
    "    <h3 style='color: #1e3a8a; margin: 0;'>SECCION 7: DETECCION COMPLETA DE PROBLEMAS EN DATAFRAMES</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Funcion: detectar_problemas_dataframe\n",
    "\n",
    "Proposito:\n",
    "    Analiza un DataFrame completo y detecta diversos tipos de problemas\n",
    "    de calidad de datos, generando un reporte detallado.\n",
    "\n",
    "Parametros:\n",
    "    dataframe (pd.DataFrame): DataFrame a analizar\n",
    "\n",
    "Retorna:\n",
    "    tuple de 3 elementos:\n",
    "        1. resumen_general (dict): Informacion global del DataFrame\n",
    "        2. chequeos_por_columna (dict): Analisis detallado de cada columna\n",
    "        3. problemas_dataframe (pd.DataFrame): Listado de filas con problemas\n",
    "\n",
    "Tipos de problemas detectados:\n",
    "    - Valores nulos y faltantes\n",
    "    - Filas duplicadas\n",
    "    - Espacios en blanco al inicio/final de textos\n",
    "    - Inconsistencias de mayusculas/minusculas\n",
    "    - Variantes con y sin acentos del mismo valor\n",
    "    - Valores atipicos numericos (IQR y Z-Score)\n",
    "    - Tokens de datos faltantes (NA, null, etc.)\n",
    "\n",
    "Analisis por tipo de columna:\n",
    "    - Columnas de texto: espacios, mayusculas, acentos, valores unicos\n",
    "    - Columnas numericas: estadisticas descriptivas, outliers\n",
    "    - Columnas de fecha: validacion de conversion a datetime\n",
    "\"\"\"\n",
    "\n",
    "def detectar_problemas_dataframe(dataframe):\n",
    "    # =====================================================\n",
    "    # PARTE 1: ANALISIS GENERAL DEL DATAFRAME\n",
    "    # =====================================================\n",
    "    \n",
    "    resumen_general = {\n",
    "        'cantidad_filas': dataframe.shape[0],\n",
    "        'cantidad_columnas': dataframe.shape[1],\n",
    "        'nombres_columnas': list(dataframe.columns.astype(str)),\n",
    "        'tipos_datos': dataframe.dtypes.apply(lambda x: str(x)).to_dict(),\n",
    "        'nulos_por_columna': dataframe.isna().sum().to_dict()\n",
    "    }\n",
    "    \n",
    "    # Identificar filas duplicadas completas\n",
    "    # keep=False marca TODAS las ocurrencias como duplicadas (no solo la segunda)\n",
    "    mascara_duplicados = dataframe.duplicated(keep=False)\n",
    "    resumen_general['filas_duplicadas'] = int(mascara_duplicados.sum())\n",
    "    \n",
    "    # Detectar posibles columnas identificadoras (ID, DNI, CUIL, etc.)\n",
    "    palabras_clave_id = ['id', 'dni', 'cuil', 'cuit', 'legajo', 'codigo', 'clave']\n",
    "    posibles_identificadores = [\n",
    "        c for c in dataframe.columns.astype(str)\n",
    "        if any(palabra in c.lower() for palabra in palabras_clave_id)\n",
    "    ]\n",
    "    resumen_general['posibles_identificadores'] = posibles_identificadores\n",
    "    \n",
    "    # =====================================================\n",
    "    # PARTE 2: ANALISIS DETALLADO POR COLUMNA\n",
    "    # =====================================================\n",
    "    \n",
    "    chequeos_por_columna = {}\n",
    "    \n",
    "    for nombre_columna in dataframe.columns:\n",
    "        serie = dataframe[nombre_columna]\n",
    "        info_columna = {\n",
    "            'tipo_dato': str(serie.dtype),\n",
    "            'cantidad_nulos': int(serie.isna().sum())\n",
    "        }\n",
    "        \n",
    "        # -----------------------------------------------------\n",
    "        # ANALISIS DE COLUMNAS DE TEXTO\n",
    "        # -----------------------------------------------------\n",
    "        if serie.dtype == object or pd.api.types.is_string_dtype(serie):\n",
    "            serie_texto = serie.astype(str)\n",
    "            \n",
    "            # Detectar espacios en blanco al inicio o final\n",
    "            # Patron regex: ^\\s+ detecta espacios al inicio\n",
    "            info_columna['espacios_inicio'] = int(serie_texto.str.match(r'^\\s+').sum())\n",
    "            # Patron regex: \\s+$ detecta espacios al final\n",
    "            info_columna['espacios_final'] = int(serie_texto.str.match(r'\\s+).sum())\n",
    "            \n",
    "            # Comparar variantes de mayusculas/minusculas\n",
    "            try:\n",
    "                unicos_original = set(serie_texto.dropna().unique())\n",
    "                unicos_minuscula = set(serie_texto.dropna().str.lower().unique())\n",
    "                \n",
    "                info_columna['valores_unicos'] = len(unicos_original)\n",
    "                info_columna['valores_unicos_minuscula'] = len(unicos_minuscula)\n",
    "                \n",
    "                # Si hay menos valores unicos al convertir a minuscula,\n",
    "                # significa que existen variantes por mayusculas\n",
    "                # Ejemplo: \"Juan\" y \"JUAN\" se convierten ambos a \"juan\"\n",
    "                info_columna['existen_variantes_mayusculas'] = len(unicos_minuscula) < len(unicos_original)\n",
    "            except Exception:\n",
    "                info_columna['valores_unicos'] = int(serie.nunique(dropna=True))\n",
    "                info_columna['valores_unicos_minuscula'] = None\n",
    "                info_columna['existen_variantes_mayusculas'] = None\n",
    "            \n",
    "            # Detectar variantes con y sin acentos\n",
    "            try:\n",
    "                # Normalizar: quitar acentos y pasar a minuscula\n",
    "                serie_sin_acentos = serie_texto.dropna().map(\n",
    "                    lambda x: sacar_acentos(x).lower()\n",
    "                )\n",
    "                \n",
    "                # Contar cuantos valores normalizados tienen multiples variantes originales\n",
    "                conteo_grupos = serie_sin_acentos.groupby(serie_sin_acentos).size()\n",
    "                conflictos = conteo_grupos[conteo_grupos > 1]\n",
    "                \n",
    "                info_columna['grupos_variantes_acentos'] = int(conflictos.shape[0])\n",
    "                \n",
    "                # Guardar ejemplos de las primeras 5 variantes\n",
    "                ejemplos_conflictos = {}\n",
    "                if not conflictos.empty:\n",
    "                    for valor in conflictos.index[:5]:\n",
    "                        # Buscar todas las variantes originales de este valor normalizado\n",
    "                        originales = sorted(\n",
    "                            list(serie_texto[serie_sin_acentos == valor].unique())[:10]\n",
    "                        )\n",
    "                        ejemplos_conflictos[valor] = originales\n",
    "                \n",
    "                info_columna['ejemplos_variantes_acentos'] = ejemplos_conflictos\n",
    "            except Exception:\n",
    "                info_columna['grupos_variantes_acentos'] = None\n",
    "                info_columna['ejemplos_variantes_acentos'] = {}\n",
    "            \n",
    "            # Detectar tokens de valores faltantes (NA, null, etc.)\n",
    "            info_columna['tokens_faltantes'] = int(\n",
    "                serie_texto.map(lambda v: buscar_valores_perdidos(v)).sum()\n",
    "            )\n",
    "            \n",
    "            # Guardar muestra de valores para inspeccion manual\n",
    "            info_columna['muestra_valores'] = list(serie_texto.dropna().unique()[:10])\n",
    "        \n",
    "        # -----------------------------------------------------\n",
    "        # ANALISIS DE COLUMNAS NUMERICAS\n",
    "        # -----------------------------------------------------\n",
    "        elif pd.api.types.is_numeric_dtype(serie):\n",
    "            serie_numerica = serie.dropna().astype(float)\n",
    "            \n",
    "            if not serie_numerica.empty:\n",
    "                # Estadisticas descriptivas basicas\n",
    "                info_columna['media'] = float(serie_numerica.mean())\n",
    "                info_columna['desviacion_estandar'] = float(serie_numerica.std())\n",
    "                info_columna['minimo'] = float(serie_numerica.min())\n",
    "                info_columna['maximo'] = float(serie_numerica.max())\n",
    "            else:\n",
    "                info_columna.update({\n",
    "                    'media': None,\n",
    "                    'desviacion_estandar': None,\n",
    "                    'minimo': None,\n",
    "                    'maximo': None\n",
    "                })\n",
    "            \n",
    "            # Deteccion de outliers por metodo IQR\n",
    "            if len(serie_numerica) >= 4:\n",
    "                q1 = serie_numerica.quantile(0.25)\n",
    "                q3 = serie_numerica.quantile(0.75)\n",
    "                rango_iqr = q3 - q1\n",
    "                \n",
    "                limite_inferior = q1 - 1.5 * rango_iqr\n",
    "                limite_superior = q3 + 1.5 * rango_iqr\n",
    "                \n",
    "                mascara_atipicos = (\n",
    "                    (serie_numerica < limite_inferior) | \n",
    "                    (serie_numerica > limite_superior)\n",
    "                )\n",
    "                \n",
    "                info_columna['valores_atipicos_iqr'] = int(mascara_atipicos.sum())\n",
    "                info_columna['limites_iqr'] = (\n",
    "                    float(limite_inferior), \n",
    "                    float(limite_superior)\n",
    "                )\n",
    "            else:\n",
    "                info_columna['valores_atipicos_iqr'] = None\n",
    "                info_columna['limites_iqr'] = None\n",
    "            \n",
    "            # Deteccion de outliers por Z-Score\n",
    "            if len(serie_numerica) >= 4 and serie_numerica.std() != 0:\n",
    "                z = (serie_numerica - serie_numerica.mean()) / serie_numerica.std()\n",
    "                info_columna['valores_atipicos_z'] = int((z.abs() > 3).sum())\n",
    "            else:\n",
    "                info_columna['valores_atipicos_z'] = None\n",
    "        \n",
    "        # -----------------------------------------------------\n",
    "        # ANALISIS DE COLUMNAS DE FECHA\n",
    "        # -----------------------------------------------------\n",
    "        else:\n",
    "            # Intentar convertir a fecha para validar formato\n",
    "            fechas_convertidas = pd.to_datetime(serie, errors='coerce', dayfirst=True)\n",
    "            info_columna['valores_fecha_convertidos'] = int(fechas_convertidas.notna().sum())\n",
    "            info_columna['muestra_valores'] = list(serie.dropna().unique()[:10])\n",
    "        \n",
    "        # Guardar informacion de esta columna\n",
    "        chequeos_por_columna[str(nombre_columna)] = info_columna\n",
    "    \n",
    "    # =====================================================\n",
    "    # PARTE 3: REVISION FILA POR FILA\n",
    "    # =====================================================\n",
    "    \n",
    "    filas_con_problemas = []\n",
    "    \n",
    "    for indice, fila in dataframe.iterrows():\n",
    "        lista_problemas = []\n",
    "        \n",
    "        # Verificar si la fila esta duplicada\n",
    "        if mascara_duplicados.loc[indice]:\n",
    "            lista_problemas.append(\"fila_duplicada\")\n",
    "        \n",
    "        # Revisar cada columna de la fila\n",
    "        for nombre_columna in dataframe.columns:\n",
    "            valor = fila[nombre_columna]\n",
    "            info_columna = chequeos_por_columna[str(nombre_columna)]\n",
    "            \n",
    "            # Problemas en valores de texto\n",
    "            if isinstance(valor, str):\n",
    "                # Espacios en blanco\n",
    "                if valor != valor.strip():\n",
    "                    lista_problemas.append(f\"espacios_en_columna_{nombre_columna}\")\n",
    "                \n",
    "                # Inconsistencias de mayusculas\n",
    "                if info_columna.get('existen_variantes_mayusculas'):\n",
    "                    lista_problemas.append(f\"inconsistencia_mayusculas_{nombre_columna}\")\n",
    "                \n",
    "                # Variantes de acentos\n",
    "                if info_columna.get('grupos_variantes_acentos', 0) > 0:\n",
    "                    lista_problemas.append(f\"variantes_acentos_{nombre_columna}\")\n",
    "                \n",
    "                # Token de valor faltante\n",
    "                if buscar_valores_perdidos(valor):\n",
    "                    lista_problemas.append(f\"token_faltante_{nombre_columna}\")\n",
    "            \n",
    "            # Problemas en valores numericos\n",
    "            elif pd.api.types.is_numeric_dtype(type(valor)):\n",
    "                # Outlier por IQR\n",
    "                limites_iqr = info_columna.get('limites_iqr')\n",
    "                if limites_iqr and (valor < limites_iqr[0] or valor > limites_iqr[1]):\n",
    "                    lista_problemas.append(f\"valor_atipico_iqr_{nombre_columna}\")\n",
    "                \n",
    "                # Outlier por Z-Score\n",
    "                media = info_columna.get('media')\n",
    "                desvio = info_columna.get('desviacion_estandar')\n",
    "                if desvio not in (None, 0):\n",
    "                    if abs((valor - media) / desvio) > 3:\n",
    "                        lista_problemas.append(f\"valor_atipico_z_{nombre_columna}\")\n",
    "        \n",
    "        # Si se encontraron problemas, agregar la fila al reporte\n",
    "        if lista_problemas:\n",
    "            filas_con_problemas.append({\n",
    "                'indice_fila': indice,\n",
    "                'problemas_detectados': ';'.join(sorted(set(lista_problemas))),\n",
    "                'muestra_fila': json.dumps({\n",
    "                    str(c): str(fila[c]) for c in dataframe.columns[:8]\n",
    "                })\n",
    "            })\n",
    "    \n",
    "    # Convertir lista de problemas a DataFrame\n",
    "    problemas_dataframe = pd.DataFrame(filas_con_problemas)\n",
    "    \n",
    "    return resumen_general, chequeos_por_columna, problemas_dataframe\n",
    "\n",
    "\n",
    "print(\"Funcion detectar_problemas_dataframe definida correctamente\")
