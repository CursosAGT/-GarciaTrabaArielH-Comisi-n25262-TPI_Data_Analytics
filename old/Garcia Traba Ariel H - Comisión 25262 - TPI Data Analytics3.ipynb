{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cedbb1b4",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color:#CCCCCC; padding:12px; border-radius:8px;\">\n",
    "<h1 style=\"color:#003366; text-align:center; margin:8px 0;\">Revisión y limpieza de 3 DataFrames (TPI - Data Analytics)</h1>\n",
    "<p style=\"text-align:center; color:#003366; margin:0;\"><em>Notebook docente en castellano — nombres descriptivos en snake_case — código y documentación</em></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c723b86a",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color:#CCCCCC; padding:10px; border-radius:6px;\">\n",
    "<h2 style=\"color:black; text-align:center; margin-top:6px;\">Resumen</h2>\n",
    "\n",
    "<p style=\"color:black;\">\n",
    "Este notebook está diseñado con finalidades pedagógicas. Revisa, normaliza y valida tres datasets contenidos en CSV:\n",
    "</p>\n",
    "\n",
    "<ul style=\"color:black;\">\n",
    "<li><code>marketing.csv</code> → variable: <code>df_marketing</code></li>\n",
    "<li><code>ventas.csv</code>    → variable: <code>df_ventas</code></li>\n",
    "<li><code>clientes.csv</code>  → variable: <code>df_clientes</code></li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"color:black;\">\n",
    "Coloca los CSV en <code>./data_in/</code> o en <code>/mnt/data/</code>. El notebook busca primero en <code>./data_in/</code> y si no encuentra, usa <code>/mnt/data/</code> (útil para entornos donde los archivos están pre-subidos).\n",
    "</p>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be46fb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports y configuración inicial (nombres en castellano)\n",
    "import os, re, json, unicodedata\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from math import isnan\n",
    "import zipfile\n",
    "from collections import defaultdict\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Any, Optional\n",
    "ruta_base = \"./\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63edff5b-828e-4254-a4d7-4b0ef05ecea6",
   "metadata": {},
   "source": [
    "## 1. Crear un documento en Google Colaboratory y cargar los sets de datos como DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfa535f-ef52-4f49-9d1b-e70dd8d5ad27",
   "metadata": {},
   "source": [
    "si se usa en disco local comentarla celda de debajo (JuPyteR , VSC, ATOM, Spider, Geany, etc)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "00de9cf8-da0d-4d31-bed7-31a704a7274d",
   "metadata": {},
   "source": [
    "# --- Paso 1: Montar Google Drive ---\n",
    "# Montar tu Google Drive\n",
    "!pip install google\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!ls \"/content/drive/MyDrive/CABA/Garcia Traba Ariel H - Comisión 25262 - TPI Data Analytics/\"\n",
    "# Ruta del archivo (ajústala a la carpeta real en tu Drive)\n",
    "ruta_base = \"/content/drive/MyDrive/CABA/Garcia Traba Ariel H - Comisión 25262 - TPI Data Analytics/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144444b3-1dc1-4f3f-a621-025d1cdb6465",
   "metadata": {},
   "source": [
    "# 1ra parte Definición de ETL\n",
    "ETL es un conjunto de procedimientos que permiten mover datos desde sistemas de origen, que pueden ser bases de datos, archivos o fuentes en la nube, hasta un sistema de destino como un data warehouse o data lake, realizando previamente procesos de limpieza, estructuración y organización de los datos para hacerlos aptos para análisis.​\n",
    "\n",
    "## Fases del proceso ETL\n",
    "Extracción: Consiste en recopilar datos relevantes de diferentes fuentes, asegurando que el impacto en los sistemas origen sea mínimo. Los datos pueden extraerse mediante diversos métodos como consultas SQL o servicios web.​\n",
    "\n",
    "Transformación: En esta etapa, los datos se limpian y se ajustan para garantizar coherencia y calidad, incluyendo la eliminación de valores nulos, normalización y conversión a formatos consistentes, además de aplicar reglas específicas de negocio.\n",
    "\n",
    "Carga: Finalmente, los datos transformados se cargan en el sistema de destino, donde estarán disponibles para análisis, informes o modelado de datos.\n",
    "\n",
    "## Importancia del ETL\n",
    "Es crucial en la minería de datos porque preparar los datos brutos para que puedan ser utilizados en análisis estadísticos, modelados predictivos o técnicas de aprendizaje automático, asegurando la calidad, coherencia y accesibilidad de la información."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4225f2e7-fccf-4a13-86a6-92acee434aa7",
   "metadata": {},
   "source": [
    "## desde python sin librerias pandas / polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86754f0-a78b-453c-9127-5abd20e1877d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876facae-5d32-47aa-8b55-d9b04deadb85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c8f6200-1ad0-4fa8-9cf1-aa1df22c4b3b",
   "metadata": {},
   "source": [
    "### 1.1 Crear estructura de directorios segun modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "605182b8-9652-410e-8c79-13b10a33e692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rutas: \n",
    "\n",
    "carpeta_entrada    = Path(ruta_base)\n",
    "#carpeta_entrada_mnt   = Path('/mnt/data')\n",
    "carpeta_datasets_entrada   = carpeta_entrada / 'datasets_entrada'\n",
    "carpeta_datasets_entrada.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "carpeta_datasets_salida   = carpeta_entrada / 'datasets_salida'\n",
    "carpeta_datasets_salida.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "carpeta_reportes   = carpeta_datasets_salida / 'reportes'\n",
    "carpeta_reportes.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "carpeta_limpios    = carpeta_datasets_salida / 'limpios'\n",
    "carpeta_limpios.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Nombres esperados de archivos\n",
    "archivo_ventas     = 'ventas.csv'\n",
    "archivo_clientes   = 'clientes.csv'\n",
    "archivo_marketing  = 'marketing.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b65281a-0cd0-43a6-8a84-ed2692ae60b2",
   "metadata": {},
   "source": [
    "### 1.2 rutas y carga de los dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "052e6d76-d227-4749-af2c-f065eabf82f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datasets del curso...\n",
      "...Arhivos cargados con exito!!!\n"
     ]
    }
   ],
   "source": [
    "# --- Paso 3: Cargar archivos del curso ---\n",
    "print(\"Cargando datasets del curso...\")\n",
    "try:\n",
    "    ruta_ventas     = os.path.join(carpeta_datasets_entrada, archivo_ventas)\n",
    "    ruta_clientes   = os.path.join(carpeta_datasets_entrada, archivo_clientes)\n",
    "    ruta_marketing  = os.path.join(carpeta_datasets_entrada, archivo_marketing)\n",
    "    \n",
    "    df_ventas       = pd.read_csv(f\"{ruta_ventas}\")\n",
    "    df_clientes     = pd.read_csv(f\"{ruta_clientes}\")\n",
    "    df_marketing    = pd.read_csv(f\"{ruta_marketing}\")\n",
    "    dic_dfs = { \"df_ventas\"   : df_ventas,\n",
    "                \"df_clientes\" : df_clientes,\n",
    "                \"df_marketing\": df_marketing}\n",
    "    print (\"...Arhivos cargados con exito!!!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Archivos no encontrados en:\", carpeta_datasets_entrada)\n",
    "    sys.exit(1)\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"Archivo vacío detectado\")\n",
    "    sys.exit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e91d4ff-4465-4ec3-b5e1-29f5d4cc0277",
   "metadata": {},
   "source": [
    "### 1.3 Estructura de parámetros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "895c63d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Proceso principal para los 3 CSV ----------\n",
    "desviacion_margen     = 1.5\n",
    "desviacion_umbral     = 3.0\n",
    "cantidad_duplicados   = 0\n",
    "reportes_creados      = []\n",
    "ruta_excel            = carpeta_reportes / 'reporte_limpieza.xlsx'\n",
    "guardado_ok           = False\n",
    "mensajes              = []\n",
    "TOKENS_VALOR_FALTANTE = {'na', 'n/a', 'null', 'none', 'sin dato', 's/d', 'nd', '-', '--', '?', 'sin_dato', 'n/d'}\n",
    "\n",
    "\n",
    "reglas= {\n",
    "            \"df_marketing\" : {\n",
    "                                'producto':     {'string' : {'tipo': 'lower', 'normalizar_acentos': True}},\n",
    "                                'canal':        {'string' : {'tipo': 'upper', 'normalizar_acentos': True}},\n",
    "                                'costo':        {'numeric': {'as_int': False}},\n",
    "                                'fecha_inicio': {'date'   : {'dayfirst': True, 'formats': ['%d/%m/%Y', '%Y-%m-%d']}},\n",
    "                                'fecha_fin':    {'date'   : {'dayfirst': True, 'formats': ['%d/%m/%Y', '%Y-%m-%d']}}\n",
    "                                },\n",
    "            # id_venta, producto, precio, cantidad, fecha_venta, categoria\n",
    "            \"df_ventas\" : {\n",
    "                                'producto':     {'string' : {'tipo': 'lower', 'normalizar_acentos': True}},\n",
    "                                'precio':       {'numeric': {'as_int': False}},\n",
    "                                'cantidad':     {'numeric': {'as_int': False}},\n",
    "                                'fecha_venta':  {'date'   : {'dayfirst': True, 'formats': ['%d/%m/%Y', '%Y-%m-%d']}},\n",
    "                                'categoria':    {'string' : {'tipo': 'lower', 'normalizar_acentos': True}}\n",
    "                            },\n",
    "            # id_cliente, nombre, edad, ciudad, ingresos\n",
    "            \"df_clientes\" : {\n",
    "                                'nombre':       {'string'  : {'tipo': 'title', 'normalizar_acentos': True}},\n",
    "                                'edad':         {'numeric' : {'as_int': True}},\n",
    "                                'ciudad':       {'string'  : {'tipo': 'title', 'normalizar_acentos': True}},\n",
    "                                'ingresos':     {'numeric' : {'as_int': False}}\n",
    "                            }\n",
    "    }\n",
    "reglas_por_archivo = {\n",
    "    'ventas.csv':    reglas[\"df_ventas\"],\n",
    "    'clientes.csv':  reglas[\"df_clientes\"],\n",
    "    'marketing.csv': reglas[\"df_marketing\"]\n",
    "}\n",
    "\n",
    "#zip_path = carpeta_reportes.parent / 'reports_dataset_tpi_v2.zip'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a06a6528-ba7e-4f2c-889a-5d428f5a1cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar el DataFrame\n",
    "def ver(df :pd.DataFrame):\n",
    "    print(f\"\"\"\n",
    "        Descripción preliminar:\n",
    "        {df.describe()}\n",
    "        Dimensiones:{ df.ndim}\n",
    "        Forma:{ df.shape}    \n",
    "        Número de elementos:{ df.size}\n",
    "        Nombres de columnas:{ df.columns}\n",
    "        Nombres de filas:{ df.index}\n",
    "        Tipos de datos:\\n{ df.dtypes}\n",
    "        Primeras 10 filas:\\n{ df.head(10)}\n",
    "        Últimas 3 filas:\\n{ df.tail(3)}\n",
    "    {\"*\"*50}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dee86f8-86c5-45f5-98d7-ea56cd905395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_en_dataframes(nombre_df, df_cada):\n",
    "    for nombre_columna, serie in df_cada.items():\n",
    "        nombre_columna= nombre_columna.lower().replace(\" \",\"_\")\n",
    "        if nombre_columna.startswith(\"id_\"):\n",
    "            continue\n",
    "        regla =  next(iter(reglas[nombre_df][nombre_columna]))\n",
    "        print (f\"\"\"\n",
    "        {nombre_columna=}\n",
    "        {serie=}\n",
    "        {regla=}\n",
    "        \"\"\")\n",
    "        s = serie.copy()\n",
    "        if regla == \"numeric\":\n",
    "            try:\n",
    "                s = s.astype(str).str.strip()\n",
    "                s = s.str.replace('$', '')\n",
    "            except:\n",
    "                pass\n",
    "            s = pd.to_numeric(s, errors=\"coerce\")\n",
    "            if reglas[nombre_df][nombre_columna][\"numeric\"][\"as_int\"] :\n",
    "                s = s.replace('.', '').replace(',', '')\n",
    "                if not s.isna().any():\n",
    "                    s = s.astype(int)\n",
    "        elif regla == \"string\":\n",
    "            s = s.astype(str).str.strip().replace(\"  \",\" \")\n",
    "            match  reglas[nombre_df][nombre_columna][regla][\"tipo\"] :\n",
    "                case \"upper\":\n",
    "                    s = s.str.upper()\n",
    "                case \"title\":\n",
    "                    s = s.str.title()\n",
    "                case \"lower\":\n",
    "                    s = s.str.lower()\n",
    "            #texto_n = unicodedata.normalize(\"NFD\", entrada)\n",
    "            s= s.apply(  lambda x: ''.join( c for c in unicodedata.normalize('NFKD', str(x)) if not unicodedata.combining(c)  )  )\n",
    "            #''.join(c for c in unicodedata.normalize('NFKD', str(x))  if not unicodedata.combining(c))  for x in df[\"columna\"]\n",
    "            '''\n",
    "            \n",
    "            Modo\tSignificado\tQué hace\tCuándo usar\n",
    "            NFD\tNormalization Form Decomposition\tDescompone los caracteres Unicode en su forma básica y diacrítica. Ej: \"á\" → \"a\" + \" ́\"\tCuando solo querés separar acentos.\n",
    "            NFKD\tCompatibility Decomposition\tHace lo mismo más normaliza formas equivalentes \"compatibles\" (por ejemplo, “①” → “1”, “ﬂ” → “fl”)\tIdeal para limpieza más completa de texto.\n",
    "            '''\n",
    "        elif regla == \"date\":\n",
    "            # Convertimos la serie a datetime\n",
    "            s = pd.to_datetime(  s,  dayfirst=reglas[nombre_df][nombre_columna][regla][\"dayfirst\"],   errors=\"coerce\"  ) \n",
    "            #s = s.dt.strftime('%Y/%m/%d')\n",
    "        elif regla == \"fillna\":\n",
    "            s = s.fillna(0)\n",
    "        dic_dfs[nombre_df][nombre_columna] = s\n",
    "        print (f\"\"\"\n",
    "        {\"*\"*50}\n",
    "        {regla=}\"\"\")\n",
    "         \n",
    "    return dic_dfs[nombre_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a93a9ef1-7c47-4115-a9ba-f3ae616f83c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'limpiar_dataframes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m (dic_dfs[nombre_df].head(\u001b[32m40\u001b[39m))\n\u001b[32m     12\u001b[39m     ver( dic_dfs[nombre_df])\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mlimpiar_dataframes\u001b[49m()\n",
      "\u001b[31mNameError\u001b[39m: name 'limpiar_dataframes' is not defined"
     ]
    }
   ],
   "source": [
    "def dataframes_en_diccionario():\n",
    "    \"\"\"\n",
    "    Limpia el DataFrame aplicando reglas_por_columna = {\"col\": (\"regla\", parametros)...}\n",
    "    Las reglas se asignan automáticamente según el tipo o formato:\n",
    "      - Columnas numéricas o con símbolos ($, %, dígitos) → 'numeric'\n",
    "      - Columnas que parecen fechas → 'date'\n",
    "      - Otras columnas → 'string'\n",
    "    \"\"\"\n",
    "    for nombre_df, df_cada in dic_dfs.items():\n",
    "        dic_dfs[nombre_df] = series_en_dataframes(nombre_df, df_cada)\n",
    "    print (dic_dfs[nombre_df].head(40))\n",
    "    ver( dic_dfs[nombre_df])\n",
    "limpiar_dataframes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3a7982-b970-401c-94bf-ee301fd62c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_dataframes() :\n",
    "    \"\"\"\n",
    "    Limpia el DataFrame aplicando reglas_por_columna = {\"col\": (\"regla\", parametros)...}\n",
    "    Las reglas se asignan automáticamente según el tipo o formato:\n",
    "      - Columnas numéricas o con símbolos ($, %, dígitos) → 'numeric'\n",
    "      - Columnas que parecen fechas → 'date'\n",
    "      - Otras columnas → 'string'\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    for nombre_df, df_cada in dic_dfs.items():\n",
    "        '''\n",
    "        print (f\"\"\"{nombre_df}\n",
    "        {dic_dfs[nombre_df]}\n",
    "        \"\"\")\n",
    "        '''\n",
    "        for nombre_columna, serie in df_cada.items():\n",
    "            nombre_columna= nombre_columna.lower().replace(\" \",\"_\")\n",
    "            if nombre_columna.startswith(\"id_\"):\n",
    "                continue\n",
    "            regla =  next(iter(reglas[nombre_df][nombre_columna]))\n",
    "            print (f\"\"\"\n",
    "            {nombre_columna=}\n",
    "            {serie=}\n",
    "            {regla=}\n",
    "            \"\"\")\n",
    "            s = serie.copy()\n",
    "            if regla == \"numeric\":\n",
    "                try:\n",
    "                    s = s.astype(str).str.strip()\n",
    "                    s = s.str.replace('$', '')\n",
    "                except:\n",
    "                    pass\n",
    "                s = pd.to_numeric(s, errors=\"coerce\")\n",
    "                if reglas[nombre_df][nombre_columna][\"numeric\"][\"as_int\"] :\n",
    "                    s = s.replace('.', '').replace(',', '')\n",
    "                    if not s.isna().any():\n",
    "                        s = s.astype(int)\n",
    "            elif regla == \"string\":\n",
    "                s = s.astype(str).str.strip().replace(\"  \",\" \")\n",
    "                match  reglas[nombre_df][nombre_columna][regla][\"tipo\"] :\n",
    "                    case \"upper\":\n",
    "                        s = s.str.upper()\n",
    "                    case \"title\":\n",
    "                        s = s.str.title()\n",
    "                    case \"lower\":\n",
    "                        s = s.str.lower()\n",
    "                #texto_n = unicodedata.normalize(\"NFD\", entrada)\n",
    "                s= s.apply(  lambda x: ''.join( c for c in unicodedata.normalize('NFKD', str(x)) if not unicodedata.combining(c)  )  )\n",
    "                #''.join(c for c in unicodedata.normalize('NFKD', str(x))  if not unicodedata.combining(c))  for x in df[\"columna\"]\n",
    "                '''\n",
    "                \n",
    "                Modo\tSignificado\tQué hace\tCuándo usar\n",
    "                NFD\tNormalization Form Decomposition\tDescompone los caracteres Unicode en su forma básica y diacrítica. Ej: \"á\" → \"a\" + \" ́\"\tCuando solo querés separar acentos.\n",
    "                NFKD\tCompatibility Decomposition\tHace lo mismo más normaliza formas equivalentes \"compatibles\" (por ejemplo, “①” → “1”, “ﬂ” → “fl”)\tIdeal para limpieza más completa de texto.\n",
    "                '''\n",
    "            elif regla == \"date\":\n",
    "                # Convertimos la serie a datetime\n",
    "                s = pd.to_datetime(  s,  dayfirst=reglas[nombre_df][nombre_columna][regla][\"dayfirst\"],   errors=\"coerce\"  ) \n",
    "                #s = s.dt.strftime('%Y/%m/%d')\n",
    "            elif regla == \"fillna\":\n",
    "                s = s.fillna(0)\n",
    "            dic_dfs[nombre_df][nombre_columna] = s\n",
    "            print (f\"\"\"\n",
    "            {\"*\"*50}\n",
    "            {regla=}\"\"\")\n",
    "        print (dic_dfs[nombre_df].head(40))\n",
    "        ver( dic_dfs[nombre_df])\n",
    "limpiar_dataframes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e42599a-abe2-4447-b8e6-6c1024f0d651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función auxiliar para guardar CSVs\n",
    "def guardar_csv(df, ruta):\n",
    "    \"\"\"\n",
    "    Guarda df en ruta (string o Path). Crea directorio padre si no existe.\n",
    "    \"\"\"\n",
    "    ruta = Path(ruta)\n",
    "    ruta.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(ruta, index=False, encoding='utf-8')\n",
    "    return ruta\n",
    "\n",
    "for [path_archivo, df_actual],[nombre_archivo,_] in zip( dic_dfs.items() , reglas_por_archivo.items() ):\n",
    "\n",
    "    nombre_limpio = nombre_archivo[:-4] + '_limpio.csv' if nombre_archivo.lower().endswith('.csv') else nombre_archivo + ' limpio.csv'\n",
    "    print(f'nombre_limpio = {nombre_limpio}')\n",
    "    # guardar cleaned\n",
    "    ruta_guardado = carpeta_limpios / nombre_limpio\n",
    "    guardar_csv(df_actual, ruta_guardado)\n",
    "    print(f'Guardado cleaned en: {ruta_guardado}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aefb8b-3e14-4917-9986-8b7f27033cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_valor_faltante(se):\n",
    "    \"\"\"\n",
    "    Determina si un valor debe considerarse faltante (True) usando tokens y NaN.\n",
    "    \"\"\"\n",
    "    if pd.isna(valor):\n",
    "        return True\n",
    "    salida = s in TOKENS_VALOR_FALTANTE\n",
    "    print (salida)\n",
    "    return salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4634aeae-c21a-47dc-9bb9-cffdeafcbe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Detección de outliers (IQR) - función corregida y robusta\n",
    "def mascara_valores_atipicos_rango_intercuartil(serie_datos):\n",
    "    \"\"\"\n",
    "    Devuelve una tupla: (mascara_bool_series, cantidad_outliers, (limite_inferior, limite_superior))\n",
    "    - serie_datos: pd.Series (acepta valores no numéricos, se intentará convertir)\n",
    "    - La máscara tiene la misma indexación que la serie original (NaNs -> False)\n",
    "    \"\"\"\n",
    "    # Intentar convertir a numérico (coerce -> NaN para no numéricos)\n",
    "    serie_numerica = pd.to_numeric(serie_datos, errors='coerce')\n",
    "    # Serie limpia para cálculos de cuartiles (sin NaN)\n",
    "    serie_limpia = serie_numerica.dropna().astype(float)\n",
    "    if serie_limpia.shape[0] < 4:\n",
    "        # No hay suficientes datos para IQR: devolver máscara False de la misma longitud\n",
    "        mascara = pd.Series([False] * len(serie_datos), index=serie_datos.index)\n",
    "        return mascara, int(mascara.sum()), (None, None)\n",
    "    cuartil_1 = float(serie_limpia.quantile(0.25))\n",
    "    cuartil_3 = float(serie_limpia.quantile(0.75))\n",
    "    rango_intercuartil = cuartil_3 - cuartil_1\n",
    "    limite_inferior = cuartil_1 - desviacion_margen * rango_intercuartil\n",
    "    limite_superior = cuartil_3 + desviacion_margen * rango_intercuartil\n",
    "    # Crear máscara sobre la serie numérica original (alineada con el index original)\n",
    "    mascara = (serie_numerica < limite_inferior) | (serie_numerica > limite_superior)\n",
    "    mascara = mascara.fillna(False).astype(bool)\n",
    "    return mascara, int(mascara.sum()), (limite_inferior, limite_superior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c15444c-3df6-409e-a932-c08afaebfd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detección de outliers (Z-score)\n",
    "def mascara_valores_atipicos_zscore(serie_datos, desviacion_umbral=3.0):\n",
    "    \"\"\"\n",
    "    Devuelve máscara booleana (True = outlier) según Z-score.\n",
    "    \"\"\"\n",
    "    serie_limpia = serie_datos.dropna().astype(float)\n",
    "    if serie_limpia.shape[0] < 4 or serie_limpia.std() == 0:\n",
    "        return pd.Series([False] * len(serie_datos), index=serie_datos.index)\n",
    "    puntaje_z = (serie_datos - serie_limpia.mean()) / serie_limpia.std()\n",
    "    return puntaje_z.abs() > desviacion_umbral  \n",
    "print('Funciones utilitarias definidas.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f411cb90-17af-45f2-9dfb-9fe7534bacbb",
   "metadata": {},
   "source": [
    "# limpieza y normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe131403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Detección de problemas en un DataFrame ----------\n",
    "def detectar_problemas_en_dataframe(df: pd.DataFrame):\n",
    " \n",
    "    resumen                       = {}\n",
    "    resumen['filas']              = df.shape[0]\n",
    "    resumen['columnas']           = df.shape[1]\n",
    "    resumen['nulos_por_columna']  = df.isna().sum().to_dict()\n",
    "    dup_mask                      = df.duplicated(keep=False)\n",
    "    resumen['duplicados_exactos'] = int(dup_mask.sum())\n",
    "    chequeos_por_columna          = {}\n",
    "    print(f\"\"\"\\033[1;37;44m\\n\n",
    "╔═════════════════════════════════════════════════════════════════════════════╗\n",
    "║                              Valores a eliminar                             ║\n",
    "╠═════════════════════════════════════════════════════════════════════════════╣\n",
    "║     Afecta                                                                  ║\n",
    "║         Elimina espacios iniciales y finales.                               ║\n",
    "║         Borra Na                                                            ║\n",
    "║         Borra duplicados                                                    ║\n",
    "╚═════════════════════════════════════════════════════════════════════════════╝\\033[0;m\"\"\") \n",
    "    print(f\"\"\"\\033[1;37;44m\\n\n",
    "╔═════════════════════════════════════════════════════════════════════════════╗\n",
    "║                               Valores atípicos                              ║\n",
    "╠═════════════════════════════════════════════════════════════════════════════╣\n",
    "║     Afecta                                                                  ║\n",
    "║         NO Modifica datos.                                                  ║\n",
    "║         Se guarda la información en archivo excel para referencias futuras  ║\n",
    "║         Se evalua es mediante dos formas                                    ║\n",
    "║            1) limites intercuartiles 25 y 75 % * desviacion_margen {desviacion_margen}      ║\n",
    "║            2) Z-score mayor a desviacion_umbral {desviacion_umbral}                         ║\n",
    "╚═════════════════════════════════════════════════════════════════════════════╝\\033[0;m\"\"\")\n",
    "    for col in df.columns:\n",
    "        serie = df[col]\n",
    "        info = {'dtype': str(serie.dtype), 'nulos': int(serie.isna().sum())}\n",
    "        if serie.dtype == object or pd.api.types.is_string_dtype(serie):\n",
    "            s = serie.astype(str)\n",
    "            info['espacios_inicio']          = int(s.str.match(r'^\\s+').sum())\n",
    "            info['espacios_final']           = int(s.str.match(r'\\s+$').sum())\n",
    "            try:\n",
    "                unique_original              = set(s.dropna().unique())\n",
    "                unique_lower                 = set(s.dropna().str.lower().unique())\n",
    "                info['unique_original']      = len(unique_original)\n",
    "                info['unique_lower']         = len(unique_lower)\n",
    "                info['variantes_mayusculas'] = len(unique_lower) < len(unique_original)\n",
    "            except Exception:\n",
    "                info['unique_original']      = serie.nunique(dropna=True)\n",
    "                info['unique_lower']         = None\n",
    "                info['variantes_mayusculas'] = None\n",
    "            try:\n",
    "                unaccented                   = s.dropna().map(lambda x: sacar_acentos(x).lower())\n",
    "                groups                       = unaccented.groupby(unaccented).size()\n",
    "                conflicts                    = groups[groups > 1]\n",
    "                info['grupos_var_acentos']   = int(conflicts.shape[0])\n",
    "                ejemplos = {}\n",
    "                if not conflicts.empty:\n",
    "                    for val in conflicts.index[:5]:\n",
    "                        originales           = sorted(list(s[unaccented == val].unique())[:10])\n",
    "                        ejemplos[val]        = originales\n",
    "                info['ejemplos_var_acentos'] = ejemplos\n",
    "            except Exception:\n",
    "                info['grupos_var_acentos']   = None\n",
    "                info['ejemplos_var_acentos'] = {}\n",
    "            info['tokens_aparente_faltante'] = int(\n",
    "                s.map(lambda x: str(x).strip().lower()).map(lambda v: sacar_acentos(v) in TOKENS_VALOR_FALTANTE).sum()\n",
    "            )\n",
    "            info['muestras'] = list(s.dropna().unique()[:10])\n",
    "        else:\n",
    "            # numeric\n",
    "            if pd.api.types.is_numeric_dtype(serie) or (serie.dropna().astype(str).str.replace('.','',1).str.isnumeric().all() if len(serie.dropna())>0 else False):\n",
    "                try:\n",
    "                    serie_numerica = serie.dropna().astype(float)\n",
    "                except Exception:\n",
    "                    serie_numerica = pd.to_numeric(serie, errors='coerce').dropna().astype(float)\n",
    "                info['media'] = float(serie_numerica.mean()) if not serie_numerica.empty else None\n",
    "                info['std']   = float(serie_numerica.std()) if not serie_numerica.empty else None\n",
    "                info['min']   = float(serie_numerica.min()) if not serie_numerica.empty else None\n",
    "                info['max']   = float(serie_numerica.max()) if not serie_numerica.empty else None\n",
    "                if len(serie_numerica) >= 4:\n",
    "                    mascara_outliers_iqr, cant,(info['outliers_iqr'] ,info['limites_iqr']) =mascara_valores_atipicos_rango_intercuartil(serie)\n",
    "                    #mascara, int(mascara.sum()), (limite_inferior, limite_superior)\n",
    "                    '''\n",
    "                    print (f\"\"\"\n",
    "                    {mascara_outliers_iqr=}\n",
    "                    {cant=}\n",
    "                    {info['outliers_iqr']=}\n",
    "                    {info['limites_iqr']=}\n",
    "                    {\"-\"*100}\n",
    "                    \"\"\")\n",
    "                    '''\n",
    "                    \n",
    "                else:\n",
    "                    info['outliers_iqr'] = None\n",
    "                    info['limites_iqr'] = None\n",
    "                if len(serie_numerica) >= 4 and serie_numerica.std() != 0:\n",
    "                    z = (serie_numerica - serie_numerica.mean()) / serie_numerica.std()\n",
    "                    info['outliers_z'] = int((z.abs() > 3).sum())\n",
    "                else:\n",
    "                    info['outliers_z'] = None\n",
    "            else:\n",
    "                # fechas intento parseo\n",
    "                parsed = pd.to_datetime(serie, errors='coerce', dayfirst=True)\n",
    "                info['fechas_parseables'] = int(parsed.notna().sum())\n",
    "                info['muestras'] = list(serie.dropna().unique()[:10])\n",
    "        chequeos_por_columna[col] = info\n",
    "\n",
    "    filas_problemas = []\n",
    "    df.columns = [c.strip().lower().replace(' ', '_') for c in df.columns]\n",
    "    for idx, fila in df.iterrows():\n",
    "        lista_problemas = []\n",
    "        if dup_mask.loc[idx]:\n",
    "            lista_problemas.append('duplicado_exacto')\n",
    "        for col in df.columns:\n",
    "            val = fila[col]\n",
    "            \n",
    "            # heurísticas textuales\n",
    "            if pd.api.types.is_string_dtype(type(val)) or isinstance(val, str) or (\n",
    "                not pd.isna(val) and not pd.api.types.is_numeric_dtype(type(val)) and str(chequeos_por_columna[col].get('dtype','')).startswith('object')\n",
    "            ):\n",
    "                s = str(val)\n",
    "                if s != s.strip():\n",
    "                    lista_problemas.append(f'espacios_en_columna_{col}')\n",
    "                if chequeos_por_columna[col].get('variantes_mayusculas'):\n",
    "                    if s and s != s.lower() and s.lower() in [str(x).lower() for x in df[col].dropna().unique()]:\n",
    "                        lista_problemas.append(f'inconsistencia_mayusculas_columna_{col}')\n",
    "                if chequeos_por_columna[col].get('grupos_var_acentos') and chequeos_por_columna[col]['grupos_var_acentos'] > 0:\n",
    "                    try:\n",
    "                        #un = sacar_acentos(s).lower()\n",
    "                        #group_vals = [x for x in chequeos_por_columna[col].get('muestras', []) if sacar_acentos(str(x)).lower() == un]\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        df['col'] = df['col'].apply(sacar_acentos)\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        if group_vals and any(sacar_acentos(str(x)).lower() != sacar_acentos(s).lower() for x in group_vals):\n",
    "                            lista_problemas.append(f'variantes_acentos_columna_{col}')\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                if es_valor_faltante(s):\n",
    "                    lista_problemas.append(f'token_faltante_columna_{col}')\n",
    "            else:\n",
    "                # heurísticas numéricas\n",
    "                try:\n",
    "                    fval = float(val)\n",
    "                    info_col = chequeos_por_columna[col]\n",
    "                    limites = info_col.get('limites_iqr')\n",
    "                    if limites and (fval < limites[0] or fval > limites[1]):\n",
    "                        lista_problemas.append(f'outlier_iqr_columna_{col}')\n",
    "                    if info_col.get('std') not in (None, 0):\n",
    "                        mean = info_col.get('media')\n",
    "                        std = info_col.get('std')\n",
    "                        if std and abs((fval - mean) / std) > 3:\n",
    "                            lista_problemas.append(f'outlier_z_columna_{col}')\n",
    "                except Exception:\n",
    "                    pass\n",
    "        if lista_problemas:\n",
    "            filas_problemas.append({\n",
    "                'row_index': idx,\n",
    "                'problemas': ';'.join(sorted(set(lista_problemas))),\n",
    "                'muestra': json.dumps({str(c): str(fila[c]) for c in df.columns[:8]})\n",
    "            })\n",
    "\n",
    "    df_problemas = pd.DataFrame(filas_problemas)\n",
    "    return resumen, chequeos_por_columna, df_problemas\n",
    "\n",
    "print('Función detectar_problemas_en_dataframe cargada.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc0f0c3-0e31-4f27-9661-3215c9185f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_frames = {\n",
    "    ruta_ventas     : df_ventas,\n",
    "    ruta_clientes   : df_clientes,\n",
    "    ruta_marketing  : df_marketing\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32235fb9-1e09-422e-931b-3471d6fbf123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Bucle/menu principal (usa dic_frames) ----------\n",
    "def menu_procesar_diccionario(dic_frames, reglas_por_archivo):\n",
    "    \"\"\"\n",
    "    Recorre dic_frames: clave = nombre_archivo (ej. 'marketing.csv'), valor = DataFrame.\n",
    "    Ejecuta: detectar_problemas_en_dataframe antes, limpiar_dataframe, detectar_problemas_en_dataframe despues,\n",
    "    imprime resúmenes y guarda cleaned en carpeta_limpios con sufijo ' limpio.csv'.\n",
    "    También sobreescribe variables en RAM (df_marketing, df_ventas, df_clientes) si se encuentran en el nombre.\n",
    "    \"\"\"\n",
    "    print(\"\"\"\\033[1;37;44m\\n\n",
    "╔═════════════════════════════════════════════════════════════════════════════╗\n",
    "║                     Aplico reglas según columna específica                  ║\n",
    "╠═════════════════════════════════════════════════════════════════════════════╣\n",
    "║     Afecta                                                                  ║\n",
    "║             Numéricos (int/float)                                           ║\n",
    "║             fechas --> YYYY,MM,DD                                           ║\n",
    "╚═════════════════════════════════════════════════════════════════════════════╝\\033[0;m\"\"\")\n",
    "    errores_df=pd.DataFrame()\n",
    "    # trabajamos sobre una copia para evitar modificar dict original por error\n",
    "    for [path_archivo, df_actual],[nombre_archivo,_] in zip( dic_frames.items() , reglas_por_archivo.items() ):\n",
    "        resumen_antes, chequeos_antes, problemas_antes = detectar_problemas_en_dataframe(df_actual)\n",
    "        print(f\"--- RESUMEN ANTES: {nombre_archivo} ---\")\n",
    "        # Para no volcar objetos muy grandes, mostramos el head del DataFrame de problemas (si existe)\n",
    "        #print('Muestras de problemas antes (primeras 5 filas):')\n",
    "        #print( display(problemas_antes.head(5) if not problemas_antes.empty else 'No se detectaron filas con problemas.') )\n",
    "\n",
    "        errores_df  = pd.concat ([errores_df,problemas_antes])\n",
    "\n",
    "        #df_actual[df_actual.duplicated(keep=False)].copy()\n",
    "        reglas    = reglas_por_archivo.get(nombre_archivo, {})\n",
    "        df_limpio = limpiar_dataframe(df_actual, reglas_por_columna=reglas)\n",
    "\n",
    "        resumen_despues, chequeos_despues, problemas_despues = detectar_problemas_en_dataframe(df_limpio)\n",
    "        '''\n",
    "        print(f\"n--- RESUMEN DESPUÉS: {nombre_archivo} ---\")\n",
    "        print(resumen_despues)\n",
    "        print('Muestras de problemas después (primeras 5 filas):')\n",
    "        print( display(problemas_despues.head(5) if not problemas_despues.empty else 'No se detectaron filas con problemas tras la limpieza.'))\n",
    "        '''\n",
    "        # mostrar separadores y tipo-nombre\n",
    "\n",
    "        #print('-'*100)\n",
    "        #print(f'nombre_archivo = {nombre_archivo}')\n",
    "        #print(f'type(nombre_archivo) = {type(nombre_archivo)}')\n",
    "        #print('-'*100)\n",
    "\n",
    "        # Guardo el archivo en limpios  ruta_base carpeta_reportes\n",
    "        nombre_limpio = nombre_archivo[:-4] + '_limpio.csv' if nombre_archivo.lower().endswith('.csv') else nombre_archivo + ' limpio.csv'\n",
    "        print(f'nombre_limpio = {nombre_limpio}')\n",
    "    \n",
    "        # guardar cleaned\n",
    "        ruta_guardado = carpeta_limpios / nombre_limpio\n",
    "        guardar_csv(df_limpio, ruta_guardado)\n",
    "        print(f'Guardado cleaned en: {ruta_guardado}')\n",
    "\n",
    "        # sobreescribir en RAM según el nombre\n",
    "        # (nota: usar globals() para actualizar variables en el entorno global del notebook)\n",
    "        dic_dfs[nombre_archivo.lower()] = df_limpio\n",
    "        '''\n",
    "        if 'marketing' in nombre_archivo.lower():\n",
    "            globals()['df_marketing'] = df_limpio\n",
    "        elif 'ventas' in nombre_archivo.lower():\n",
    "            globals()['df_ventas'] = df_limpio\n",
    "        elif 'clientes' in nombre_archivo.lower():\n",
    "            globals()['df_clientes'] = df_limpio\n",
    "        '''\n",
    "    print('Proceso completo del diccionario de DataFrames.')\n",
    "    return errores_df\n",
    "errores_df = menu_procesar_diccionario(dic_frames, reglas_por_archivo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f4ffe5-0f8d-465e-be9e-6d180fa42789",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#CCCCCC; padding:10px; border-radius:6px;\">\n",
    "<h2 style=\"color:black; text-align:center;\">Resultados de limpieza</h2>\n",
    "<p style=\"color:black;\">- Revisados los 3 csv  pasados a DataFrames.</p>\n",
    "<p style=\"color:blue;\">- DataFrames filtrados.</p>\n",
    "<p style=\"color:black;\">- Filtrado de Nulos.</p>\n",
    "<p style=\"color:black;\">- Filtrado de duplicados.</p>\n",
    "<p style=\"color:black;\">- Sin '', 'na', 'n/a', 'null', 'none', 'sin dato', 's/d', 'nd', '-', '--', '?', 'sin_dato', 'n/d'</p>    \n",
    "<p style=\"color:black;\">- Normalisados Strings segun reglas. Estilo (lower,string.upper) unicodedata.normalize('NFKD')</p>\n",
    "<p style=\"color:black;\">- Normalisados precios a float sin signo ($)</p>\n",
    "<p style=\"color:black;\">- Normalisados Numericos a int o float segun regla</p>    \n",
    "<p style=\"color:black;\">- Normalisados Fechas segun regla YYYY/MM/DD</p>    \n",
    "<p style=\"color:black;\">- Resguardo <code>datasets_salida/limpios/clientes_limpio.csv</code>.</p>\n",
    "<p style=\"color:black;\">- Resguardo <code>datasets_salida/limpios/marketing_limpio.csv</code>.</p>\n",
    "<p style=\"color:black;\">- Resguardo <code>datasets_salida/limpios/ventas_limpio.csv</code>.</p>\n",
    "<p style=\"color:blue;\">- Registros filtrados eliminados</p>\n",
    "<p style=\"color:black;\">- Resguardo <code>datasets_salida/reportes/reporte_limpieza.xlsx</code> con hojas (duplicados borrados, outliers, totales)</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1ab45f-4494-4b83-8470-20c6aab64b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Crear df_duplicados_total\n",
    "print (f\"\"\"\n",
    "{cantidad_duplicados=}\n",
    "\"\"\")\n",
    "# Filtra las filas donde la columna 'problemas' contiene la subcadena 'duplicado_exacto'\n",
    "df_duplicados_total = errores_df[errores_df['problemas'].str.contains('duplicado_exacto', case=False, na=False)].copy()\n",
    "\n",
    "# 2. Crear df_outliers_total\n",
    "# Filtra las filas donde la columna 'problemas' contiene la subcadena 'outlier_'\n",
    "df_outliers_total = errores_df[errores_df['problemas'].str.contains('outlier_', case=False, na=False)].copy()\n",
    "\n",
    "# 3. Crear df_resumen (Combinación y Ordenamiento)\n",
    "# Concatena los dos DataFrames creados\n",
    "df_resumen = pd.concat([df_duplicados_total, df_outliers_total])\n",
    "\n",
    "# Ordena el DataFrame resultante por la columna 'problemas'\n",
    "df_resumen = df_resumen.sort_values(by=\"problemas\").reset_index(drop=True)\n",
    "'''\n",
    "print (f\"\"\"\n",
    "df_duplicados_total\n",
    "{df_duplicados_total}\n",
    "\n",
    "{\"-\"*100}\n",
    "\n",
    "df_outliers_total\n",
    "{df_outliers_total}\n",
    "\n",
    "{\"-\"*100}\n",
    "df_resumen\n",
    "{df_resumen}\n",
    "{\"-\"*100}\n",
    "\"\"\")\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3a213a-5293-46f6-ad63-4cabc9e5b920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar en Excel con manejo de fallo si no existe el engine\n",
    "try:\n",
    "    with pd.ExcelWriter(ruta_excel, engine='openpyxl') as writer:\n",
    "        try:\n",
    "            df_duplicados_total.to_excel(writer, index=False, sheet_name='duplicados')\n",
    "        except Exception:\n",
    "            pd.DataFrame().to_excel(writer, index=False, sheet_name='duplicados')\n",
    "        try:\n",
    "            df_outliers_total.to_excel(writer, index=False, sheet_name='outliers')\n",
    "        except Exception:\n",
    "            pd.DataFrame().to_excel(writer, index=False, sheet_name='outliers')\n",
    "        try:\n",
    "            df_resumen.to_excel(writer, index=False, sheet_name='resumen_filtros')\n",
    "        except Exception:\n",
    "            pd.DataFrame(lista_resumen).to_excel(writer, index=False, sheet_name='resumen_filtros')\n",
    "    guardado_ok = True\n",
    "except Exception as e_openpyxl:\n",
    "    mensajes.append('Error usando openpyxl: ' + str(e_openpyxl))\n",
    "    try:\n",
    "        with pd.ExcelWriter(ruta_excel) as writer:\n",
    "            df_duplicados_total.to_excel(writer, index=False, sheet_name='duplicados')\n",
    "            df_outliers_total.to_excel(writer, index=False, sheet_name='outliers')\n",
    "            df_resumen.to_excel(writer, index=False, sheet_name='resumen_filtros')\n",
    "        guardado_ok = True\n",
    "    except Exception as e_default:\n",
    "        mensajes.append('Error sin engine: ' + str(e_default))\n",
    "        try:\n",
    "            df_duplicados_total.to_csv(carpeta_reportes / 'duplicados.csv', index=False, encoding='utf-8')\n",
    "            df_outliers_total.to_csv(carpeta_reportes / 'outliers.csv', index=False, encoding='utf-8')\n",
    "            df_resumen.to_csv(carpeta_reportes / 'resumen_filtros.csv', index=False, encoding='utf-8')\n",
    "            mensajes.append('Se guardaron CSVs separados como fallback.')\n",
    "            guardado_ok = True\n",
    "        except Exception as e_csv:\n",
    "            mensajes.append('Error guardando CSV fallback: ' + str(e_csv))\n",
    "            guardado_ok = False\n",
    "\n",
    "print('Guardado OK:', guardado_ok)\n",
    "if mensajes:\n",
    "    print('Mensajes/Errores durante guardado:')\n",
    "    for m in mensajes:\n",
    "        print('-', m)\n",
    "print('Ruta final esperada del Excel (si guardado):', ruta_excel)\n",
    "print('Resumen por dataset:')\n",
    "print(df_resumen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f43150b-eb7b-4ae8-a0a6-233f4b6ca5f1",
   "metadata": {},
   "source": [
    "ventas.csv  análisis de ventas, limpieza de datos y estadísticas descriptivas.\n",
    " \t\n",
    "clientes.csv  unirse a las ventas mediante el uso de funciones de combinación para analizar características de los clientes relacionados con sus \tcompras.\n",
    " \t\n",
    "marketing.csv analizar la efectividad de las campañas de marketing en las ventas y buscar correlaciones.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f5fcf9c-6140-4160-9ca0-ebf688f8c921",
   "metadata": {},
   "source": [
    "2) Claves candidatas para merge o concat\n",
    "\n",
    "Intersecciones observadas:\n",
    "\n",
    "ventas ∩ marketing → producto (clave natural para unir campañas con ventas por producto)\n",
    "\n",
    "ventas ∩ clientes → ninguna columna en común\n",
    "\n",
    "clientes ∩ marketing → ninguna columna en común\n",
    "\n",
    "Interpretación pedagógica:\n",
    "\n",
    "Puedes unir ventas con marketing por producto (p. ej. para ver qué canal promocionó qué producto y coste).\n",
    "\n",
    "No puedes unir directamente ventas con clientes porque ventas_limpio no contiene id_cliente ni email ni nombre_cliente. Para unir ventas↔clientes necesitas:\n",
    "\n",
    "que ventas_limpio tenga una columna id_cliente (recomendado), o\n",
    "\n",
    "un fichero/mapeo que vincule id_venta → id_cliente, o\n",
    "\n",
    "usar correspondencia por email/nombre (menos fiable) si esos campos existieran.\n",
    "\n",
    "3) Ejemplos de combinaciones útiles (casos prácticos)\n",
    "\n",
    "Ventas por canal de marketing (recomendado)\n",
    "\n",
    "Merge ventas_limpio ⟵ marketing_limpio por producto (left join): asignás a cada venta el canal y id_campanha. Luego agrupás por canal para métricas.\n",
    "\n",
    "Análisis de ticket promedio por categoría y canal\n",
    "\n",
    "Después del merge anterior, crear monto = precio * cantidad y agrupar por categoria y canal.\n",
    "\n",
    "Concatenación (vertical)\n",
    "\n",
    "Si tuvieras varios archivos de ventas de distintos periodos: pd.concat([ventas_periodo1, ventas_periodo2], axis=0).\n",
    "\n",
    "Unir clientes (si se dispone de id_cliente en ventas)\n",
    "\n",
    "ventas_limpio.merge(clientes_limpio, on='id_cliente', how='left') → permite segmentar ventas por edad, ciudad, ingresos.\n",
    "\n",
    "Si no hay id_cliente en ventas\n",
    "\n",
    "Crear un mapping table (archivo) que contenga id_venta → id_cliente y hacer merge por esa tabla.\n",
    "\n",
    "4) Código limpio en pandas (listo para ejecutar — adaptá nombres de columnas si quieres otro comportamiento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93885b49-e04a-401c-9c03-78439696cdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3) Tipos y limpiezas básicas\n",
    "ventas['fecha_venta'] = pd.to_datetime(ventas['fecha_venta'], errors='coerce')\n",
    "marketing['fecha_inicio'] = pd.to_datetime(marketing['fecha_inicio'], errors='coerce')\n",
    "marketing['fecha_fin'] = pd.to_datetime(marketing['fecha_fin'], errors='coerce')\n",
    "\n",
    "# Asegurar numéricos\n",
    "ventas['precio'] = pd.to_numeric(ventas['precio'], errors='coerce')\n",
    "ventas['cantidad'] = pd.to_numeric(ventas['cantidad'], errors='coerce')\n",
    "clientes['ingresos'] = pd.to_numeric(clientes['ingresos'], errors='coerce')\n",
    "marketing['costo'] = pd.to_numeric(marketing['costo'], errors='coerce')\n",
    "\n",
    "# 4) Crear columnas útiles\n",
    "ventas['monto'] = ventas['precio'] * ventas['cantidad']\n",
    "\n",
    "# 5) Merge ejemplo: ventas + marketing por 'producto' (asignar canal a cada venta)\n",
    "ventas_marketing = pd.merge(\n",
    "    ventas,\n",
    "    marketing[['producto', 'id_campanha', 'canal', 'costo', 'fecha_inicio', 'fecha_fin']],\n",
    "    on='producto',\n",
    "    how='left',   # left para conservar todas las ventas aunque no tengan campana asociada\n",
    "    validate='m:1'  # opcional: espera muchos registros ventas para 1 campaña por producto\n",
    ")\n",
    "\n",
    "# 6) Agregados: ventas por canal\n",
    "ventas_por_canal = (\n",
    "    ventas_marketing\n",
    "    .groupby('canal', dropna=False)\n",
    "    .agg(\n",
    "        total_monto=('monto', 'sum'),\n",
    "        cantidad_transacciones=('monto', 'count'),\n",
    "        ticket_promedio=('monto', 'mean')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# 7) Agregado: ventas por categoria y canal\n",
    "ventas_categoria_canal = (\n",
    "    ventas_marketing\n",
    "    .groupby(['categoria', 'canal'], dropna=False)\n",
    "    .agg(\n",
    "        total_monto     = ('monto', 'sum'),\n",
    "        transacciones   = ('monto', 'count'),\n",
    "        ticket_promedio = ('monto', 'mean')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# 8) Guardar resultados (opcional)\n",
    "ventas_por_canal.to_csv('/mnt/data/ventas_por_canal.csv', index=False)\n",
    "ventas_categoria_canal.to_csv('/mnt/data/ventas_categoria_canal.csv', index=False)\n",
    "\n",
    "# 9) ¿Y clientes? Si tienes id_cliente en ventas:\n",
    "# ventas_con_clientes = ventas.merge(clientes, on='id_cliente', how='left')\n",
    "\n",
    "# 10) Checks útiles\n",
    "# - Ver duplicados en claves: ventas['id_venta'].duplicated().sum()\n",
    "# - Ver clientes sin ventas: clientes[~clientes['id_cliente'].isin(ventas.get('id_cliente', []))]\n",
    "\n",
    "Recomendaciones prácticas (breves, accionables)\n",
    "\n",
    "Si querés unir ventas con clientes agregá id_cliente a ventas_limpio (registro en punto de venta o mapeo).\n",
    "\n",
    "Revisá duplicados en producto dentro de marketing (puede haber varias campañas por producto: decidir estrategia — por ejemplo filtrar la campaña activa por fecha).\n",
    "\n",
    "Elegí tipo de join con criterio pedagógico:\n",
    "\n",
    "left join para preservar todas las ventas (evitar perder datos).\n",
    "\n",
    "inner join si sólo te interesa el subset con campaña asociada.\n",
    "\n",
    "Creá fecha de periodo (día/semana/mes) para series temporales: ventas['mes'] = ventas['fecha_venta'].dt.to_period('M').\n",
    "\n",
    "Documentá supuestos: por qué usás how='left', cómo tratás ventas sin campaña, cómo imputás nulos en precio/cantidad.\n",
    "\n",
    "Ejemplo aplicado (interpretación cotidiana)\n",
    "\n",
    "Imaginá a Juan, vendedor en una pyme familiar con 2 hijos. Quiere saber si la campaña en Instagram está trayendo ventas: con el merge ventas + marketing por producto obtiene canal asignado a cada venta. Luego agrupa por canal y ve: “Instagram” tiene muchas visitas pero ticket promedio bajo — decisión: ajustar oferta o dirigir una campaña de cross-sell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40448d5e-62bf-4d58-9c5c-28da021964b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a mano\n",
    "ver que es lo que falta en drop na\n",
    "precios == 0 buscar en categoria el producto o promedo si no hay otro dato\n",
    "duplicate si el id es =\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21ab2c8-5a03-4fe6-9f02-60d2dc73d966",
   "metadata": {},
   "outputs": [],
   "source": [
    "productos mas vendidos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0737e69-7b61-4fa9-b678-3a178e09b62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ventas por mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d1d542-093a-49ad-b788-5e1b23af562d",
   "metadata": {},
   "outputs": [],
   "source": [
    "git remote set-url origin https://github.com/CursosAGT/nombre-nuevo.git\n",
    "git push -u origin main\n",
    "https://github.com/CursosAGT/-GarciaTrabaArielH-Comisi-n25262-TPI_Data_Analytics/blob/main/Garcia%20Traba%20Ariel%20H%20-%20Comisi%C3%B3n%2025262%20-%20TPI%20Data%20Analytics.ipynb\n",
    "Garcia Traba Ariel H - Comisión 25262 - TPI Data Analytics.ipynb\n",
    "https://github.com/CursosAGT/-GarciaTrabaArielH-Comisi-n25262-TPI_Data_Analytics/blob/main/datasets_entrada/clientes.csv\n",
    "https://github.com/CursosAGT/-GarciaTrabaArielH-Comisi-n25262-TPI_Data_Analytics/blob/main/datasets_entrada/marketing.csv\n",
    "https://github.com/CursosAGT/-GarciaTrabaArielH-Comisi-n25262-TPI_Data_Analytics/blob/main/datasets_entrada/ventas.csv.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
