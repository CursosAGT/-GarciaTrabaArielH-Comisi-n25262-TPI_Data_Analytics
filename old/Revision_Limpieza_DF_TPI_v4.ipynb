{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cedbb1b4",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color:#CCCCCC; padding:12px; border-radius:8px;\">\n",
    "<h1 style=\"color:#003366; text-align:center; margin:8px 0;\">Revisión y limpieza de 3 DataFrames (TPI - Data Analytics)</h1>\n",
    "<p style=\"text-align:center; color:#003366; margin:0;\"><em>Notebook docente en castellano — nombres descriptivos en snake_case — código y documentación</em></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c723b86a",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color:#CCCCCC; padding:10px; border-radius:6px;\">\n",
    "<h2 style=\"color:black; text-align:center; margin-top:6px;\">Resumen</h2>\n",
    "\n",
    "<p style=\"color:black;\">\n",
    "Este notebook está diseñado con finalidades pedagógicas. Revisa, normaliza y valida tres datasets contenidos en CSV:\n",
    "</p>\n",
    "\n",
    "<ul style=\"color:black;\">\n",
    "<li><code>marketing.csv</code> → variable: <code>df_marketing</code></li>\n",
    "<li><code>ventas.csv</code> → variable: <code>df_ventas</code></li>\n",
    "<li><code>clientes.csv</code> → variable: <code>df_clientes</code></li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"color:black;\">\n",
    "Coloca los CSV en <code>./data_in/</code> o en <code>/mnt/data/</code>. El notebook busca primero en <code>./data_in/</code> y si no encuentra, usa <code>/mnt/data/</code> (útil para entornos donde los archivos están pre-subidos).\n",
    "</p>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "be46fb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports y configuración inicial (nombres en castellano)\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import unicodedata\n",
    "import zipfile\n",
    "\n",
    "#!pip install gdown\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import zipfile\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from math import isnan\n",
    "ruta_base = \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63edff5b-828e-4254-a4d7-4b0ef05ecea6",
   "metadata": {},
   "source": [
    "## 1. Crear un documento en Google Colaboratory y cargar los sets de datos como DataFrames"
   ]
  },
  {
   "cell_type": "raw",
   "id": "97e86735-e26d-4583-a22b-a2d79a57666c",
   "metadata": {},
   "source": [
    "# --- Paso 1: Montar Google Drive ---\n",
    "# Montar tu Google Drive\n",
    "!pip install google\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!ls \"/content/drive/MyDrive/CABA/Garcia Traba Ariel H - Comisión 25262 - TPI Data Analytics/\"\n",
    "# Ruta del archivo (ajústala a la carpeta real en tu Drive)\n",
    "ruta_base = \"/content/drive/MyDrive/CABA/Garcia Traba Ariel H - Comisión 25262 - TPI Data Analytics/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "605182b8-9652-410e-8c79-13b10a33e692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rutas: busca en ./data_in primero, si no existe usa /mnt/data\n",
    "\n",
    "carpeta_entrada    = Path(ruta_base)\n",
    "#carpeta_entrada_mnt   = Path('/mnt/data')\n",
    "\n",
    "carpeta_reportes   = carpeta_entrada / 'reportes'\n",
    "carpeta_reportes.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "carpeta_limpios    = carpeta_entrada / 'limpios'\n",
    "carpeta_limpios.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Nombres esperados de archivos\n",
    "archivo_ventas     = 'ventas.csv'\n",
    "archivo_clientes   = 'clientes.csv'\n",
    "archivo_marketing  = 'marketing.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "052e6d76-d227-4749-af2c-f065eabf82f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datasets del curso...\n"
     ]
    }
   ],
   "source": [
    "# --- Paso 3: Cargar archivos del curso ---\n",
    "print(\"Cargando datasets del curso...\")\n",
    "ruta_ventas     = os.path.join(ruta_base, archivo_ventas)\n",
    "ruta_clientes   = os.path.join(ruta_base, archivo_clientes)\n",
    "ruta_marketing  = os.path.join(ruta_base, archivo_marketing)\n",
    "\n",
    "df_ventas    = pd.read_csv(f\"{ruta_ventas}\")\n",
    "df_clientes  = pd.read_csv(f\"{ruta_clientes}\")\n",
    "df_marketing = pd.read_csv(f\"{ruta_marketing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a06a6528-ba7e-4f2c-889a-5d428f5a1cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "df_ventas\n",
      "      id_venta                producto   precio  cantidad fecha_venta  \\\n",
      "0          792       Cuadro decorativo   $69.94       5.0  02/01/2024   \n",
      "1          811         Lámpara de mesa  $105.10       5.0  02/01/2024   \n",
      "2         1156                Secadora   $97.96       3.0  02/01/2024   \n",
      "3         1372                Heladera  $114.35       8.0  02/01/2024   \n",
      "4         1546                Secadora  $106.21       4.0  02/01/2024   \n",
      "...        ...                     ...      ...       ...         ...   \n",
      "3030      1837         Horno eléctrico  $104.12       9.0  30/12/2024   \n",
      "3031      2276                  Laptop   $85.27       9.0  30/12/2024   \n",
      "3032      2696                  Laptop  $107.81       4.0  30/12/2024   \n",
      "3033      2913              Smartphone   $99.85       7.0  30/12/2024   \n",
      "3034      2930  Consola de videojuegos   $55.47       6.0  30/12/2024   \n",
      "\n",
      "              categoria  \n",
      "0            Decoración  \n",
      "1            Decoración  \n",
      "2     Electrodomésticos  \n",
      "3     Electrodomésticos  \n",
      "4     Electrodomésticos  \n",
      "...                 ...  \n",
      "3030  Electrodomésticos  \n",
      "3031        Electrónica  \n",
      "3032        Electrónica  \n",
      "3033        Electrónica  \n",
      "3034        Electrónica  \n",
      "\n",
      "[3035 rows x 6 columns]\n",
      "**************************************************\n",
      "df_clientes\n",
      "     id_cliente               nombre  edad         ciudad  ingresos\n",
      "0             1      Aloysia Screase    44  Mar del Plata  42294.68\n",
      "1             2  Kristina Scaplehorn    25        Posadas  24735.04\n",
      "2             3       Filip Castagne    50    Resistencia  35744.85\n",
      "3             4          Liuka Luard    39   Bahía Blanca  27647.96\n",
      "4             5        Dore Cockshtt    28        Rosario  28245.65\n",
      "..          ...                  ...   ...            ...       ...\n",
      "562         563        Dione Forsyde    29        Posadas  26757.73\n",
      "563         564          Fleming Gow    39       Santa Fe  43674.96\n",
      "564         565      Jewelle Mabbett    33        Córdoba  30522.64\n",
      "565         566          Lauri Munns    23    Resistencia  31259.14\n",
      "566         567          Micah Matis    31     Corrientes  42927.86\n",
      "\n",
      "[567 rows x 5 columns]\n",
      "**************************************************\n",
      "df_marketing\n",
      "    id_campanha            producto  canal  costo fecha_inicio   fecha_fin\n",
      "0            74     Adorno de pared     TV   4.81   20/03/2024  03/05/2024\n",
      "1            12              Tablet   RRSS   3.40   26/03/2024  13/05/2024\n",
      "2            32     Lámpara de mesa  Email   5.54   28/03/2024  20/04/2024\n",
      "3            21          Smartphone   RRSS   6.37   29/03/2024  16/05/2024\n",
      "4            58            Alfombra  Email   4.25   31/03/2024  05/05/2024\n",
      "..          ...                 ...    ...    ...          ...         ...\n",
      "85           70          Aspiradora     TV   3.06   13/12/2024  29/12/2024\n",
      "86           89           Televisor     TV   4.98   13/12/2024    8/2/2025\n",
      "87           68   Rincón de plantas     TV   5.81   17/12/2024   14/2/2025\n",
      "88           33            Secadora  Email   3.80   20/12/2024    7/1/2025\n",
      "89           11  Freidora eléctrica   RRSS   5.27   29/12/2024   21/1/2025\n",
      "\n",
      "[90 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Mostrar el DataFrame\n",
    "print(f\"\"\"\n",
    "df_ventas\n",
    "{df_ventas}\n",
    "{\"*\"*50}\n",
    "df_clientes\n",
    "{df_clientes}\n",
    "{\"*\"*50}\n",
    "df_marketing\n",
    "{df_marketing}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "21030142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Funciones utilitarias en castellano (snake_case) ----------\n",
    "def sacar_acentos(texto):\n",
    "    \"\"\"\n",
    "    Elimina acentos (tildes/diacríticos) de un texto.\n",
    "    Mantiene NaN intactos.\n",
    "    \"\"\"\n",
    "    if pd.isna(texto):\n",
    "        return texto\n",
    "    texto = str(texto)\n",
    "    nk = unicodedata.normalize('NFKD', texto)\n",
    "    return ''.join([c for c in nk if not unicodedata.combining(c)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f4aefb8b-3e14-4917-9986-8b7f27033cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENS_VALOR_FALTANTE = {\n",
    "    '', 'na', 'n/a', 'null', 'none', 'sin dato', 's/d', 'nd', '-', '--', '?', 'sin_dato', 'n/d'\n",
    "}\n",
    "\n",
    "def es_valor_faltante(valor):\n",
    "    \"\"\"\n",
    "    Determina si un valor debe considerarse faltante (True) usando tokens y NaN.\n",
    "    \"\"\"\n",
    "    if pd.isna(valor):\n",
    "        return True\n",
    "    s = str(valor).strip().lower()\n",
    "    s = sacar_acentos(s)\n",
    "    return s in TOKENS_VALOR_FALTANTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ddbaa5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Funciones de limpieza ----------\n",
    "def aplicar_regla_columna(serie, regla):\n",
    "    \"\"\"\n",
    "    Aplica una regla a una serie (columna).\n",
    "    Firma compatible con la versión anterior (reemplaza la implementación previa).\n",
    "    regla puede ser:\n",
    "      - 'strip','lower','upper','title','quitar_acentos','numeric','date'\n",
    "    o un tuple (tipo, opciones) con opciones:\n",
    "      - numeric: remove_non_digits (bool), remove_thousands (bool), as_int (bool), thousands_separator (',' o '.')\n",
    "      - date: formats (list), dayfirst (bool), format_output ('YYYY/MM/DD' para forzar cadena)\n",
    "      - texto: normalizar_acentos (bool)\n",
    "    Retorna la serie transformada (sin forzar dtype final).\n",
    "    \"\"\"\n",
    "    tipo, opts = regla if isinstance(regla, tuple) else (regla, {})\n",
    "    opts = opts or {}\n",
    "    s = serie.copy()\n",
    "\n",
    "    # -------- Texto y normalización de acentos opcional --------\n",
    "    if tipo == 'strip':\n",
    "        s = s.map(lambda x: str(x).strip() if not pd.isna(x) else x)\n",
    "    elif tipo == 'lower':\n",
    "        s = s.map(lambda x: str(x).strip().lower() if not pd.isna(x) else x)\n",
    "        if opts.get('normalizar_acentos'):\n",
    "            s = s.map(lambda x: sacar_acentos(x) if not pd.isna(x) else x)\n",
    "    elif tipo == 'upper':\n",
    "        s = s.map(lambda x: str(x).strip().upper() if not pd.isna(x) else x)\n",
    "        if opts.get('normalizar_acentos'):\n",
    "            s = s.map(lambda x: sacar_acentos(x) if not pd.isna(x) else x)\n",
    "    elif tipo == 'title':\n",
    "        s = s.map(lambda x: str(x).strip().title() if not pd.isna(x) else x)\n",
    "        if opts.get('normalizar_acentos'):\n",
    "            s = s.map(lambda x: sacar_acentos(x) if not pd.isna(x) else x)\n",
    "    elif tipo == 'quitar_acentos':\n",
    "        s = s.map(lambda x: sacar_acentos(x).strip() if not pd.isna(x) else x)\n",
    "\n",
    "    # -------- Numeric robusto --------\n",
    "    elif tipo == 'numeric':\n",
    "        def to_num(v):\n",
    "            t = t.replace('$', '')\n",
    "            if pd.isna(v):\n",
    "                return np.nan\n",
    "            t = str(v).strip()\n",
    "            if opts.get('remove_non_digits', False):\n",
    "                t = ''.join([c for c in t if c.isdigit() or c in '.-'])\n",
    "            if opts.get('remove_thousands', False):\n",
    "                sep = opts.get('thousands_separator', ',')\n",
    "                if sep == ',':\n",
    "                    # Quitar puntos mil y comas de decimales no soportadas -> suponer coma miles y punto decimales\n",
    "                    t = t.replace('.', '').replace(',', '')\n",
    "                else:\n",
    "                    t = t.replace(',', '').replace('.', '')   \n",
    "            try:\n",
    "                val = pd.to_numeric(t, errors='coerce')\n",
    "                if opts.get('as_int', False):\n",
    "                    if pd.isna(val):\n",
    "                        return pd.NA\n",
    "                    try:\n",
    "                        # intentar entero simple\n",
    "                        return int(val)\n",
    "                    except Exception:\n",
    "                        return pd.NA\n",
    "                return float(val) if not pd.isna(val) else np.nan\n",
    "            except Exception:\n",
    "                return np.nan\n",
    "        s = s.map(to_num)\n",
    "\n",
    "    # -------- Fecha robusta --------\n",
    "    elif tipo == 'date':\n",
    "        formatos = opts.get('formats', [])\n",
    "        dayfirst = opts.get('dayfirst', True)\n",
    "        def to_date(v):\n",
    "            if pd.isna(v):\n",
    "                return pd.NaT\n",
    "            t = str(v).strip()\n",
    "            # Probar formatos explícitos\n",
    "            for fmt in formatos:\n",
    "                try:\n",
    "                    return pd.to_datetime(datetime.strptime(t, fmt))\n",
    "                except Exception:\n",
    "                    continue\n",
    "            # Fallback: pandas con dayfirst\n",
    "            try:\n",
    "                return pd.to_datetime(t, dayfirst=dayfirst, errors='coerce')\n",
    "            except Exception:\n",
    "                return pd.NaT\n",
    "        s = s.map(to_date)\n",
    "\n",
    "    else:\n",
    "        # Default: trim y convertir tokens faltantes a NaN para texto\n",
    "        if s.dtype == object or pd.api.types.is_string_dtype(s):\n",
    "            s = s.map(lambda x: np.nan if es_valor_faltante(x) else (str(x).strip() if not pd.isna(x) else x))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e185c054-2f13-4a34-be4d-67dbd546abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertir_tipos_postprocesamiento(df, reglas_por_columna):\n",
    "    \"\"\"\n",
    "    Garantiza dtypes correctos:\n",
    "     - Para columnas numeric: pd.to_numeric(...) + conversión a Int64 nullable si as_int, o float.\n",
    "     - Para columnas date: intenta parsear según formatos; deja datetime64[ns] o, si 'format_output'=='YYYY/MM/DD', devuelve strings con ese formato.\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "    for col, regla in (reglas_por_columna or {}).items():\n",
    "        tipo = regla if not isinstance(regla, tuple) else regla[0]\n",
    "        opts = {} if not isinstance(regla, tuple) else regla[1] or {}\n",
    "        if tipo == 'numeric':\n",
    "            df2[col] = pd.to_numeric(df2[col], errors='coerce')\n",
    "            if opts.get('as_int', False):\n",
    "                # convertir a Int64 nullable\n",
    "                try:\n",
    "                    df2[col] = df2[col].astype('Int64')\n",
    "                except Exception:\n",
    "                    # fallback: mantener float si conversion falla\n",
    "                    df2[col] = pd.to_numeric(df2[col], errors='coerce')\n",
    "        elif tipo == 'date':\n",
    "            formatos = opts.get('formats', [])\n",
    "            dayfirst = opts.get('dayfirst', True)\n",
    "            parsed = pd.Series(pd.NaT, index=df2.index)\n",
    "            # probar formatos explícitos uno por uno\n",
    "            for fmt in formatos:\n",
    "                try:\n",
    "                    mask_necesita = parsed.isna()\n",
    "                    intent = pd.to_datetime(df2.loc[mask_necesita, col].astype(str), format=fmt, errors='coerce')\n",
    "                    parsed.loc[mask_necesita] = intent\n",
    "                except Exception:\n",
    "                    pass\n",
    "            # fallback general para los que quedaron NaT\n",
    "            still_na = parsed.isna()\n",
    "            if still_na.any():\n",
    "                parsed.loc[still_na] = pd.to_datetime(df2.loc[still_na, col].astype(str), dayfirst=dayfirst, errors='coerce')\n",
    "            df2[col] = parsed\n",
    "            if opts.get('format_output') == 'YYYY/MM/DD':\n",
    "                # convertir a string con formato pedido (mantener NaT como NaN)\n",
    "                df2[col] = df2[col].dt.strftime('%Y/%m/%d')\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "337bd9f4-5291-4b9d-af66-23a8e3086c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_dataframe(df, reglas_por_columna=None):\n",
    "    \"\"\"\n",
    "    Limpieza principal (misma firma que antes):\n",
    "     1) Aplica aplicar_regla_columna por cada columna según reglas_por_columna\n",
    "     2) Para columnas texto por defecto: strip + tokens faltantes -> NaN\n",
    "     3) Elimina duplicados exactos\n",
    "     4) Convierte tipos numéricos y fechas con convertir_tipos_postprocesamiento\n",
    "    Retorna df limpio con dtypes corregidos.\n",
    "    \"\"\"\n",
    "    reglas_por_columna = reglas_por_columna or {}\n",
    "    df2 = df.copy()\n",
    "    df2.columns = [str(c).strip() for c in df2.columns]\n",
    "\n",
    "    for col in df2.columns:\n",
    "        if col in reglas_por_columna:\n",
    "            df2[col] = aplicar_regla_columna(df2[col], reglas_por_columna[col])\n",
    "        else:\n",
    "            if df2[col].dtype == object or pd.api.types.is_string_dtype(df2[col]):\n",
    "                df2[col] = df2[col].map(lambda x: np.nan if es_valor_faltante(x) else (str(x).strip() if not pd.isna(x) else x))\n",
    "\n",
    "    df2 = df2.drop_duplicates(keep='first').reset_index(drop=True)\n",
    "    # Postprocesamiento de tipos\n",
    "    df2 = convertir_tipos_postprocesamiento(df2, reglas_por_columna)\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4634aeae-c21a-47dc-9bb9-cffdeafcbe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detección de outliers (IQR)\n",
    "def mascara_valores_atipicos_rango_intercuartil(serie_datos):\n",
    "    \"\"\"\n",
    "    Devuelve máscara booleana (True = outlier) según IQR.\n",
    "    \"\"\"\n",
    "    serie_limpia = serie_datos.dropna().astype(float)\n",
    "    if serie_limpia.shape[0] < 4:\n",
    "        return pd.Series([False] * len(serie_datos), index=serie_datos.index)\n",
    "    q1 = serie_limpia.quantile(0.25)\n",
    "    q3 = serie_limpia.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    limite_inferior = q1 - 1.5 * iqr\n",
    "    limite_superior = q3 + 1.5 * iqr\n",
    "    return (serie_datos < limite_inferior) | (serie_datos > limite_superior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4c15444c-3df6-409e-a932-c08afaebfd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detección de outliers (Z-score)\n",
    "def mascara_valores_atipicos_zscore(serie_datos, umbral=3.0):\n",
    "    serie_limpia = serie_datos.dropna().astype(float)\n",
    "    if serie_limpia.shape[0] < 4 or serie_limpia.std() == 0:\n",
    "        return pd.Series([False] * len(serie_datos), index=serie_datos.index)\n",
    "    puntaje_z = (serie_datos - serie_limpia.mean()) / serie_limpia.std()\n",
    "    return puntaje_z.abs() > umbral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "32837ebb-e71d-4c31-bcfc-9e747390519e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funciones utilitarias definidas.\n"
     ]
    }
   ],
   "source": [
    "print('Funciones utilitarias definidas.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f411cb90-17af-45f2-9dfb-9fe7534bacbb",
   "metadata": {},
   "source": [
    "# limpieza y normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fe131403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Función detectar_problemas_en_dataframe cargada.\n"
     ]
    }
   ],
   "source": [
    "# ---------- Detección de problemas en un DataFrame ----------\n",
    "def detectar_problemas_en_dataframe(df: pd.DataFrame):\n",
    "    \"\"\"Detecta problemas comunes en un DataFrame y retorna:\n",
    "    - resumen: diccionario con métricas generales\n",
    "    - chequeos_por_columna: dict con información por columna\n",
    "    - problemas_df: DataFrame con filas problemáticas y descripción\n",
    "    \"\"\"\n",
    "    resumen = {}\n",
    "    resumen['filas'] = df.shape[0]\n",
    "    resumen['columnas'] = df.shape[1]\n",
    "    resumen['nulos_por_columna'] = df.isna().sum().to_dict()\n",
    "    dup_mask = df.duplicated(keep=False)\n",
    "    resumen['duplicados_exactos'] = int(dup_mask.sum())\n",
    "\n",
    "    chequeos_por_columna = {}\n",
    "    for col in df.columns:\n",
    "        ser = df[col]\n",
    "        info = {'dtype': str(ser.dtype), 'nulos': int(ser.isna().sum())}\n",
    "        if ser.dtype == object or pd.api.types.is_string_dtype(ser):\n",
    "            s = ser.astype(str)\n",
    "            info['espacios_inicio'] = int(s.str.match(r'^\\s+').sum())\n",
    "            info['espacios_final'] = int(s.str.match(r'\\s+$').sum())\n",
    "            try:\n",
    "                unique_original = set(s.dropna().unique())\n",
    "                unique_lower = set(s.dropna().str.lower().unique())\n",
    "                info['unique_original'] = len(unique_original)\n",
    "                info['unique_lower'] = len(unique_lower)\n",
    "                info['variantes_mayusculas'] = len(unique_lower) < len(unique_original)\n",
    "            except Exception:\n",
    "                info['unique_original'] = ser.nunique(dropna=True)\n",
    "                info['unique_lower'] = None\n",
    "                info['variantes_mayusculas'] = None\n",
    "            try:\n",
    "                unaccented = s.dropna().map(lambda x: sacar_acentos(x).lower())\n",
    "                groups = unaccented.groupby(unaccented).size()\n",
    "                conflicts = groups[groups > 1]\n",
    "                info['grupos_variantes_acentos'] = int(conflicts.shape[0])\n",
    "                ejemplos = {}\n",
    "                if not conflicts.empty:\n",
    "                    for val in conflicts.index[:5]:\n",
    "                        originales = sorted(list(s[unaccented == val].unique())[:10])\n",
    "                        ejemplos[val] = originales\n",
    "                info['ejemplos_variantes_acentos'] = ejemplos\n",
    "            except Exception:\n",
    "                info['grupos_variantes_acentos'] = None\n",
    "                info['ejemplos_variantes_acentos'] = {}\n",
    "            info['tokens_aparente_faltante'] = int(s.map(lambda x: str(x).strip().lower()).map(lambda v: sacar_acentos(v) in TOKENS_VALOR_FALTANTE).sum())\n",
    "            info['muestras'] = list(s.dropna().unique()[:10])\n",
    "        else:\n",
    "            if pd.api.types.is_numeric_dtype(ser):\n",
    "                s_f = ser.dropna().astype(float)\n",
    "                info['media'] = float(s_f.mean()) if not s_f.empty else None\n",
    "                info['std'] = float(s_f.std()) if not s_f.empty else None\n",
    "                info['min'] = float(s_f.min()) if not s_f.empty else None\n",
    "                info['max'] = float(s_f.max()) if not s_f.empty else None\n",
    "                if len(s_f) >= 4:\n",
    "                    q1 = s_f.quantile(0.25)\n",
    "                    q3 = s_f.quantile(0.75)\n",
    "                    iqr = q3 - q1\n",
    "                    lb = q1 - 1.5 * iqr\n",
    "                    ub = q3 + 1.5 * iqr\n",
    "                    out_iqr = (s_f < lb) | (s_f > ub)\n",
    "                    info['outliers_iqr'] = int(out_iqr.sum())\n",
    "                    info['limites_iqr'] = (float(lb), float(ub))\n",
    "                else:\n",
    "                    info['outliers_iqr'] = None\n",
    "                    info['limites_iqr'] = None\n",
    "                if len(s_f) >= 4 and s_f.std() != 0:\n",
    "                    z = (s_f - s_f.mean()) / s_f.std()\n",
    "                    info['outliers_z'] = int((z.abs() > 3).sum())\n",
    "                else:\n",
    "                    info['outliers_z'] = None\n",
    "            else:\n",
    "                parsed = pd.to_datetime(ser, errors='coerce', dayfirst=True)\n",
    "                info['fechas_parseables'] = int(parsed.notna().sum())\n",
    "                info['muestras'] = list(ser.dropna().unique()[:10])\n",
    "        chequeos_por_columna[col] = info\n",
    "\n",
    "    filas_problemas = []\n",
    "    for idx, fila in df.iterrows():\n",
    "        lista_problemas = []\n",
    "        if dup_mask.loc[idx]:\n",
    "            lista_problemas.append('duplicado_exacto')\n",
    "        for col in df.columns:\n",
    "            val = fila[col]\n",
    "            # heuristicas textuales\n",
    "            if pd.api.types.is_string_dtype(type(val)) or isinstance(val, str) or (not pd.isna(val) and not pd.api.types.is_numeric_dtype(type(val)) and str(chequeos_por_columna[col].get('dtype','')).startswith('object')):\n",
    "                s = str(val)\n",
    "                if s != s.strip():\n",
    "                    lista_problemas.append(f'espacios_en_columna_{col}')\n",
    "                if chequeos_por_columna[col].get('variantes_mayusculas'):\n",
    "                    if s and s != s.lower() and s.lower() in [str(x).lower() for x in df[col].dropna().unique()]:\n",
    "                        lista_problemas.append(f'inconsistencia_mayusculas_columna_{col}')\n",
    "                if chequeos_por_columna[col].get('grupos_variantes_acentos') and chequeos_por_columna[col]['grupos_variantes_acentos'] > 0:\n",
    "                    try:\n",
    "                        un = sacar_acentos(s).lower()\n",
    "                        group_vals = [x for x in chequeos_por_columna[col].get('muestras', []) if sacar_acentos(str(x)).lower() == un]\n",
    "                        if group_vals and any(sacar_acentos(str(x)).lower() != sacar_acentos(s).lower() for x in group_vals):\n",
    "                            lista_problemas.append(f'variantes_acentos_columna_{col}')\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                if es_valor_faltante(s):\n",
    "                    lista_problemas.append(f'token_faltante_columna_{col}')\n",
    "            else:\n",
    "                try:\n",
    "                    fval = float(val)\n",
    "                    info_col = chequeos_por_columna[col]\n",
    "                    limites = info_col.get('limites_iqr')\n",
    "                    if limites and (fval < limites[0] or fval > limites[1]):\n",
    "                        lista_problemas.append(f'outlier_iqr_columna_{col}')\n",
    "                    if info_col.get('std') not in (None, 0):\n",
    "                        mean = info_col.get('media')\n",
    "                        std = info_col.get('std')\n",
    "                        if std and abs((fval - mean) / std) > 3:\n",
    "                            lista_problemas.append(f'outlier_z_columna_{col}')\n",
    "                except Exception:\n",
    "                    pass\n",
    "        if lista_problemas:\n",
    "            filas_problemas.append({'row_index': idx, 'problemas': ';'.join(sorted(set(lista_problemas))), 'muestra': json.dumps({str(c): str(fila[c]) for c in df.columns[:8]})})\n",
    "    df_problemas = pd.DataFrame(filas_problemas)\n",
    "    return resumen, chequeos_por_columna, df_problemas\n",
    "\n",
    "print('Función detectar_problemas_en_dataframe cargada.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ddb47523-22aa-403a-bfd2-82f3ae8fa01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funciones de limpieza cargadas.\n"
     ]
    }
   ],
   "source": [
    "print('Funciones de limpieza cargadas.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "895c63d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Proceso principal para los 3 CSV ----------\n",
    "reglas_ejemplo_marketing = {\n",
    "                        'producto': ('lower', {}),\n",
    "                        'canal': ('lower', {}),\n",
    "                        'costo': ('numeric', {'remove_thousands': True, 'as_int': False}),\n",
    "                        'fecha_inicio': ('date', {'dayfirst': True, 'formats': ['%d/%m/%Y', '%Y-%m-%d']}),\n",
    "                        'fecha_fin': ('date', {'dayfirst': True, 'formats': ['%d/%m/%Y', '%Y-%m-%d']})   \n",
    "}\n",
    "reglas_ejemplo_ventas = {\n",
    "                        'producto': ('lower', {}),\n",
    "                        'precio': ('numeric', {'remove_thousands': True, 'as_int': False}),\n",
    "                        'cantidad': ('numeric', {'remove_thousands': True, 'as_int': False}),\n",
    "                        'fecha_venta': ('date', {'dayfirst': True, 'formats': ['%d/%m/%Y', '%Y-%m-%d']}),\n",
    "                        'categoria': ('lower', {})\n",
    "}\n",
    "reglas_ejemplo_clientes = {\n",
    "                        'edad': ('numeric', {'remove_thousands': True, 'as_int': True}),\n",
    "                        'ciudad': ('lower', {}),\n",
    "                        'ingresos': ('numeric', {'remove_thousands': True, 'as_int': False}),\n",
    "                        'apellido': ('title', {}),\n",
    "                        'dni': ('numeric', {'remove_non_digits': True, 'as_int': True})\n",
    "}\n",
    "reglas_por_archivo = {\n",
    "    'marketing.csv': reglas_ejemplo_marketing,\n",
    "    'ventas.csv': reglas_ejemplo_ventas,\n",
    "    'clientes.csv': reglas_ejemplo_clientes\n",
    "}\n",
    "reportes_creados = []\n",
    "zip_path = carpeta_limpios.parent / 'reports_dataset_tpi_v2.zip'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6e42599a-abe2-4447-b8e6-6c1024f0d651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función auxiliar para guardar CSVs\n",
    "def guardar_csv(df, ruta):\n",
    "    \"\"\"\n",
    "    Guarda df en ruta (string o Path). Crea directorio padre si no existe.\n",
    "    \"\"\"\n",
    "    ruta = Path(ruta)\n",
    "    ruta.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(ruta, index=False, encoding='utf-8')\n",
    "    return ruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ccc0f0c3-0e31-4f27-9661-3215c9185f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_frames = {\n",
    "    ruta_ventas     : df_ventas,\n",
    "    ruta_clientes   : df_clientes,\n",
    "    ruta_marketing  : df_marketing\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "abb6dde0-762c-440f-9321-f242bde4a7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    resumen_antes={'filas': 3035, 'columnas': 6, 'nulos_por_columna': {'id_venta': 0, 'producto': 0, 'precio': 2, 'cantidad': 2, 'fecha_venta': 0, 'categoria': 0}, 'duplicados_exactos': 70}\n",
      "    chequeos_antes={'id_venta': {'dtype': 'int64', 'nulos': 0, 'media': 1499.8514003294893, 'std': 866.4653790689496, 'min': 1.0, 'max': 3000.0, 'outliers_iqr': 0, 'limites_iqr': (-1503.0, 4501.0), 'outliers_z': 0}, 'producto': {'dtype': 'object', 'nulos': 0, 'espacios_inicio': 0, 'espacios_final': 0, 'unique_original': 30, 'unique_lower': 30, 'variantes_mayusculas': False, 'grupos_variantes_acentos': 30, 'ejemplos_variantes_acentos': {'adorno de pared': ['Adorno de pared'], 'alfombra': ['Alfombra'], 'aspiradora': ['Aspiradora'], 'auriculares': ['Auriculares'], 'batidora': ['Batidora']}, 'tokens_aparente_faltante': 0, 'muestras': ['Cuadro decorativo', 'Lámpara de mesa', 'Secadora', 'Heladera', 'Horno eléctrico', 'Plancha de vapor', 'Proyector', 'Rincón de plantas', 'Candelabro', 'Aspiradora']}, 'precio': {'dtype': 'object', 'nulos': 2, 'espacios_inicio': 0, 'espacios_final': 0, 'unique_original': 2591, 'unique_lower': 2591, 'variantes_mayusculas': False, 'grupos_variantes_acentos': 396, 'ejemplos_variantes_acentos': {'$100.58': ['$100.58'], '$100.69': ['$100.69'], '$101.08': ['$101.08'], '$101.54': ['$101.54'], '$101.57': ['$101.57']}, 'tokens_aparente_faltante': 0, 'muestras': ['$69.94', '$105.10', '$97.96', '$114.35', '$106.21', '$35.35', '$65.43', '$88.17', '$79.86', '$66.11']}, 'cantidad': {'dtype': 'float64', 'nulos': 2, 'media': 6.496538081107814, 'std': 3.457249927514167, 'min': 1.0, 'max': 12.0, 'outliers_iqr': 0, 'limites_iqr': (-6.0, 18.0), 'outliers_z': 0}, 'fecha_venta': {'dtype': 'object', 'nulos': 0, 'espacios_inicio': 0, 'espacios_final': 0, 'unique_original': 364, 'unique_lower': 364, 'variantes_mayusculas': False, 'grupos_variantes_acentos': 361, 'ejemplos_variantes_acentos': {'01/02/2024': ['01/02/2024'], '01/03/2024': ['01/03/2024'], '01/04/2024': ['01/04/2024'], '01/05/2024': ['01/05/2024'], '01/06/2024': ['01/06/2024']}, 'tokens_aparente_faltante': 0, 'muestras': ['02/01/2024', '03/01/2024', '04/01/2024', '05/01/2024', '06/01/2024', '07/01/2024', '08/01/2024', '09/01/2024', '10/01/2024', '11/01/2024']}, 'categoria': {'dtype': 'object', 'nulos': 0, 'espacios_inicio': 0, 'espacios_final': 0, 'unique_original': 3, 'unique_lower': 3, 'variantes_mayusculas': False, 'grupos_variantes_acentos': 3, 'ejemplos_variantes_acentos': {'decoracion': ['Decoración'], 'electrodomesticos': ['Electrodomésticos'], 'electronica': ['Electrónica']}, 'tokens_aparente_faltante': 0, 'muestras': ['Decoración', 'Electrodomésticos', 'Electrónica']}}\n",
      "    problemas_antes=    row_index         problemas  \\\n",
      "0         820  duplicado_exacto   \n",
      "1         821  duplicado_exacto   \n",
      "2         822  duplicado_exacto   \n",
      "3         823  duplicado_exacto   \n",
      "4         824  duplicado_exacto   \n",
      "..        ...               ...   \n",
      "65        885  duplicado_exacto   \n",
      "66        886  duplicado_exacto   \n",
      "67        887  duplicado_exacto   \n",
      "68        888  duplicado_exacto   \n",
      "69        889  duplicado_exacto   \n",
      "\n",
      "                                              muestra  \n",
      "0   {\"id_venta\": \"56\", \"producto\": \"Cortinas\", \"pr...  \n",
      "1   {\"id_venta\": \"421\", \"producto\": \"L\\u00e1mpara ...  \n",
      "2   {\"id_venta\": \"424\", \"producto\": \"Jarr\\u00f3n d...  \n",
      "3   {\"id_venta\": \"1868\", \"producto\": \"Cafetera\", \"...  \n",
      "4   {\"id_venta\": \"2545\", \"producto\": \"Auriculares\"...  \n",
      "..                                                ...  \n",
      "65  {\"id_venta\": \"1381\", \"producto\": \"Freidora el\\...  \n",
      "66  {\"id_venta\": \"2365\", \"producto\": \"Auriculares\"...  \n",
      "67  {\"id_venta\": \"2486\", \"producto\": \"Laptop\", \"pr...  \n",
      "68  {\"id_venta\": \"2506\", \"producto\": \"Laptop\", \"pr...  \n",
      "69  {\"id_venta\": \"2705\", \"producto\": \"Auriculares\"...  \n",
      "\n",
      "[70 rows x 3 columns]\n",
      "    \n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 't' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m (\u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[33m\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresumen_antes\u001b[38;5;132;01m=}\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33m\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchequeos_antes\u001b[38;5;132;01m=}\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33m\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproblemas_antes\u001b[38;5;132;01m=}\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m)\n\u001b[32m      8\u001b[39m reglas = reglas_por_archivo.get(nombre_archivo, {})\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m df_limpio = \u001b[43mlimpiar_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_actual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreglas_por_columna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreglas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m resumen_despues, chequeos_despues, problemas_despues = detectar_problemas_en_dataframe(df_limpio)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m (\u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[33m\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresumen_antes\u001b[38;5;132;01m=}\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33m\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchequeos_antes\u001b[38;5;132;01m=}\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33m\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproblemas_antes\u001b[38;5;132;01m=}\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m)    \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mlimpiar_dataframe\u001b[39m\u001b[34m(df, reglas_por_columna)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df2.columns:\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m reglas_por_columna:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m         df2[col] = \u001b[43maplicar_regla_columna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf2\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreglas_por_columna\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     18\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m df2[col].dtype == \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m pd.api.types.is_string_dtype(df2[col]):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36maplicar_regla_columna\u001b[39m\u001b[34m(serie, regla)\u001b[39m\n\u001b[32m     63\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m     64\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m np.nan\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     s = \u001b[43ms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_num\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# -------- Fecha robusta --------\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m tipo == \u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:4711\u001b[39m, in \u001b[36mSeries.map\u001b[39m\u001b[34m(self, arg, na_action)\u001b[39m\n\u001b[32m   4631\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmap\u001b[39m(\n\u001b[32m   4632\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4633\u001b[39m     arg: Callable | Mapping | Series,\n\u001b[32m   4634\u001b[39m     na_action: Literal[\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   4635\u001b[39m ) -> Series:\n\u001b[32m   4636\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4637\u001b[39m \u001b[33;03m    Map values of Series according to an input mapping or function.\u001b[39;00m\n\u001b[32m   4638\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4709\u001b[39m \u001b[33;03m    dtype: object\u001b[39;00m\n\u001b[32m   4710\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4711\u001b[39m     new_values = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4712\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._constructor(new_values, index=\u001b[38;5;28mself\u001b[39m.index, copy=\u001b[38;5;28;01mFalse\u001b[39;00m).__finalize__(\n\u001b[32m   4713\u001b[39m         \u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mmap\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4714\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\site-packages\\pandas\\core\\base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36maplicar_regla_columna.<locals>.to_num\u001b[39m\u001b[34m(v)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mto_num\u001b[39m(v):\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     t = \u001b[43mt\u001b[49m.replace(\u001b[33m'\u001b[39m\u001b[33m$\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pd.isna(v):\n\u001b[32m     41\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m np.nan\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 't' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "for  nombre_archivo, df_actual in dic_frames.items():\n",
    "    resumen_antes, chequeos_antes, problemas_antes = detectar_problemas_en_dataframe(df_actual)\n",
    "    print (f\"\"\"\n",
    "    {resumen_antes=}\n",
    "    {chequeos_antes=}\n",
    "    {problemas_antes=}\n",
    "    \"\"\")\n",
    "    reglas = reglas_por_archivo.get(nombre_archivo, {})\n",
    "    df_limpio = limpiar_dataframe(df_actual, reglas_por_columna=reglas)\n",
    "    \n",
    "    resumen_despues, chequeos_despues, problemas_despues = detectar_problemas_en_dataframe(df_limpio)\n",
    "    print (f\"\"\"\n",
    "    {resumen_antes=}\n",
    "    {chequeos_antes=}\n",
    "    {problemas_antes=}\n",
    "    \"\"\")    \n",
    "    print(f\"\"\"\n",
    "    {\"-\"*100}\n",
    "    {nombre_archivo=}\n",
    "    {type(nombre_archivo)=}\n",
    "    {\"-\"*100}\n",
    "    \"\"\")\n",
    "    ruta_nombre_limpio= nombre_archivo[:-4]+\" limpio.csv\"\n",
    "    print(f\"\"\"\n",
    "    {\"-\"*100}\n",
    "    {ruta_nombre_limpio=}\n",
    "    {\"-\"*100}\n",
    "    \"\"\")\n",
    "    \n",
    "    guardar_csv(df_limpio, ruta_nombre_limpio)\n",
    "\n",
    "\n",
    "    #########################################################\n",
    "    # sobreescribo en RAM\n",
    "    if \"marketing\"  in nombre_archivo :\n",
    "        df_marketing  =  df_actual\n",
    "    elif \"ventas\"   in nombre_archivo :\n",
    "        df_ventas     =  df_actual\n",
    "    elif \"clientes\" in nombre_archivo :\n",
    "        df_clientes   =  df_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32235fb9-1e09-422e-931b-3471d6fbf123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Asegurarse de tener las utilidades básicas antes de esto ----------\n",
    "# sacar_acentos(texto)  -> elimina tildes\n",
    "# es_valor_faltante(valor) -> True/False para tokens de faltante\n",
    "# carpeta_limpios  -> Path donde guardar los cleaned (si no existe, se crea)\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# crear carpeta_limpios si no existe (ajustar ruta si corresponde)\n",
    "try:\n",
    "    carpeta_limpios\n",
    "except NameError:\n",
    "    carpeta_limpios = Path('./reportes/limpios')\n",
    "carpeta_limpios.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# ---------- Funciones requeridas (misma firma que pediste) ----------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def mascara_valores_atipicos_zscore(serie_datos, umbral=3.0):\n",
    "    \"\"\"\n",
    "    Devuelve máscara booleana (True = outlier) según Z-score.\n",
    "    \"\"\"\n",
    "    s = serie_datos.dropna().astype(float)\n",
    "    if s.shape[0] < 4 or s.std() == 0:\n",
    "        return pd.Series([False] * len(serie_datos), index=serie_datos.index)\n",
    "    z = (serie_datos - s.mean()) / s.std()\n",
    "    return z.abs() > umbral\n",
    "\n",
    "\n",
    "def detectar_problemas_en_dataframe(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Detecta problemas comunes en un DataFrame.\n",
    "    Retorna: resumen (dict), chequeos_por_columna (dict), problemas_df (DataFrame filas con problemas)\n",
    "    \"\"\"\n",
    "    resumen = {}\n",
    "    resumen['filas'] = df.shape[0]\n",
    "    resumen['columnas'] = df.shape[1]\n",
    "    resumen['nulos_por_columna'] = df.isna().sum().to_dict()\n",
    "    dup_mask = df.duplicated(keep=False)\n",
    "    resumen['duplicados_exactos'] = int(dup_mask.sum())\n",
    "\n",
    "    chequeos_por_columna = {}\n",
    "    for col in df.columns:\n",
    "        ser = df[col]\n",
    "        info = {'dtype': str(ser.dtype), 'nulos': int(ser.isna().sum())}\n",
    "        # texto\n",
    "        if ser.dtype == object or pd.api.types.is_string_dtype(ser):\n",
    "            s = ser.astype(str)\n",
    "            info['espacios_inicio'] = int(s.str.match(r'^\\s+').sum())\n",
    "            info['espacios_final'] = int(s.str.match(r'\\s+$').sum())\n",
    "            try:\n",
    "                unique_original = set(s.dropna().unique())\n",
    "                unique_lower = set(s.dropna().str.lower().unique())\n",
    "                info['unique_original'] = len(unique_original)\n",
    "                info['unique_lower'] = len(unique_lower)\n",
    "                info['variantes_mayusculas'] = len(unique_lower) < len(unique_original)\n",
    "            except Exception:\n",
    "                info['unique_original'] = ser.nunique(dropna=True)\n",
    "                info['unique_lower'] = None\n",
    "                info['variantes_mayusculas'] = None\n",
    "            try:\n",
    "                unaccented = s.dropna().map(lambda x: sacar_acentos(x).lower())\n",
    "                groups = unaccented.groupby(unaccented).size()\n",
    "                conflicts = groups[groups > 1]\n",
    "                info['grupos_variantes_acentos'] = int(conflicts.shape[0])\n",
    "                ejemplos = {}\n",
    "                if not conflicts.empty:\n",
    "                    for val in conflicts.index[:5]:\n",
    "                        originales = sorted(list(s[unaccented == val].unique())[:10])\n",
    "                        ejemplos[val] = originales\n",
    "                info['ejemplos_variantes_acentos'] = ejemplos\n",
    "            except Exception:\n",
    "                info['grupos_variantes_acentos'] = None\n",
    "                info['ejemplos_variantes_acentos'] = {}\n",
    "            info['tokens_aparente_faltante'] = int(\n",
    "                s.map(lambda x: str(x).strip().lower()).map(lambda v: sacar_acentos(v) in TOKENS_VALOR_FALTANTE).sum()\n",
    "            )\n",
    "            info['muestras'] = list(s.dropna().unique()[:10])\n",
    "        else:\n",
    "            # numeric\n",
    "            if pd.api.types.is_numeric_dtype(ser) or (ser.dropna().astype(str).str.replace('.','',1).str.isnumeric().all() if len(ser.dropna())>0 else False):\n",
    "                try:\n",
    "                    s_f = ser.dropna().astype(float)\n",
    "                except Exception:\n",
    "                    s_f = pd.to_numeric(ser, errors='coerce').dropna().astype(float)\n",
    "                info['media'] = float(s_f.mean()) if not s_f.empty else None\n",
    "                info['std'] = float(s_f.std()) if not s_f.empty else None\n",
    "                info['min'] = float(s_f.min()) if not s_f.empty else None\n",
    "                info['max'] = float(s_f.max()) if not s_f.empty else None\n",
    "                if len(s_f) >= 4:\n",
    "                    q1 = s_f.quantile(0.25)\n",
    "                    q3 = s_f.quantile(0.75)\n",
    "                    iqr = q3 - q1\n",
    "                    lb = q1 - 1.5 * iqr\n",
    "                    ub = q3 + 1.5 * iqr\n",
    "                    out_iqr = (s_f < lb) | (s_f > ub)\n",
    "                    info['outliers_iqr'] = int(out_iqr.sum())\n",
    "                    info['limites_iqr'] = (float(lb), float(ub))\n",
    "                else:\n",
    "                    info['outliers_iqr'] = None\n",
    "                    info['limites_iqr'] = None\n",
    "                if len(s_f) >= 4 and s_f.std() != 0:\n",
    "                    z = (s_f - s_f.mean()) / s_f.std()\n",
    "                    info['outliers_z'] = int((z.abs() > 3).sum())\n",
    "                else:\n",
    "                    info['outliers_z'] = None\n",
    "            else:\n",
    "                # fechas intento parseo\n",
    "                parsed = pd.to_datetime(ser, errors='coerce', dayfirst=True)\n",
    "                info['fechas_parseables'] = int(parsed.notna().sum())\n",
    "                info['muestras'] = list(ser.dropna().unique()[:10])\n",
    "        chequeos_por_columna[col] = info\n",
    "\n",
    "    filas_problemas = []\n",
    "    for idx, fila in df.iterrows():\n",
    "        lista_problemas = []\n",
    "        if dup_mask.loc[idx]:\n",
    "            lista_problemas.append('duplicado_exacto')\n",
    "        for col in df.columns:\n",
    "            val = fila[col]\n",
    "            # heurísticas textuales\n",
    "            if pd.api.types.is_string_dtype(type(val)) or isinstance(val, str) or (\n",
    "                not pd.isna(val) and not pd.api.types.is_numeric_dtype(type(val)) and str(chequeos_por_columna[col].get('dtype','')).startswith('object')\n",
    "            ):\n",
    "                s = str(val)\n",
    "                if s != s.strip():\n",
    "                    lista_problemas.append(f'espacios_en_columna_{col}')\n",
    "                if chequeos_por_columna[col].get('variantes_mayusculas'):\n",
    "                    if s and s != s.lower() and s.lower() in [str(x).lower() for x in df[col].dropna().unique()]:\n",
    "                        lista_problemas.append(f'inconsistencia_mayusculas_columna_{col}')\n",
    "                if chequeos_por_columna[col].get('grupos_variantes_acentos') and chequeos_por_columna[col]['grupos_variantes_acentos'] > 0:\n",
    "                    try:\n",
    "                        un = sacar_acentos(s).lower()\n",
    "                        group_vals = [x for x in chequeos_por_columna[col].get('muestras', []) if sacar_acentos(str(x)).lower() == un]\n",
    "                        if group_vals and any(sacar_acentos(str(x)).lower() != sacar_acentos(s).lower() for x in group_vals):\n",
    "                            lista_problemas.append(f'variantes_acentos_columna_{col}')\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                if es_valor_faltante(s):\n",
    "                    lista_problemas.append(f'token_faltante_columna_{col}')\n",
    "            else:\n",
    "                # heurísticas numéricas\n",
    "                try:\n",
    "                    fval = float(val)\n",
    "                    info_col = chequeos_por_columna[col]\n",
    "                    limites = info_col.get('limites_iqr')\n",
    "                    if limites and (fval < limites[0] or fval > limites[1]):\n",
    "                        lista_problemas.append(f'outlier_iqr_columna_{col}')\n",
    "                    if info_col.get('std') not in (None, 0):\n",
    "                        mean = info_col.get('media')\n",
    "                        std = info_col.get('std')\n",
    "                        if std and abs((fval - mean) / std) > 3:\n",
    "                            lista_problemas.append(f'outlier_z_columna_{col}')\n",
    "                except Exception:\n",
    "                    pass\n",
    "        if lista_problemas:\n",
    "            filas_problemas.append({\n",
    "                'row_index': idx,\n",
    "                'problemas': ';'.join(sorted(set(lista_problemas))),\n",
    "                'muestra': json.dumps({str(c): str(fila[c]) for c in df.columns[:8]})\n",
    "            })\n",
    "\n",
    "    df_problemas = pd.DataFrame(filas_problemas)\n",
    "    return resumen, chequeos_por_columna, df_problemas\n",
    "\n",
    "\n",
    "def limpiar_dataframe(df, reglas_por_columna=None):\n",
    "    \"\"\"\n",
    "    Limpieza principal (misma firma):\n",
    "    - aplica reglas por columna usando aplicar_regla_columna (debe existir)\n",
    "    - default para texto: strip y tokens faltantes -> NaN\n",
    "    - elimina duplicados exactos\n",
    "    - postprocesa conversiones basicas de tipo (numeric/date) si fue posible\n",
    "    \"\"\"\n",
    "    reglas_por_columna = reglas_por_columna or {}\n",
    "    df2 = df.copy()\n",
    "    df2.columns = [str(c).strip() for c in df2.columns]\n",
    "\n",
    "    for col in df2.columns:\n",
    "        if col in reglas_por_columna:\n",
    "            # usar la función aplicar_regla_columna existente en el notebook\n",
    "            df2[col] = aplicar_regla_columna(df2[col], reglas_por_columna[col])\n",
    "        else:\n",
    "            if df2[col].dtype == object or pd.api.types.is_string_dtype(df2[col]):\n",
    "                df2[col] = df2[col].map(lambda x: np.nan if es_valor_faltante(x) else (str(x).strip() if not pd.isna(x) else x))\n",
    "\n",
    "    # eliminar duplicados exactos\n",
    "    df2 = df2.drop_duplicates(keep='first').reset_index(drop=True)\n",
    "\n",
    "    # intentos de post-conversión sencillos para numeric/date según reglas:\n",
    "    for col, regla in (reglas_por_columna or {}).items():\n",
    "        tipo = regla if not isinstance(regla, tuple) else regla[0]\n",
    "        opts = {} if not isinstance(regla, tuple) else regla[1] or {}\n",
    "        if tipo == 'numeric':\n",
    "            df2[col] = pd.to_numeric(df2[col], errors='coerce')\n",
    "            if opts.get('as_int', False):\n",
    "                try:\n",
    "                    df2[col] = df2[col].astype('Int64')\n",
    "                except Exception:\n",
    "                    pass\n",
    "        elif tipo == 'date':\n",
    "            formatos = opts.get('formats', [])\n",
    "            dayfirst = opts.get('dayfirst', True)\n",
    "            parsed = pd.Series(pd.NaT, index=df2.index)\n",
    "            for fmt in formatos:\n",
    "                try:\n",
    "                    mask = parsed.isna()\n",
    "                    parsed.loc[mask] = pd.to_datetime(df2.loc[mask, col].astype(str), format=fmt, errors='coerce')\n",
    "                except Exception:\n",
    "                    pass\n",
    "            still_na = parsed.isna()\n",
    "            if still_na.any():\n",
    "                parsed.loc[still_na] = pd.to_datetime(df2.loc[still_na, col].astype(str), dayfirst=dayfirst, errors='coerce')\n",
    "            # si se pidió format_output, devolver cadena con YYYY/MM/DD\n",
    "            if opts.get('format_output') == 'YYYY/MM/DD':\n",
    "                df2[col] = parsed.dt.strftime('%Y/%m/%d')\n",
    "            else:\n",
    "                df2[col] = parsed\n",
    "\n",
    "    return df2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Bucle/menu principal (usa dic_frames) ----------\n",
    "def menu_procesar_diccionario(dic_frames, reglas_por_archivo):\n",
    "    \"\"\"\n",
    "    Recorre dic_frames: clave = nombre_archivo (ej. 'marketing.csv'), valor = DataFrame.\n",
    "    Ejecuta: detectar_problemas_en_dataframe antes, limpiar_dataframe, detectar_problemas_en_dataframe despues,\n",
    "    imprime resúmenes y guarda cleaned en carpeta_limpios con sufijo ' limpio.csv'.\n",
    "    También sobreescribe variables en RAM (df_marketing, df_ventas, df_clientes) si se encuentran en el nombre.\n",
    "    \"\"\"\n",
    "    # trabajamos sobre una copia para evitar modificar dict original por error\n",
    "    for nombre_archivo, df_actual in dic_frames.items():\n",
    "        resumen_antes, chequeos_antes, problemas_antes = detectar_problemas_en_dataframe(df_actual)\n",
    "        print(f\\\"\\\\n--- RESUMEN ANTES: {nombre_archivo} ---\\\\n\\\")\n",
    "        print(resumen_antes)\n",
    "        # Para no volcar objetos muy grandes, mostramos el head del DataFrame de problemas (si existe)\n",
    "        print('\\\\nMuestras de problemas antes (primeras 5 filas):')\n",
    "        display(problemas_antes.head(5) if not problemas_antes.empty else 'No se detectaron filas con problemas.')\n",
    "\n",
    "        reglas = reglas_por_archivo.get(nombre_archivo, {})\n",
    "        df_limpio = limpiar_dataframe(df_actual, reglas_por_columna=reglas)\n",
    "\n",
    "        resumen_despues, chequeos_despues, problemas_despues = detectar_problemas_en_dataframe(df_limpio)\n",
    "        print(f\\\"\\\\n--- RESUMEN DESPUÉS: {nombre_archivo} ---\\\\n\\\")\n",
    "        print(resumen_despues)\n",
    "        print('\\\\nMuestras de problemas después (primeras 5 filas):')\n",
    "        display(problemas_despues.head(5) if not problemas_despues.empty else 'No se detectaron filas con problemas tras la limpieza.')\n",
    "\n",
    "        # mostrar separadores y tipo-nombre\n",
    "        print('\\\\n' + '-'*100)\n",
    "        print(f'nombre_archivo = {nombre_archivo}')\n",
    "        print(f'type(nombre_archivo) = {type(nombre_archivo)}')\n",
    "        print('-'*100 + '\\\\n')\n",
    "\n",
    "        ruta_nombre_limpio = nombre_archivo[:-4] + ' limpio.csv' if nombre_archivo.lower().endswith('.csv') else nombre_archivo + ' limpio.csv'\n",
    "        print(f'ruta_nombre_limpio = {ruta_nombre_limpio}')\n",
    "\n",
    "        # guardar cleaned\n",
    "        ruta_guardado = carpeta_limpios / ruta_nombre_limpio\n",
    "        guardar_csv(df_limpio, ruta_guardado)\n",
    "        print(f'Guardado cleaned en: {ruta_guardado}')\n",
    "\n",
    "        # sobreescribir en RAM según el nombre\n",
    "        # (nota: usar globals() para actualizar variables en el entorno global del notebook)\n",
    "        if 'marketing' in nombre_archivo.lower():\n",
    "            globals()['df_marketing'] = df_limpio\n",
    "        elif 'ventas' in nombre_archivo.lower():\n",
    "            globals()['df_ventas'] = df_limpio\n",
    "        elif 'clientes' in nombre_archivo.lower():\n",
    "            globals()['df_clientes'] = df_limpio\n",
    "\n",
    "    print('\\\\nProceso completo del diccionario de DataFrames.')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bf7d2207-957f-40dd-8336-fa3664d5fdd4",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color:#CCCCCC; padding:10px; border-radius:6px;\">\n",
    "<h2 style=\"color:black; text-align:center;\">Notas pedagógicas y siguientes pasos</h2>\n",
    "\n",
    "<p style=\"color:black;\">- Revisa los archivos <code>reporte_antes_limpieza_<archivo>.csv</code> para ver ejemplos por fila y decidir reglas adicionales.</p>\n",
    "<p style=\"color:black;\">- Ajusta <code>reglas_por_archivo</code> según los nombres reales de las columnas en tus CSV.</p>\n",
    "<p style=\"color:black;\">- Si querés que ejecute el notebook aquí y muestre resultados, subí los 3 CSV a <code>./data_in/</code> o confirma que están en <code>/mnt/data/</code>.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50338261-fcc2-4dda-85b8-d6c12320e8fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f895b5b-5c12-442f-abf1-4b7a9d3ac96d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
