{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cedbb1b4",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color:#CCCCCC; padding:12px; border-radius:8px;\">\n",
    "<h1 style=\"color:#003366; text-align:center; margin:8px 0;\">Revisión y limpieza de 3 DataFrames (TPI - Data Analytics)</h1>\n",
    "<p style=\"text-align:center; color:#003366; margin:0;\"><em>Notebook docente en castellano — nombres descriptivos en snake_case — código y documentación</em></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c723b86a",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color:#CCCCCC; padding:10px; border-radius:6px;\">\n",
    "<h2 style=\"color:black; text-align:center; margin-top:6px;\">Resumen</h2>\n",
    "\n",
    "<p style=\"color:black;\">\n",
    "Este notebook está diseñado con finalidades pedagógicas. Revisa, normaliza y valida tres datasets contenidos en CSV:\n",
    "</p>\n",
    "\n",
    "<ul style=\"color:black;\">\n",
    "<li><code>marketing.csv</code> → variable: <code>df_marketing</code></li>\n",
    "<li><code>ventas.csv</code>    → variable: <code>df_ventas</code></li>\n",
    "<li><code>clientes.csv</code>  → variable: <code>df_clientes</code></li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"color:black;\">\n",
    "Coloca los CSV en <code>./data_in/</code> o en <code>/mnt/data/</code>. El notebook busca primero en <code>./data_in/</code> y si no encuentra, usa <code>/mnt/data/</code> (útil para entornos donde los archivos están pre-subidos).\n",
    "</p>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be46fb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports y configuración inicial (nombres en castellano)\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import unicodedata\n",
    "from colorama import *\n",
    "\n",
    "#!pip install gdown\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import zipfile\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from math import isnan\n",
    "ruta_base = \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63edff5b-828e-4254-a4d7-4b0ef05ecea6",
   "metadata": {},
   "source": [
    "## 1. Crear un documento en Google Colaboratory y cargar los sets de datos como DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfa535f-ef52-4f49-9d1b-e70dd8d5ad27",
   "metadata": {},
   "source": [
    "si se usa en disco local comentarla celda de debajo (JuPyteR , VSC, ATOM, Spider, Geany, etc)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "00de9cf8-da0d-4d31-bed7-31a704a7274d",
   "metadata": {},
   "source": [
    "# --- Paso 1: Montar Google Drive ---\n",
    "# Montar tu Google Drive\n",
    "!pip install google\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!ls \"/content/drive/MyDrive/CABA/Garcia Traba Ariel H - Comisión 25262 - TPI Data Analytics/\"\n",
    "# Ruta del archivo (ajústala a la carpeta real en tu Drive)\n",
    "ruta_base = \"/content/drive/MyDrive/CABA/Garcia Traba Ariel H - Comisión 25262 - TPI Data Analytics/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144444b3-1dc1-4f3f-a621-025d1cdb6465",
   "metadata": {},
   "source": [
    "# 1ra parte Definición de ETL\n",
    "ETL es un conjunto de procedimientos que permiten mover datos desde sistemas de origen, que pueden ser bases de datos, archivos o fuentes en la nube, hasta un sistema de destino como un data warehouse o data lake, realizando previamente procesos de limpieza, estructuración y organización de los datos para hacerlos aptos para análisis.​\n",
    "\n",
    "## Fases del proceso ETL\n",
    "Extracción: Consiste en recopilar datos relevantes de diferentes fuentes, asegurando que el impacto en los sistemas origen sea mínimo. Los datos pueden extraerse mediante diversos métodos como consultas SQL o servicios web.​\n",
    "\n",
    "Transformación: En esta etapa, los datos se limpian y se ajustan para garantizar coherencia y calidad, incluyendo la eliminación de valores nulos, normalización y conversión a formatos consistentes, además de aplicar reglas específicas de negocio.\n",
    "\n",
    "Carga: Finalmente, los datos transformados se cargan en el sistema de destino, donde estarán disponibles para análisis, informes o modelado de datos.\n",
    "\n",
    "## Importancia del ETL\n",
    "Es crucial en la minería de datos porque preparar los datos brutos para que puedan ser utilizados en análisis estadísticos, modelados predictivos o técnicas de aprendizaje automático, asegurando la calidad, coherencia y accesibilidad de la información."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8f6200-1ad0-4fa8-9cf1-aa1df22c4b3b",
   "metadata": {},
   "source": [
    "### 1.1 Crear estructura de directorios segun modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "605182b8-9652-410e-8c79-13b10a33e692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rutas: \n",
    "\n",
    "carpeta_entrada    = Path(ruta_base)\n",
    "#carpeta_entrada_mnt   = Path('/mnt/data')\n",
    "carpeta_datasets_entrada   = carpeta_entrada / 'datasets_entrada'\n",
    "carpeta_datasets_entrada.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "carpeta_datasets_salida   = carpeta_entrada / 'datasets_salida'\n",
    "carpeta_datasets_salida.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "carpeta_reportes   = carpeta_datasets_salida / 'reportes'\n",
    "carpeta_reportes.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "carpeta_limpios    = carpeta_datasets_salida / 'limpios'\n",
    "carpeta_limpios.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Nombres esperados de archivos\n",
    "archivo_ventas     = 'ventas.csv'\n",
    "archivo_clientes   = 'clientes.csv'\n",
    "archivo_marketing  = 'marketing.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b65281a-0cd0-43a6-8a84-ed2692ae60b2",
   "metadata": {},
   "source": [
    "### 1.2 rutas y carga de los dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "052e6d76-d227-4749-af2c-f065eabf82f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datasets del curso...\n",
      "...Arhivos cargados\n"
     ]
    }
   ],
   "source": [
    "# --- Paso 3: Cargar archivos del curso ---\n",
    "print(\"Cargando datasets del curso...\")\n",
    "try:\n",
    "    ruta_ventas     = os.path.join(carpeta_datasets_entrada, archivo_ventas)\n",
    "    ruta_clientes   = os.path.join(carpeta_datasets_entrada, archivo_clientes)\n",
    "    ruta_marketing  = os.path.join(carpeta_datasets_entrada, archivo_marketing)\n",
    "    \n",
    "    df_ventas       = pd.read_csv(f\"{ruta_ventas}\")\n",
    "    df_clientes     = pd.read_csv(f\"{ruta_clientes}\")\n",
    "    df_marketing    = pd.read_csv(f\"{ruta_marketing}\")\n",
    "    dic_dfs = { \"df_ventas\"   : df_ventas,\n",
    "                \"df_clientes\" : df_clientes,\n",
    "                \"df_marketing\": df_marketing}\n",
    "    print (\"...Arhivos cargados\")\n",
    "except:\n",
    "    print (\"Arhivos no encontrados\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e91d4ff-4465-4ec3-b5e1-29f5d4cc0277",
   "metadata": {},
   "source": [
    "### 1.3 Estructura de parámetros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "895c63d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Proceso principal para los 3 CSV ----------\n",
    "desviacion_margen     = 1.5\n",
    "desviacion_umbral     = 3.0\n",
    "cantidad_duplicados   = 0\n",
    "reportes_creados      = []\n",
    "ruta_excel            = carpeta_reportes / 'reporte_limpieza.xlsx'\n",
    "guardado_ok           = False\n",
    "mensajes              = []\n",
    "TOKENS_VALOR_FALTANTE = {'na', 'n/a', 'null', 'none', 'sin dato', 's/d', 'nd', '-', '--', '?', 'sin_dato', 'n/d'}\n",
    "#id_campanha,producto,canal,costo,fecha_inicio,fecha_fin\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# id_campanha,producto,canal,costo,fecha_inicio,fecha_fin\n",
    "reglas_marketing = {\n",
    "                        'producto':    ('lower',   {'normalizar_acentos': True}),\n",
    "                        'canal':       ('lower',   {'normalizar_acentos': True}),\n",
    "                        'costo':       ('numeric', {'remove_thousands': True, 'as_int': False}),\n",
    "                        'fecha_inicio':('date',    {'dayfirst': True, 'formats': ['%d/%m/%Y', '%Y-%m-%d']}),\n",
    "                        'fecha_fin':   ('date',    {'dayfirst': True, 'formats': ['%d/%m/%Y', '%Y-%m-%d']})\n",
    "}\n",
    "# id_venta,producto,precio,cantidad,fecha_venta,categoria\n",
    "reglas_ventas = {\n",
    "                        'producto':    ('lower',   {'normalizar_acentos': True}),\n",
    "                        'precio':      ('numeric', {'remove_thousands': True, 'as_int': False}),\n",
    "                        'cantidad':    ('numeric', {'remove_thousands': True, 'as_int': False}),\n",
    "                        'fecha_venta': ('date',    {'dayfirst': True, 'formats': ['%d/%m/%Y', '%Y-%m-%d']}),\n",
    "                        'categoria':   ('lower',   {'normalizar_acentos': True})\n",
    "}\n",
    "#id_cliente,nombre,edad,ciudad,ingresos\n",
    "reglas_clientes = {\n",
    "                        'nombre'  :    ('title',   {'normalizar_acentos': True}),\n",
    "                        'edad'    :    ('numeric', {'remove_thousands': True, 'as_int': True}),\n",
    "                        'ciudad'  :    ('title',   {'normalizar_acentos': True}),\n",
    "                        'ingresos':    ('numeric', {'remove_thousands': True, 'as_int': False})\n",
    "}\n",
    "\n",
    "reglas_por_archivo = {\n",
    "                        'ventas.csv'   : reglas_ventas,\n",
    "                        'clientes.csv' : reglas_clientes,\n",
    "                        'marketing.csv': reglas_marketing\n",
    "}\n",
    "\n",
    "#zip_path = carpeta_reportes.parent / 'reports_dataset_tpi_v2.zip'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a06a6528-ba7e-4f2c-889a-5d428f5a1cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    df_ventas\n",
      "        Descripción preliminar:\n",
      "                  id_venta     cantidad\n",
      "count  3035.000000  3033.000000\n",
      "mean   1499.851400     6.496538\n",
      "std     866.465379     3.457250\n",
      "min       1.000000     1.000000\n",
      "25%     748.500000     3.000000\n",
      "50%    1502.000000     7.000000\n",
      "75%    2249.500000     9.000000\n",
      "max    3000.000000    12.000000\n",
      "        Dimensiones:2\n",
      "        Forma:(3035, 6)    \n",
      "        Número de elementos:18210\n",
      "        Nombres de columnas:Index(['id_venta', 'producto', 'precio', 'cantidad', 'fecha_venta',\n",
      "       'categoria'],\n",
      "      dtype='object')\n",
      "        Nombres de filas:RangeIndex(start=0, stop=3035, step=1)\n",
      "        Tipos de datos:\n",
      "id_venta         int64\n",
      "producto        object\n",
      "precio          object\n",
      "cantidad       float64\n",
      "fecha_venta     object\n",
      "categoria       object\n",
      "dtype: object\n",
      "        Primeras 10 filas:\n",
      "   id_venta           producto   precio  cantidad fecha_venta  \\\n",
      "0       792  Cuadro decorativo   $69.94       5.0  02/01/2024   \n",
      "1       811    Lámpara de mesa  $105.10       5.0  02/01/2024   \n",
      "2      1156           Secadora   $97.96       3.0  02/01/2024   \n",
      "3      1372           Heladera  $114.35       8.0  02/01/2024   \n",
      "4      1546           Secadora  $106.21       4.0  02/01/2024   \n",
      "5      1697    Horno eléctrico   $35.35       9.0  02/01/2024   \n",
      "6      1710   Plancha de vapor   $65.43       2.0  02/01/2024   \n",
      "7      2959          Proyector   $88.17       9.0  02/01/2024   \n",
      "8       318  Rincón de plantas   $79.86      11.0  03/01/2024   \n",
      "9       419         Candelabro   $66.11       8.0  03/01/2024   \n",
      "\n",
      "           categoria  \n",
      "0         Decoración  \n",
      "1         Decoración  \n",
      "2  Electrodomésticos  \n",
      "3  Electrodomésticos  \n",
      "4  Electrodomésticos  \n",
      "5  Electrodomésticos  \n",
      "6  Electrodomésticos  \n",
      "7        Electrónica  \n",
      "8         Decoración  \n",
      "9         Decoración  \n",
      "        Últimas 3 filas:\n",
      "      id_venta                producto   precio  cantidad fecha_venta  \\\n",
      "3032      2696                  Laptop  $107.81       4.0  30/12/2024   \n",
      "3033      2913              Smartphone   $99.85       7.0  30/12/2024   \n",
      "3034      2930  Consola de videojuegos   $55.47       6.0  30/12/2024   \n",
      "\n",
      "        categoria  \n",
      "3032  Electrónica  \n",
      "3033  Electrónica  \n",
      "3034  Electrónica  \n",
      "    **************************************************\n",
      "    \n",
      "\n",
      "    df_clientes\n",
      "        Descripción preliminar:\n",
      "               id_cliente        edad      ingresos\n",
      "count  578.000000  578.000000    578.000000\n",
      "mean   289.500000   37.968858  34755.977266\n",
      "std    166.998503   10.253244  12989.576812\n",
      "min      1.000000   20.000000    170.290000\n",
      "25%    145.250000   30.000000  26119.060000\n",
      "50%    289.500000   37.000000  35102.285000\n",
      "75%    433.750000   43.000000  42600.435000\n",
      "max    578.000000   81.000000  88053.010000\n",
      "        Dimensiones:2\n",
      "        Forma:(578, 5)    \n",
      "        Número de elementos:2890\n",
      "        Nombres de columnas:Index(['id_cliente', 'nombre', 'edad', 'ciudad', 'ingresos'], dtype='object')\n",
      "        Nombres de filas:RangeIndex(start=0, stop=578, step=1)\n",
      "        Tipos de datos:\n",
      "id_cliente      int64\n",
      "nombre         object\n",
      "edad            int64\n",
      "ciudad         object\n",
      "ingresos      float64\n",
      "dtype: object\n",
      "        Primeras 10 filas:\n",
      "   id_cliente               nombre  edad                 ciudad  ingresos\n",
      "0           1      Aloysia Screase    44          Mar del Plata  42294.68\n",
      "1           2  Kristina Scaplehorn    25                Posadas  24735.04\n",
      "2           3       Filip Castagne    50            Resistencia  35744.85\n",
      "3           4          Liuka Luard    39           Bahía Blanca  27647.96\n",
      "4           5        Dore Cockshtt    28                Rosario  28245.65\n",
      "5           6        Patrick Earle    34  San Miguel de Tucumán  62763.31\n",
      "6           7           Etan Deeth    35            Resistencia  37489.71\n",
      "7           8       Booth Bielfelt    40                Córdoba  35255.94\n",
      "8           9         Shirl Labone    29                Rosario  27592.08\n",
      "9          10      Andy Mendenhall    52                Rosario  37153.94\n",
      "        Últimas 3 filas:\n",
      "     id_cliente           nombre  edad         ciudad  ingresos\n",
      "575         576  Cari Marzellano    38  Mar del Plata  53114.05\n",
      "576         577   Magdalene Pegg    34        Córdoba  49849.42\n",
      "577         578    Lorry Santori    47     Corrientes  42000.81\n",
      "    **************************************************\n",
      "    \n",
      "\n",
      "    df_marketing\n",
      "        Descripción preliminar:\n",
      "               id_campanha      costo\n",
      "count    90.000000  90.000000\n",
      "mean     45.500000   4.928667\n",
      "std      26.124701   0.947750\n",
      "min       1.000000   2.950000\n",
      "25%      23.250000   4.372500\n",
      "50%      45.500000   4.900000\n",
      "75%      67.750000   5.562500\n",
      "max      90.000000   7.390000\n",
      "        Dimensiones:2\n",
      "        Forma:(90, 6)    \n",
      "        Número de elementos:540\n",
      "        Nombres de columnas:Index(['id_campanha', 'producto', 'canal', 'costo', 'fecha_inicio',\n",
      "       'fecha_fin'],\n",
      "      dtype='object')\n",
      "        Nombres de filas:RangeIndex(start=0, stop=90, step=1)\n",
      "        Tipos de datos:\n",
      "id_campanha       int64\n",
      "producto         object\n",
      "canal            object\n",
      "costo           float64\n",
      "fecha_inicio     object\n",
      "fecha_fin        object\n",
      "dtype: object\n",
      "        Primeras 10 filas:\n",
      "   id_campanha             producto  canal  costo fecha_inicio   fecha_fin\n",
      "0           74      Adorno de pared     TV   4.81   20/03/2024  03/05/2024\n",
      "1           12               Tablet   RRSS   3.40   26/03/2024  13/05/2024\n",
      "2           32      Lámpara de mesa  Email   5.54   28/03/2024  20/04/2024\n",
      "3           21           Smartphone   RRSS   6.37   29/03/2024  16/05/2024\n",
      "4           58             Alfombra  Email   4.25   31/03/2024  05/05/2024\n",
      "5           85           SmartWatch     TV   5.07   01/04/2024  05/05/2024\n",
      "6           36     Plancha de vapor  Email   5.41   02/04/2024  01/06/2024\n",
      "7           57             Batidora  Email   4.48   10/04/2024  08/06/2024\n",
      "8           44      Adorno de pared  Email   5.08   13/04/2024  10/05/2024\n",
      "9           84  Parlantes Bluetooth     TV   4.42   17/04/2024  05/05/2024\n",
      "        Últimas 3 filas:\n",
      "    id_campanha            producto  canal  costo fecha_inicio  fecha_fin\n",
      "87           68   Rincón de plantas     TV   5.81   17/12/2024  14/2/2025\n",
      "88           33            Secadora  Email   3.80   20/12/2024   7/1/2025\n",
      "89           11  Freidora eléctrica   RRSS   5.27   29/12/2024  21/1/2025\n",
      "    **************************************************\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Mostrar el DataFrame\n",
    "def ver():\n",
    "    for nombre,df in dic_dfs.items():\n",
    "        print(f\"\"\"\n",
    "    {nombre}\n",
    "        Descripción preliminar:\n",
    "        {df.describe()}\n",
    "        Dimensiones:{ df.ndim}\n",
    "        Forma:{ df.shape}    \n",
    "        Número de elementos:{ df.size}\n",
    "        Nombres de columnas:{ df.columns}\n",
    "        Nombres de filas:{ df.index}\n",
    "        Tipos de datos:\\n{ df.dtypes}\n",
    "        Primeras 10 filas:\\n{ df.head(10)}\n",
    "        Últimas 3 filas:\\n{ df.tail(3)}\n",
    "    {\"*\"*50}\n",
    "    \"\"\")\n",
    "ver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21030142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Funciones utilitarias en castellano (snake_case) ----------\n",
    "def sacar_acentos(entrada):\n",
    "    \"\"\"\n",
    "    Elimina acentos (tildes/diacríticos) de un texto.\n",
    "    Mantiene NaN intactos.\n",
    "    \"\"\"\n",
    "    if pd.isna(entrada):\n",
    "        return entrada\n",
    "    texto = str(entrada)\n",
    "    nk = unicodedata.normalize('NFKD', entrada)\n",
    "    salida = ''.join([c for c in nk if not unicodedata.combining(c)])\n",
    "    '''\n",
    "    if salida != entrada:\n",
    "        print(\"\"\"\\033[1;37;44m\\n\n",
    "╔═════════════════════════════════════════════════════════════════════════════╗\n",
    "║                                sacar_acentos                                ║\n",
    "╚═════════════════════════════════════════════════════════════════════════════╝\\033[0;m\"\"\")\n",
    "    print(f\"    {texto} -->  {salida}\")\n",
    "    '''\n",
    "    return salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ddbaa5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Funciones de limpieza ----------\n",
    "def aplicar_regla_columna(serie, regla):\n",
    "    \"\"\"\n",
    "    Aplica una regla a una serie (columna).\n",
    "    Firma compatible con la versión anterior (reemplaza la implementación previa).\n",
    "    regla puede ser:\n",
    "      - 'strip','lower','upper','title','quitar_acentos','numeric','date'\n",
    "    o un tuple (tipo, opciones) con opciones:\n",
    "      - numeric: remove_non_digits (bool), remove_thousands (bool), as_int (bool), thousands_separator (',' o '.')\n",
    "      - date: formats (list), dayfirst (bool), format_output ('YYYY/MM/DD' para forzar cadena)\n",
    "      - texto: normalizar_acentos (bool)\n",
    "    Retorna la serie transformada (sin forzar dtype final).\n",
    "    \"\"\"\n",
    "    tipo, opts = regla if isinstance(regla, tuple) else (regla, {})\n",
    "    opts       = opts or {}\n",
    "    s          = serie.copy()\n",
    "    # -------- Texto y normalización de acentos opcional --------\n",
    "    if tipo == 'strip':\n",
    "        s = s.map(lambda x: str(x).strip() if not pd.isna(x) else x)\n",
    "    elif tipo == 'lower':\n",
    "        s = s.map(lambda x: str(x).strip().lower() if not pd.isna(x) else x)\n",
    "        if opts.get('normalizar_acentos'):\n",
    "            s = s.map(lambda x: sacar_acentos(x) if not pd.isna(x) else x)\n",
    "    elif tipo == 'upper':\n",
    "        s = s.map(lambda x: str(x).strip().upper() if not pd.isna(x) else x)\n",
    "        if opts.get('normalizar_acentos'):\n",
    "            s = s.map(lambda x: sacar_acentos(x) if not pd.isna(x) else x)\n",
    "    elif tipo == 'title':\n",
    "        s = s.map(lambda x: str(x).strip().title()  if not pd.isna(x) else x)\n",
    "        if opts.get('normalizar_acentos'):\n",
    "            s = s.map(lambda x: sacar_acentos(x) if not pd.isna(x) else x)\n",
    "    elif tipo == 'quitar_acentos':\n",
    "        s = s.map(lambda x: sacar_acentos(x).strip() if not pd.isna(x) else x)\n",
    "    # -------- Numeric robusto --------\n",
    "    elif tipo == 'numeric':\n",
    "        def to_num(v):\n",
    "            if pd.isna(v):\n",
    "                return np.nan\n",
    "            t = str(v).strip()\n",
    "            t = t.replace('$', '')\n",
    "            if opts.get('remove_non_digits', False):\n",
    "                t = ''.join([c for c in t if c.isdigit() or c in '.-'])\n",
    "            if opts.get('remove_thousands', False):\n",
    "                sep = opts.get('thousands_separator', ',')\n",
    "                if sep == ',':\n",
    "                    # Quitar puntos mil y comas de decimales no soportadas -> suponer coma miles y punto decimales\n",
    "                    t = t.replace('.', '').replace(',', '')\n",
    "                else:\n",
    "                    t = t.replace(',', '').replace('.', '')   \n",
    "            try:\n",
    "                val = pd.to_numeric(t, errors='coerce')\n",
    "                if opts.get('as_int', False):\n",
    "                    if pd.isna(val):\n",
    "                        return pd.NA\n",
    "                    try:\n",
    "                        # intentar entero simple\n",
    "                        return int(val)\n",
    "                    except Exception:\n",
    "                        return pd.NA\n",
    "                return float(val) if not pd.isna(val) else np.nan\n",
    "            except Exception:\n",
    "                return np.nan\n",
    "        s = s.map(to_num)\n",
    "\n",
    "    # -------- Fecha robusta --------\n",
    "    elif tipo == 'date':\n",
    "        formatos = opts.get('formats', [])\n",
    "        dayfirst = opts.get('dayfirst', True)\n",
    "        def to_date(v):\n",
    "            if pd.isna(v):\n",
    "                return pd.NaT\n",
    "            t = str(v).strip()\n",
    "            # Probar formatos explícitos\n",
    "            for fmt in formatos:\n",
    "                try:\n",
    "                    return pd.to_datetime(datetime.strptime(t, fmt))\n",
    "                except Exception:\n",
    "                    continue\n",
    "            # Fallback: pandas con dayfirst\n",
    "            try:\n",
    "                return pd.to_datetime(t, dayfirst=dayfirst, errors='coerce')\n",
    "            except Exception:\n",
    "                return pd.NaT\n",
    "        s = s.map(to_date)\n",
    "\n",
    "    else:\n",
    "        # Default: trim y convertir tokens faltantes a NaN para texto\n",
    "        if s.dtype == object or pd.api.types.is_string_dtype(s):\n",
    "            s = s.map(lambda x: np.nan if es_valor_faltante(x) else (str(x).strip() if not pd.isna(x) else x))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4aefb8b-3e14-4917-9986-8b7f27033cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_valor_faltante(valor):\n",
    "    \"\"\"\n",
    "    Determina si un valor debe considerarse faltante (True) usando tokens y NaN.\n",
    "    \"\"\"\n",
    "    if pd.isna(valor):\n",
    "        return True\n",
    "    s = str(valor).strip().lower()\n",
    "    s = sacar_acentos(s)\n",
    "    return s in TOKENS_VALOR_FALTANTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e185c054-2f13-4a34-be4d-67dbd546abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertir_tipos_postprocesamiento(df, reglas_por_columna):\n",
    "    \"\"\"\n",
    "    Garantiza dtypes correctos:\n",
    "     - Para columnas numeric: pd.to_numeric(...) + conversión a Int64 nullable si as_int, o float.\n",
    "     - Para columnas date: intenta parsear según formatos; deja datetime64[ns] o, si 'format_output'=='YYYY/MM/DD', devuelve strings con ese formato.\n",
    "    \"\"\"\n",
    "    print(\"\"\"\\033[1;37;44m\n",
    "╔═════════════════════════════════════════════════════════════════════════════╗\n",
    "║                          Cambio de tipos de datos                           ║\n",
    "╚═════════════════════════════════════════════════════════════════════════════╝\\033[0;m\"\"\")\n",
    "    df2 = df.copy()\n",
    "    for col, regla in (reglas_por_columna or {}).items():\n",
    "        tipo = regla if not isinstance(regla, tuple) else regla[0]\n",
    "        opts = {} if not isinstance(regla, tuple) else regla[1] or {}\n",
    "        if tipo == 'numeric':\n",
    "            df2[col] = pd.to_numeric(df2[col], errors='coerce')\n",
    "            if opts.get('as_int', False):\n",
    "                # convertir a Int64 nullable\n",
    "                try:\n",
    "                    df2[col] = df2[col].astype('Int64')\n",
    "                except Exception:\n",
    "                    # fallback: mantener float si conversion falla\n",
    "                    df2[col] = pd.to_numeric(df2[col], errors='coerce')\n",
    "        elif tipo == 'date':\n",
    "            formatos = opts.get('formats', [])\n",
    "            dayfirst = opts.get('dayfirst', True)\n",
    "            parsed = pd.Series(pd.NaT, index=df2.index)\n",
    "            # probar formatos explícitos uno por uno\n",
    "            for fmt in formatos:\n",
    "                try:\n",
    "                    mask_necesita = parsed.isna()\n",
    "                    intent = pd.to_datetime(df2.loc[mask_necesita, col].astype(str), format=fmt, errors='coerce')\n",
    "                    parsed.loc[mask_necesita] = intent\n",
    "                except Exception:\n",
    "                    pass\n",
    "            # fallback general para los que quedaron NaT\n",
    "            still_na = parsed.isna()\n",
    "            if still_na.any():\n",
    "                parsed.loc[still_na] = pd.to_datetime(df2.loc[still_na, col].astype(str), dayfirst=dayfirst, errors='coerce')\n",
    "            df2[col] = parsed\n",
    "            if opts.get('format_output') == 'YYYY/MM/DD':\n",
    "                # convertir a string con formato pedido (mantener NaT como NaN)\n",
    "                df2[col] = df2[col].dt.strftime('%Y/%m/%d')\n",
    "    for col in df2.select_dtypes(include=['object']).columns:\n",
    "        df2[col] = df2[col].map(sacar_acentos)\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "337bd9f4-5291-4b9d-af66-23a8e3086c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_dataframe(df, reglas_por_columna=None):\n",
    "    global cantidad_duplicados\n",
    "    \"\"\"\n",
    "    Limpieza principal (misma firma que antes):\n",
    "     1) Aplica aplicar_regla_columna por cada columna según reglas_por_columna\n",
    "     2) Para columnas texto por defecto: strip + tokens faltantes -> NaN\n",
    "     3) Elimina duplicados exactos\n",
    "     4) Convierte tipos numéricos y fechas con convertir_tipos_postprocesamiento\n",
    "    Retorna df limpio con dtypes corregidos.\n",
    "\n",
    "    Aplica reglas de limpieza columna por columna.\n",
    "    Si una columna no existe en el DataFrame, se omite con aviso educativo.\n",
    "    \"\"\"\n",
    "    #reglas_por_columna = reglas_por_columna or {}\n",
    "\n",
    "    reglas_por_columna = reglas_por_columna or {}\n",
    "    df2 = df.copy()\n",
    "    df2.columns = [str(c).strip() for c in df2.columns]\n",
    "\n",
    "    for col in df2.columns:\n",
    "        if col in reglas_por_columna:\n",
    "            # usar la función aplicar_regla_columna existente en el notebook\n",
    "            df2[col] = aplicar_regla_columna(df2[col], reglas_por_columna[col])\n",
    "        else:\n",
    "            if df2[col].dtype == object or pd.api.types.is_string_dtype(df2[col]):\n",
    "                df2[col] = df2[col].map(lambda x: np.nan if es_valor_faltante(x) else (str(x).strip() if not pd.isna(x) else x))\n",
    "                if col == \"categoria\":\n",
    "                    input(f\"{col=}\")\n",
    "\n",
    "                \n",
    "    print(\"\"\"\\033[1;37;44m\n",
    "╔═════════════════════════════════════════════════════════════════════════════╗\n",
    "║                                DUPLICADOS                                   ║\n",
    "╠═════════════════════════════════════════════════════════════════════════════╣\n",
    "║     cantidad_duplicados                                                     ║\n",
    "║             Se cuentan la totalidad de los duplicados (incluye el original) ║\n",
    "║             Se eliminan las repeticiones (2do en adelante)                  ║\n",
    "╚═════════════════════════════════════════════════════════════════════════════╝\\033[0;m\"\"\")\n",
    "    \n",
    "    cantidad_duplicados = df.duplicated(keep=False).sum()\n",
    "    print(f\"Total de filas duplicadas (incluyendo original): {cantidad_duplicados}\")\n",
    "    \n",
    "    # Eliminar duplicados dejando la primera aparición\n",
    "    df2 = df.drop_duplicates(keep='first').reset_index(drop=True)\n",
    "    print(f\"Duplicados eliminados. Total de filas restantes: {len(df2)}\")\n",
    "    \n",
    "    cantidad_nulos = df2.isnull().sum().sum()\n",
    "    print(f\"\"\"\\033[1;37;44m\n",
    "╔═════════════════════════════════════════════════════════════════════════════╗\n",
    "║                                 VALORES NULOS                               ║\n",
    "╠═════════════════════════════════════════════════════════════════════════════╣\n",
    "║     cantidad_nulos = {cantidad_nulos}                                                      ║\n",
    "║             Se cuentan los NaN o valores vacíos en el DataFrame             ║\n",
    "║             Se pueden imputar o eliminar según el caso                      ║\n",
    "╚═════════════════════════════════════════════════════════════════════════════╝\\033[0;m\"\"\")\n",
    "    if cantidad_nulos > 0:\n",
    "        print(\"\\nColumnas con nulos:\")\n",
    "        print(df2.isnull().sum()[df2.isnull().sum() > 0])\n",
    "        \n",
    "        # Opción 1: eliminar filas con nulos\n",
    "        df2 = df2.dropna().reset_index(drop=True)\n",
    "        print(f\"\\nFilas con nulos eliminadas. Total de filas restantes: {len(df2)}\")\n",
    "    else:\n",
    "        print(\"No se encontraron valores nulos en este DataFrame.\")\n",
    "   \n",
    "    print(\"\"\"\\033[1;37;44m\\n\n",
    "╔═════════════════════════════════════════════════════════════════════════════╗\n",
    "║                     Aplico reglas según columna específica                  ║\n",
    "╠═════════════════════════════════════════════════════════════════════════════╣\n",
    "║     Afecta                                                                  ║\n",
    "║             Numéricos (int/float)                                           ║\n",
    "║             fechas --> YYYY,MM,DD                                           ║\n",
    "╚═════════════════════════════════════════════════════════════════════════════╝\\033[0;m\"\"\")\n",
    "    for col, regla in (reglas_por_columna or {}).items():\n",
    "        tipo = regla if not isinstance(regla, tuple) else regla[0]\n",
    "        opts = {} if not isinstance(regla, tuple) else regla[1] or {}\n",
    "        if tipo == 'numeric':\n",
    "            df2[col] = pd.to_numeric(df2[col], errors='coerce')\n",
    "            if opts.get('as_int', False):\n",
    "                try:\n",
    "                    df2[col] = df2[col].astype('Int64')\n",
    "                except Exception:\n",
    "                    pass\n",
    "        elif tipo == 'date':\n",
    "            formatos = opts.get('formats', [])\n",
    "            dayfirst = opts.get('dayfirst', True)\n",
    "            parsed = pd.Series(pd.NaT, index=df2.index)\n",
    "            for fmt in formatos:\n",
    "                try:\n",
    "                    mask = parsed.isna()\n",
    "                    parsed.loc[mask] = pd.to_datetime(df2.loc[mask, col].astype(str), format=fmt, errors='coerce')\n",
    "                except Exception:\n",
    "                    pass\n",
    "            still_na = parsed.isna()\n",
    "            if still_na.any():\n",
    "                parsed.loc[still_na] = pd.to_datetime(df2.loc[still_na, col].astype(str), dayfirst=dayfirst, errors='coerce')\n",
    "            # si se pidió format_output, devolver cadena con YYYY/MM/DD\n",
    "            if opts.get('format_output') == 'YYYY/MM/DD':\n",
    "                df2[col] = parsed.dt.strftime('%Y/%m/%d')\n",
    "            else:\n",
    "                df2[col] = parsed\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4634aeae-c21a-47dc-9bb9-cffdeafcbe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Detección de outliers (IQR) - función corregida y robusta\n",
    "def mascara_valores_atipicos_rango_intercuartil(serie_datos):\n",
    "    \"\"\"\n",
    "    Devuelve una tupla: (mascara_bool_series, cantidad_outliers, (limite_inferior, limite_superior))\n",
    "    - serie_datos: pd.Series (acepta valores no numéricos, se intentará convertir)\n",
    "    - La máscara tiene la misma indexación que la serie original (NaNs -> False)\n",
    "    \"\"\"\n",
    "    # Intentar convertir a numérico (coerce -> NaN para no numéricos)\n",
    "    serie_numerica = pd.to_numeric(serie_datos, errors='coerce')\n",
    "    # Serie limpia para cálculos de cuartiles (sin NaN)\n",
    "    serie_limpia = serie_numerica.dropna().astype(float)\n",
    "    if serie_limpia.shape[0] < 4:\n",
    "        # No hay suficientes datos para IQR: devolver máscara False de la misma longitud\n",
    "        mascara = pd.Series([False] * len(serie_datos), index=serie_datos.index)\n",
    "        return mascara, int(mascara.sum()), (None, None)\n",
    "    cuartil_1 = float(serie_limpia.quantile(0.25))\n",
    "    cuartil_3 = float(serie_limpia.quantile(0.75))\n",
    "    rango_intercuartil = cuartil_3 - cuartil_1\n",
    "    limite_inferior = cuartil_1 - desviacion_margen * rango_intercuartil\n",
    "    limite_superior = cuartil_3 + desviacion_margen * rango_intercuartil\n",
    "    # Crear máscara sobre la serie numérica original (alineada con el index original)\n",
    "    mascara = (serie_numerica < limite_inferior) | (serie_numerica > limite_superior)\n",
    "    mascara = mascara.fillna(False).astype(bool)\n",
    "    return mascara, int(mascara.sum()), (limite_inferior, limite_superior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c15444c-3df6-409e-a932-c08afaebfd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funciones utilitarias definidas.\n"
     ]
    }
   ],
   "source": [
    "# Detección de outliers (Z-score)\n",
    "def mascara_valores_atipicos_zscore(serie_datos, desviacion_umbral=3.0):\n",
    "    \"\"\"\n",
    "    Devuelve máscara booleana (True = outlier) según Z-score.\n",
    "    \"\"\"\n",
    "    serie_limpia = serie_datos.dropna().astype(float)\n",
    "    if serie_limpia.shape[0] < 4 or serie_limpia.std() == 0:\n",
    "        return pd.Series([False] * len(serie_datos), index=serie_datos.index)\n",
    "    puntaje_z = (serie_datos - serie_limpia.mean()) / serie_limpia.std()\n",
    "    return puntaje_z.abs() > desviacion_umbral  \n",
    "print('Funciones utilitarias definidas.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f411cb90-17af-45f2-9dfb-9fe7534bacbb",
   "metadata": {},
   "source": [
    "# limpieza y normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe131403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Función detectar_problemas_en_dataframe cargada.\n"
     ]
    }
   ],
   "source": [
    "# ---------- Detección de problemas en un DataFrame ----------\n",
    "def detectar_problemas_en_dataframe(df: pd.DataFrame):\n",
    " \n",
    "    resumen                       = {}\n",
    "    resumen['filas']              = df.shape[0]\n",
    "    resumen['columnas']           = df.shape[1]\n",
    "    resumen['nulos_por_columna']  = df.isna().sum().to_dict()\n",
    "    dup_mask                      = df.duplicated(keep=False)\n",
    "    resumen['duplicados_exactos'] = int(dup_mask.sum())\n",
    "    chequeos_por_columna          = {}\n",
    "    print(f\"\"\"\\033[1;37;44m\\n\n",
    "╔═════════════════════════════════════════════════════════════════════════════╗\n",
    "║                              Valores a eliminar                             ║\n",
    "╠═════════════════════════════════════════════════════════════════════════════╣\n",
    "║     Afecta                                                                  ║\n",
    "║         Elimina espacios iniciales y finales.                               ║\n",
    "║         Borra Na                                                            ║\n",
    "║         Borra duplicados                                                    ║\n",
    "╚═════════════════════════════════════════════════════════════════════════════╝\\033[0;m\"\"\") \n",
    "    print(f\"\"\"\\033[1;37;44m\\n\n",
    "╔═════════════════════════════════════════════════════════════════════════════╗\n",
    "║                               Valores atípicos                              ║\n",
    "╠═════════════════════════════════════════════════════════════════════════════╣\n",
    "║     Afecta                                                                  ║\n",
    "║         NO Modifica datos.                                                  ║\n",
    "║         Se guarda la información en archivo excel para referencias futuras  ║\n",
    "║         Se evalua es mediante dos formas                                    ║\n",
    "║            1) limites intercuartiles 25 y 75 % * desviacion_margen {desviacion_margen}      ║\n",
    "║            2) Z-score mayor a desviacion_umbral {desviacion_umbral}                         ║\n",
    "╚═════════════════════════════════════════════════════════════════════════════╝\\033[0;m\"\"\")\n",
    "    for col in df.columns:\n",
    "        serie = df[col]\n",
    "        info = {'dtype': str(serie.dtype), 'nulos': int(serie.isna().sum())}\n",
    "        if serie.dtype == object or pd.api.types.is_string_dtype(serie):\n",
    "            s = serie.astype(str)\n",
    "            info['espacios_inicio']          = int(s.str.match(r'^\\s+').sum())\n",
    "            info['espacios_final']           = int(s.str.match(r'\\s+$').sum())\n",
    "            try:\n",
    "                unique_original              = set(s.dropna().unique())\n",
    "                unique_lower                 = set(s.dropna().str.lower().unique())\n",
    "                info['unique_original']      = len(unique_original)\n",
    "                info['unique_lower']         = len(unique_lower)\n",
    "                info['variantes_mayusculas'] = len(unique_lower) < len(unique_original)\n",
    "            except Exception:\n",
    "                info['unique_original']      = serie.nunique(dropna=True)\n",
    "                info['unique_lower']         = None\n",
    "                info['variantes_mayusculas'] = None\n",
    "            try:\n",
    "                unaccented                   = s.dropna().map(lambda x: sacar_acentos(x).lower())\n",
    "                groups                       = unaccented.groupby(unaccented).size()\n",
    "                conflicts                    = groups[groups > 1]\n",
    "                info['grupos_var_acentos']   = int(conflicts.shape[0])\n",
    "                ejemplos = {}\n",
    "                if not conflicts.empty:\n",
    "                    for val in conflicts.index[:5]:\n",
    "                        originales           = sorted(list(s[unaccented == val].unique())[:10])\n",
    "                        ejemplos[val]        = originales\n",
    "                info['ejemplos_var_acentos'] = ejemplos\n",
    "            except Exception:\n",
    "                info['grupos_var_acentos']   = None\n",
    "                info['ejemplos_var_acentos'] = {}\n",
    "            info['tokens_aparente_faltante'] = int(\n",
    "                s.map(lambda x: str(x).strip().lower()).map(lambda v: sacar_acentos(v) in TOKENS_VALOR_FALTANTE).sum()\n",
    "            )\n",
    "            info['muestras'] = list(s.dropna().unique()[:10])\n",
    "        else:\n",
    "            # numeric\n",
    "            if pd.api.types.is_numeric_dtype(serie) or (serie.dropna().astype(str).str.replace('.','',1).str.isnumeric().all() if len(serie.dropna())>0 else False):\n",
    "                try:\n",
    "                    serie_numerica = serie.dropna().astype(float)\n",
    "                except Exception:\n",
    "                    serie_numerica = pd.to_numeric(serie, errors='coerce').dropna().astype(float)\n",
    "                info['media'] = float(serie_numerica.mean()) if not serie_numerica.empty else None\n",
    "                info['std']   = float(serie_numerica.std()) if not serie_numerica.empty else None\n",
    "                info['min']   = float(serie_numerica.min()) if not serie_numerica.empty else None\n",
    "                info['max']   = float(serie_numerica.max()) if not serie_numerica.empty else None\n",
    "                if len(serie_numerica) >= 4:\n",
    "                    mascara_outliers_iqr, cant,(info['outliers_iqr'] ,info['limites_iqr']) =mascara_valores_atipicos_rango_intercuartil(serie)\n",
    "                    #mascara, int(mascara.sum()), (limite_inferior, limite_superior)\n",
    "                    '''\n",
    "                    print (f\"\"\"\n",
    "                    {mascara_outliers_iqr=}\n",
    "                    {cant=}\n",
    "                    {info['outliers_iqr']=}\n",
    "                    {info['limites_iqr']=}\n",
    "                    {\"-\"*100}\n",
    "                    \"\"\")\n",
    "                    '''\n",
    "                    \n",
    "                else:\n",
    "                    info['outliers_iqr'] = None\n",
    "                    info['limites_iqr'] = None\n",
    "                if len(serie_numerica) >= 4 and serie_numerica.std() != 0:\n",
    "                    z = (serie_numerica - serie_numerica.mean()) / serie_numerica.std()\n",
    "                    info['outliers_z'] = int((z.abs() > 3).sum())\n",
    "                else:\n",
    "                    info['outliers_z'] = None\n",
    "            else:\n",
    "                # fechas intento parseo\n",
    "                parsed = pd.to_datetime(serie, errors='coerce', dayfirst=True)\n",
    "                info['fechas_parseables'] = int(parsed.notna().sum())\n",
    "                info['muestras'] = list(serie.dropna().unique()[:10])\n",
    "        chequeos_por_columna[col] = info\n",
    "\n",
    "    filas_problemas = []\n",
    "    df.columns = [c.strip().lower().replace(' ', '_') for c in df.columns]\n",
    "    for idx, fila in df.iterrows():\n",
    "        lista_problemas = []\n",
    "        if dup_mask.loc[idx]:\n",
    "            lista_problemas.append('duplicado_exacto')\n",
    "        for col in df.columns:\n",
    "            val = fila[col]\n",
    "            \n",
    "            # heurísticas textuales\n",
    "            if pd.api.types.is_string_dtype(type(val)) or isinstance(val, str) or (\n",
    "                not pd.isna(val) and not pd.api.types.is_numeric_dtype(type(val)) and str(chequeos_por_columna[col].get('dtype','')).startswith('object')\n",
    "            ):\n",
    "                s = str(val)\n",
    "                if s != s.strip():\n",
    "                    lista_problemas.append(f'espacios_en_columna_{col}')\n",
    "                if chequeos_por_columna[col].get('variantes_mayusculas'):\n",
    "                    if s and s != s.lower() and s.lower() in [str(x).lower() for x in df[col].dropna().unique()]:\n",
    "                        lista_problemas.append(f'inconsistencia_mayusculas_columna_{col}')\n",
    "                if chequeos_por_columna[col].get('grupos_var_acentos') and chequeos_por_columna[col]['grupos_var_acentos'] > 0:\n",
    "                    try:\n",
    "                        #un = sacar_acentos(s).lower()\n",
    "                        #group_vals = [x for x in chequeos_por_columna[col].get('muestras', []) if sacar_acentos(str(x)).lower() == un]\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        df['col'] = df['col'].apply(sacar_acentos)\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        if group_vals and any(sacar_acentos(str(x)).lower() != sacar_acentos(s).lower() for x in group_vals):\n",
    "                            lista_problemas.append(f'variantes_acentos_columna_{col}')\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                if es_valor_faltante(s):\n",
    "                    lista_problemas.append(f'token_faltante_columna_{col}')\n",
    "            else:\n",
    "                # heurísticas numéricas\n",
    "                try:\n",
    "                    fval = float(val)\n",
    "                    info_col = chequeos_por_columna[col]\n",
    "                    limites = info_col.get('limites_iqr')\n",
    "                    if limites and (fval < limites[0] or fval > limites[1]):\n",
    "                        lista_problemas.append(f'outlier_iqr_columna_{col}')\n",
    "                    if info_col.get('std') not in (None, 0):\n",
    "                        mean = info_col.get('media')\n",
    "                        std = info_col.get('std')\n",
    "                        if std and abs((fval - mean) / std) > 3:\n",
    "                            lista_problemas.append(f'outlier_z_columna_{col}')\n",
    "                except Exception:\n",
    "                    pass\n",
    "        if lista_problemas:\n",
    "            filas_problemas.append({\n",
    "                'row_index': idx,\n",
    "                'problemas': ';'.join(sorted(set(lista_problemas))),\n",
    "                'muestra': json.dumps({str(c): str(fila[c]) for c in df.columns[:8]})\n",
    "            })\n",
    "\n",
    "    df_problemas = pd.DataFrame(filas_problemas)\n",
    "    return resumen, chequeos_por_columna, df_problemas\n",
    "\n",
    "print('Función detectar_problemas_en_dataframe cargada.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e42599a-abe2-4447-b8e6-6c1024f0d651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función auxiliar para guardar CSVs\n",
    "def guardar_csv(df, ruta):\n",
    "    \"\"\"\n",
    "    Guarda df en ruta (string o Path). Crea directorio padre si no existe.\n",
    "    \"\"\"\n",
    "    ruta = Path(ruta)\n",
    "    ruta.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(ruta, index=False, encoding='utf-8')\n",
    "    return ruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ccc0f0c3-0e31-4f27-9661-3215c9185f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_frames = {\n",
    "    ruta_ventas     : df_ventas,\n",
    "    ruta_clientes   : df_clientes,\n",
    "    ruta_marketing  : df_marketing\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "32235fb9-1e09-422e-931b-3471d6fbf123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37;44m\n",
      "\n",
      "╔═════════════════════════════════════════════════════════════════════════════╗\n",
      "║                     Aplico reglas según columna específica                  ║\n",
      "╠═════════════════════════════════════════════════════════════════════════════╣\n",
      "║     Afecta                                                                  ║\n",
      "║             Numéricos (int/float)                                           ║\n",
      "║             fechas --> YYYY,MM,DD                                           ║\n",
      "╚═════════════════════════════════════════════════════════════════════════════╝\u001b[0;m\n",
      "\u001b[1;37;44m\n",
      "\n",
      "╔═════════════════════════════════════════════════════════════════════════════╗\n",
      "║                              Valores a eliminar                             ║\n",
      "╠═════════════════════════════════════════════════════════════════════════════╣\n",
      "║     Afecta                                                                  ║\n",
      "║         Elimina espacios iniciales y finales.                               ║\n",
      "║         Borra Na                                                            ║\n",
      "║         Borra duplicados                                                    ║\n",
      "╚═════════════════════════════════════════════════════════════════════════════╝\u001b[0;m\n",
      "\u001b[1;37;44m\n",
      "\n",
      "╔═════════════════════════════════════════════════════════════════════════════╗\n",
      "║                               Valores atípicos                              ║\n",
      "╠═════════════════════════════════════════════════════════════════════════════╣\n",
      "║     Afecta                                                                  ║\n",
      "║         NO Modifica datos.                                                  ║\n",
      "║         Se guarda la información en archivo excel para referencias futuras  ║\n",
      "║         Se evalua es mediante dos formas                                    ║\n",
      "║            1) limites intercuartiles 25 y 75 % * desviacion_margen 1.5      ║\n",
      "║            2) Z-score mayor a desviacion_umbral 3.0                         ║\n",
      "╚═════════════════════════════════════════════════════════════════════════════╝\u001b[0;m\n",
      "--- RESUMEN ANTES: ventas.csv ---\n",
      "\u001b[1;37;44m\n",
      "╔═════════════════════════════════════════════════════════════════════════════╗\n",
      "║                                DUPLICADOS                                   ║\n",
      "╠═════════════════════════════════════════════════════════════════════════════╣\n",
      "║     cantidad_duplicados                                                     ║\n",
      "║             Se cuentan la totalidad de los duplicados (incluye el original) ║\n",
      "║             Se eliminan las repeticiones (2do en adelante)                  ║\n",
      "╚═════════════════════════════════════════════════════════════════════════════╝\u001b[0;m\n",
      "Total de filas duplicadas (incluyendo original): 70\n",
      "Duplicados eliminados. Total de filas restantes: 3000\n",
      "\u001b[1;37;44m\n",
      "╔═════════════════════════════════════════════════════════════════════════════╗\n",
      "║                                 VALORES NULOS                               ║\n",
      "╠═════════════════════════════════════════════════════════════════════════════╣\n",
      "║     cantidad_nulos = 4                                                       ║\n",
      "║             Se cuentan los NaN o valores vacíos en el DataFrame             ║\n",
      "║             Se pueden imputar o eliminar según el caso                      ║\n",
      "╚═════════════════════════════════════════════════════════════════════════════╝\u001b[0;m\n",
      "\n",
      "Columnas con nulos:\n",
      "precio      2\n",
      "cantidad    2\n",
      "dtype: int64\n",
      "\n",
      "Filas con nulos eliminadas. Total de filas restantes: 2998\n",
      "\u001b[1;37;44m\n",
      "\n",
      "╔═════════════════════════════════════════════════════════════════════════════╗\n",
      "║                     Aplico reglas según columna específica                  ║\n",
      "╠═════════════════════════════════════════════════════════════════════════════╣\n",
      "║     Afecta                                                                  ║\n",
      "║             Numéricos (int/float)                                           ║\n",
      "║             fechas --> YYYY,MM,DD                                           ║\n",
      "╚═════════════════════════════════════════════════════════════════════════════╝\u001b[0;m\n",
      "\u001b[1;37;44m\n",
      "\n",
      "╔═════════════════════════════════════════════════════════════════════════════╗\n",
      "║                              Valores a eliminar                             ║\n",
      "╠═════════════════════════════════════════════════════════════════════════════╣\n",
      "║     Afecta                                                                  ║\n",
      "║         Elimina espacios iniciales y finales.                               ║\n",
      "║         Borra Na                                                            ║\n",
      "║         Borra duplicados                                                    ║\n",
      "╚═════════════════════════════════════════════════════════════════════════════╝\u001b[0;m\n",
      "\u001b[1;37;44m\n",
      "\n",
      "╔═════════════════════════════════════════════════════════════════════════════╗\n",
      "║                               Valores atípicos                              ║\n",
      "╠═════════════════════════════════════════════════════════════════════════════╣\n",
      "║     Afecta                                                                  ║\n",
      "║         NO Modifica datos.                                                  ║\n",
      "║         Se guarda la información en archivo excel para referencias futuras  ║\n",
      "║         Se evalua es mediante dos formas                                    ║\n",
      "║            1) limites intercuartiles 25 y 75 % * desviacion_margen 1.5      ║\n",
      "║            2) Z-score mayor a desviacion_umbral 3.0                         ║\n",
      "╚═════════════════════════════════════════════════════════════════════════════╝\u001b[0;m\n",
      "nombre_limpio = ventas_limpio.csv\n",
      "Guardado cleaned en: datasets_salida\\limpios\\ventas_limpio.csv\n",
      "\u001b[1;37;44m\n",
      "\n",
      "╔═════════════════════════════════════════════════════════════════════════════╗\n",
      "║                              Valores a eliminar                             ║\n",
      "╠═════════════════════════════════════════════════════════════════════════════╣\n",
      "║     Afecta                                                                  ║\n",
      "║         Elimina espacios iniciales y finales.                               ║\n",
      "║         Borra Na                                                            ║\n",
      "║         Borra duplicados                                                    ║\n",
      "╚═════════════════════════════════════════════════════════════════════════════╝\u001b[0;m\n",
      "\u001b[1;37;44m\n",
      "\n",
      "╔═════════════════════════════════════════════════════════════════════════════╗\n",
      "║                               Valores atípicos                              ║\n",
      "╠═════════════════════════════════════════════════════════════════════════════╣\n",
      "║     Afecta                                                                  ║\n",
      "║         NO Modifica datos.                                                  ║\n",
      "║         Se guarda la información en archivo excel para referencias futuras  ║\n",
      "║         Se evalua es mediante dos formas                                    ║\n",
      "║            1) limites intercuartiles 25 y 75 % * desviacion_margen 1.5      ║\n",
      "║            2) Z-score mayor a desviacion_umbral 3.0                         ║\n",
      "╚═════════════════════════════════════════════════════════════════════════════╝\u001b[0;m\n",
      "--- RESUMEN ANTES: clientes.csv ---\n",
      "\u001b[1;37;44m\n",
      "╔═════════════════════════════════════════════════════════════════════════════╗\n",
      "║                                DUPLICADOS                                   ║\n",
      "╠═════════════════════════════════════════════════════════════════════════════╣\n",
      "║     cantidad_duplicados                                                     ║\n",
      "║             Se cuentan la totalidad de los duplicados (incluye el original) ║\n",
      "║             Se eliminan las repeticiones (2do en adelante)                  ║\n",
      "╚═════════════════════════════════════════════════════════════════════════════╝\u001b[0;m\n",
      "Total de filas duplicadas (incluyendo original): 0\n",
      "Duplicados eliminados. Total de filas restantes: 578\n",
      "\u001b[1;37;44m\n",
      "╔═════════════════════════════════════════════════════════════════════════════╗\n",
      "║                                 VALORES NULOS                               ║\n",
      "╠═════════════════════════════════════════════════════════════════════════════╣\n",
      "║     cantidad_nulos = 0                                                       ║\n",
      "║             Se cuentan los NaN o valores vacíos en el DataFrame             ║\n",
      "║             Se pueden imputar o eliminar según el caso                      ║\n",
      "╚═════════════════════════════════════════════════════════════════════════════╝\u001b[0;m\n",
      "No se encontraron valores nulos en este DataFrame.\n",
      "\u001b[1;37;44m\n",
      "\n",
      "╔═════════════════════════════════════════════════════════════════════════════╗\n",
      "║                     Aplico reglas según columna específica                  ║\n",
      "╠═════════════════════════════════════════════════════════════════════════════╣\n",
      "║     Afecta                                                                  ║\n",
      "║             Numéricos (int/float)                                           ║\n",
      "║             fechas --> YYYY,MM,DD                                           ║\n",
      "╚═════════════════════════════════════════════════════════════════════════════╝\u001b[0;m\n",
      "\u001b[1;37;44m\n",
      "\n",
      "╔═════════════════════════════════════════════════════════════════════════════╗\n",
      "║                              Valores a eliminar                             ║\n",
      "╠═════════════════════════════════════════════════════════════════════════════╣\n",
      "║     Afecta                                                                  ║\n",
      "║         Elimina espacios iniciales y finales.                               ║\n",
      "║         Borra Na                                                            ║\n",
      "║         Borra duplicados                                                    ║\n",
      "╚═════════════════════════════════════════════════════════════════════════════╝\u001b[0;m\n",
      "\u001b[1;37;44m\n",
      "\n",
      "╔═════════════════════════════════════════════════════════════════════════════╗\n",
      "║                               Valores atípicos                              ║\n",
      "╠═════════════════════════════════════════════════════════════════════════════╣\n",
      "║     Afecta                                                                  ║\n",
      "║         NO Modifica datos.                                                  ║\n",
      "║         Se guarda la información en archivo excel para referencias futuras  ║\n",
      "║         Se evalua es mediante dos formas                                    ║\n",
      "║            1) limites intercuartiles 25 y 75 % * desviacion_margen 1.5      ║\n",
      "║            2) Z-score mayor a desviacion_umbral 3.0                         ║\n",
      "╚═════════════════════════════════════════════════════════════════════════════╝\u001b[0;m\n",
      "nombre_limpio = clientes_limpio.csv\n",
      "Guardado cleaned en: datasets_salida\\limpios\\clientes_limpio.csv\n",
      "\u001b[1;37;44m\n",
      "\n",
      "╔═════════════════════════════════════════════════════════════════════════════╗\n",
      "║                              Valores a eliminar                             ║\n",
      "╠═════════════════════════════════════════════════════════════════════════════╣\n",
      "║     Afecta                                                                  ║\n",
      "║         Elimina espacios iniciales y finales.                               ║\n",
      "║         Borra Na                                                            ║\n",
      "║         Borra duplicados                                                    ║\n",
      "╚═════════════════════════════════════════════════════════════════════════════╝\u001b[0;m\n",
      "\u001b[1;37;44m\n",
      "\n",
      "╔═════════════════════════════════════════════════════════════════════════════╗\n",
      "║                               Valores atípicos                              ║\n",
      "╠═════════════════════════════════════════════════════════════════════════════╣\n",
      "║     Afecta                                                                  ║\n",
      "║         NO Modifica datos.                                                  ║\n",
      "║         Se guarda la información en archivo excel para referencias futuras  ║\n",
      "║         Se evalua es mediante dos formas                                    ║\n",
      "║            1) limites intercuartiles 25 y 75 % * desviacion_margen 1.5      ║\n",
      "║            2) Z-score mayor a desviacion_umbral 3.0                         ║\n",
      "╚═════════════════════════════════════════════════════════════════════════════╝\u001b[0;m\n",
      "--- RESUMEN ANTES: marketing.csv ---\n",
      "\u001b[1;37;44m\n",
      "╔═════════════════════════════════════════════════════════════════════════════╗\n",
      "║                                DUPLICADOS                                   ║\n",
      "╠═════════════════════════════════════════════════════════════════════════════╣\n",
      "║     cantidad_duplicados                                                     ║\n",
      "║             Se cuentan la totalidad de los duplicados (incluye el original) ║\n",
      "║             Se eliminan las repeticiones (2do en adelante)                  ║\n",
      "╚═════════════════════════════════════════════════════════════════════════════╝\u001b[0;m\n",
      "Total de filas duplicadas (incluyendo original): 0\n",
      "Duplicados eliminados. Total de filas restantes: 90\n",
      "\u001b[1;37;44m\n",
      "╔═════════════════════════════════════════════════════════════════════════════╗\n",
      "║                                 VALORES NULOS                               ║\n",
      "╠═════════════════════════════════════════════════════════════════════════════╣\n",
      "║     cantidad_nulos = 0                                                       ║\n",
      "║             Se cuentan los NaN o valores vacíos en el DataFrame             ║\n",
      "║             Se pueden imputar o eliminar según el caso                      ║\n",
      "╚═════════════════════════════════════════════════════════════════════════════╝\u001b[0;m\n",
      "No se encontraron valores nulos en este DataFrame.\n",
      "\u001b[1;37;44m\n",
      "\n",
      "╔═════════════════════════════════════════════════════════════════════════════╗\n",
      "║                     Aplico reglas según columna específica                  ║\n",
      "╠═════════════════════════════════════════════════════════════════════════════╣\n",
      "║     Afecta                                                                  ║\n",
      "║             Numéricos (int/float)                                           ║\n",
      "║             fechas --> YYYY,MM,DD                                           ║\n",
      "╚═════════════════════════════════════════════════════════════════════════════╝\u001b[0;m\n",
      "\u001b[1;37;44m\n",
      "\n",
      "╔═════════════════════════════════════════════════════════════════════════════╗\n",
      "║                              Valores a eliminar                             ║\n",
      "╠═════════════════════════════════════════════════════════════════════════════╣\n",
      "║     Afecta                                                                  ║\n",
      "║         Elimina espacios iniciales y finales.                               ║\n",
      "║         Borra Na                                                            ║\n",
      "║         Borra duplicados                                                    ║\n",
      "╚═════════════════════════════════════════════════════════════════════════════╝\u001b[0;m\n",
      "\u001b[1;37;44m\n",
      "\n",
      "╔═════════════════════════════════════════════════════════════════════════════╗\n",
      "║                               Valores atípicos                              ║\n",
      "╠═════════════════════════════════════════════════════════════════════════════╣\n",
      "║     Afecta                                                                  ║\n",
      "║         NO Modifica datos.                                                  ║\n",
      "║         Se guarda la información en archivo excel para referencias futuras  ║\n",
      "║         Se evalua es mediante dos formas                                    ║\n",
      "║            1) limites intercuartiles 25 y 75 % * desviacion_margen 1.5      ║\n",
      "║            2) Z-score mayor a desviacion_umbral 3.0                         ║\n",
      "╚═════════════════════════════════════════════════════════════════════════════╝\u001b[0;m\n",
      "nombre_limpio = marketing_limpio.csv\n",
      "Guardado cleaned en: datasets_salida\\limpios\\marketing_limpio.csv\n",
      "Proceso completo del diccionario de DataFrames.\n"
     ]
    }
   ],
   "source": [
    "# ---------- Bucle/menu principal (usa dic_frames) ----------\n",
    "def menu_procesar_diccionario(dic_frames, reglas_por_archivo):\n",
    "    \"\"\"\n",
    "    Recorre dic_frames: clave = nombre_archivo (ej. 'marketing.csv'), valor = DataFrame.\n",
    "    Ejecuta: detectar_problemas_en_dataframe antes, limpiar_dataframe, detectar_problemas_en_dataframe despues,\n",
    "    imprime resúmenes y guarda cleaned en carpeta_limpios con sufijo ' limpio.csv'.\n",
    "    También sobreescribe variables en RAM (df_marketing, df_ventas, df_clientes) si se encuentran en el nombre.\n",
    "    \"\"\"\n",
    "    print(\"\"\"\\033[1;37;44m\\n\n",
    "╔═════════════════════════════════════════════════════════════════════════════╗\n",
    "║                     Aplico reglas según columna específica                  ║\n",
    "╠═════════════════════════════════════════════════════════════════════════════╣\n",
    "║     Afecta                                                                  ║\n",
    "║             Numéricos (int/float)                                           ║\n",
    "║             fechas --> YYYY,MM,DD                                           ║\n",
    "╚═════════════════════════════════════════════════════════════════════════════╝\\033[0;m\"\"\")\n",
    "    errores_df=pd.DataFrame()\n",
    "    # trabajamos sobre una copia para evitar modificar dict original por error\n",
    "    for [path_archivo, df_actual],[nombre_archivo,_] in zip( dic_frames.items() , reglas_por_archivo.items() ):\n",
    "        resumen_antes, chequeos_antes, problemas_antes = detectar_problemas_en_dataframe(df_actual)\n",
    "        print(f\"--- RESUMEN ANTES: {nombre_archivo} ---\")\n",
    "        # Para no volcar objetos muy grandes, mostramos el head del DataFrame de problemas (si existe)\n",
    "        #print('Muestras de problemas antes (primeras 5 filas):')\n",
    "        #print( display(problemas_antes.head(5) if not problemas_antes.empty else 'No se detectaron filas con problemas.') )\n",
    "\n",
    "        errores_df  = pd.concat ([errores_df,problemas_antes])\n",
    "\n",
    "        #df_actual[df_actual.duplicated(keep=False)].copy()\n",
    "        reglas    = reglas_por_archivo.get(nombre_archivo, {})\n",
    "        df_limpio = limpiar_dataframe(df_actual, reglas_por_columna=reglas)\n",
    "\n",
    "        resumen_despues, chequeos_despues, problemas_despues = detectar_problemas_en_dataframe(df_limpio)\n",
    "        '''\n",
    "        print(f\"n--- RESUMEN DESPUÉS: {nombre_archivo} ---\")\n",
    "        print(resumen_despues)\n",
    "        print('Muestras de problemas después (primeras 5 filas):')\n",
    "        print( display(problemas_despues.head(5) if not problemas_despues.empty else 'No se detectaron filas con problemas tras la limpieza.'))\n",
    "        '''\n",
    "        # mostrar separadores y tipo-nombre\n",
    "\n",
    "        #print('-'*100)\n",
    "        #print(f'nombre_archivo = {nombre_archivo}')\n",
    "        #print(f'type(nombre_archivo) = {type(nombre_archivo)}')\n",
    "        #print('-'*100)\n",
    "\n",
    "        # Guardo el archivo en limpios  ruta_base carpeta_reportes\n",
    "        nombre_limpio = nombre_archivo[:-4] + '_limpio.csv' if nombre_archivo.lower().endswith('.csv') else nombre_archivo + ' limpio.csv'\n",
    "        print(f'nombre_limpio = {nombre_limpio}')\n",
    "    \n",
    "        # guardar cleaned\n",
    "        ruta_guardado = carpeta_limpios / nombre_limpio\n",
    "        guardar_csv(df_limpio, ruta_guardado)\n",
    "        print(f'Guardado cleaned en: {ruta_guardado}')\n",
    "\n",
    "        # sobreescribir en RAM según el nombre\n",
    "        # (nota: usar globals() para actualizar variables en el entorno global del notebook)\n",
    "        dic_dfs[nombre_archivo.lower()] = df_limpio\n",
    "        '''\n",
    "        if 'marketing' in nombre_archivo.lower():\n",
    "            globals()['df_marketing'] = df_limpio\n",
    "        elif 'ventas' in nombre_archivo.lower():\n",
    "            globals()['df_ventas'] = df_limpio\n",
    "        elif 'clientes' in nombre_archivo.lower():\n",
    "            globals()['df_clientes'] = df_limpio\n",
    "        '''\n",
    "    print('Proceso completo del diccionario de DataFrames.')\n",
    "    return errores_df\n",
    "errores_df = menu_procesar_diccionario(dic_frames, reglas_por_archivo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f4ffe5-0f8d-465e-be9e-6d180fa42789",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#CCCCCC; padding:10px; border-radius:6px;\">\n",
    "<h2 style=\"color:black; text-align:center;\">Resultados de limpieza</h2>\n",
    "<p style=\"color:black;\">- Revisados los 3 csv  pasados a DataFrames.</p>\n",
    "<p style=\"color:blue;\">- DataFrames filtrados.</p>\n",
    "<p style=\"color:black;\">- Filtrado de Nulos.</p>\n",
    "<p style=\"color:black;\">- Filtrado de duplicados.</p>\n",
    "<p style=\"color:black;\">- Sin '', 'na', 'n/a', 'null', 'none', 'sin dato', 's/d', 'nd', '-', '--', '?', 'sin_dato', 'n/d'</p>    \n",
    "<p style=\"color:black;\">- Normalisados Strings segun reglas. Estilo (lower,title.upper) unicodedata.normalize('NFKD')</p>\n",
    "<p style=\"color:black;\">- Normalisados precios a float sin signo ($)</p>\n",
    "<p style=\"color:black;\">- Normalisados Numericos a int o float segun regla</p>    \n",
    "<p style=\"color:black;\">- Normalisados Fechas segun regla YYYY/MM/DD</p>    \n",
    "<p style=\"color:black;\">- Resguardo <code>datasets_salida/limpios/clientes_limpio.csv</code>.</p>\n",
    "<p style=\"color:black;\">- Resguardo <code>datasets_salida/limpios/marketing_limpio.csv</code>.</p>\n",
    "<p style=\"color:black;\">- Resguardo <code>datasets_salida/limpios/ventas_limpio.csv</code>.</p>\n",
    "<p style=\"color:blue;\">- Registros filtrados eliminados</p>\n",
    "<p style=\"color:black;\">- Resguardo <code>datasets_salida/reportes/reporte_limpieza.xlsx</code> con hojas (duplicados borrados, outliers, totales)</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1ab45f-4494-4b83-8470-20c6aab64b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Crear df_duplicados_total\n",
    "print (f\"\"\"\n",
    "{cantidad_duplicados=}\n",
    "\"\"\")\n",
    "# Filtra las filas donde la columna 'problemas' contiene la subcadena 'duplicado_exacto'\n",
    "df_duplicados_total = errores_df[errores_df['problemas'].str.contains('duplicado_exacto', case=False, na=False)].copy()\n",
    "\n",
    "# 2. Crear df_outliers_total\n",
    "# Filtra las filas donde la columna 'problemas' contiene la subcadena 'outlier_'\n",
    "df_outliers_total = errores_df[errores_df['problemas'].str.contains('outlier_', case=False, na=False)].copy()\n",
    "\n",
    "# 3. Crear df_resumen (Combinación y Ordenamiento)\n",
    "# Concatena los dos DataFrames creados\n",
    "df_resumen = pd.concat([df_duplicados_total, df_outliers_total])\n",
    "\n",
    "# Ordena el DataFrame resultante por la columna 'problemas'\n",
    "df_resumen = df_resumen.sort_values(by=\"problemas\").reset_index(drop=True)\n",
    "'''\n",
    "print (f\"\"\"\n",
    "df_duplicados_total\n",
    "{df_duplicados_total}\n",
    "\n",
    "{\"-\"*100}\n",
    "\n",
    "df_outliers_total\n",
    "{df_outliers_total}\n",
    "\n",
    "{\"-\"*100}\n",
    "df_resumen\n",
    "{df_resumen}\n",
    "{\"-\"*100}\n",
    "\"\"\")\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3a213a-5293-46f6-ad63-4cabc9e5b920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar en Excel con manejo de fallo si no existe el engine\n",
    "try:\n",
    "    with pd.ExcelWriter(ruta_excel, engine='openpyxl') as writer:\n",
    "        try:\n",
    "            df_duplicados_total.to_excel(writer, index=False, sheet_name='duplicados')\n",
    "        except Exception:\n",
    "            pd.DataFrame().to_excel(writer, index=False, sheet_name='duplicados')\n",
    "        try:\n",
    "            df_outliers_total.to_excel(writer, index=False, sheet_name='outliers')\n",
    "        except Exception:\n",
    "            pd.DataFrame().to_excel(writer, index=False, sheet_name='outliers')\n",
    "        try:\n",
    "            df_resumen.to_excel(writer, index=False, sheet_name='resumen_filtros')\n",
    "        except Exception:\n",
    "            pd.DataFrame(lista_resumen).to_excel(writer, index=False, sheet_name='resumen_filtros')\n",
    "    guardado_ok = True\n",
    "except Exception as e_openpyxl:\n",
    "    mensajes.append('Error usando openpyxl: ' + str(e_openpyxl))\n",
    "    try:\n",
    "        with pd.ExcelWriter(ruta_excel) as writer:\n",
    "            df_duplicados_total.to_excel(writer, index=False, sheet_name='duplicados')\n",
    "            df_outliers_total.to_excel(writer, index=False, sheet_name='outliers')\n",
    "            df_resumen.to_excel(writer, index=False, sheet_name='resumen_filtros')\n",
    "        guardado_ok = True\n",
    "    except Exception as e_default:\n",
    "        mensajes.append('Error sin engine: ' + str(e_default))\n",
    "        try:\n",
    "            df_duplicados_total.to_csv(carpeta_reportes / 'duplicados.csv', index=False, encoding='utf-8')\n",
    "            df_outliers_total.to_csv(carpeta_reportes / 'outliers.csv', index=False, encoding='utf-8')\n",
    "            df_resumen.to_csv(carpeta_reportes / 'resumen_filtros.csv', index=False, encoding='utf-8')\n",
    "            mensajes.append('Se guardaron CSVs separados como fallback.')\n",
    "            guardado_ok = True\n",
    "        except Exception as e_csv:\n",
    "            mensajes.append('Error guardando CSV fallback: ' + str(e_csv))\n",
    "            guardado_ok = False\n",
    "\n",
    "print('Guardado OK:', guardado_ok)\n",
    "if mensajes:\n",
    "    print('Mensajes/Errores durante guardado:')\n",
    "    for m in mensajes:\n",
    "        print('-', m)\n",
    "print('Ruta final esperada del Excel (si guardado):', ruta_excel)\n",
    "print('Resumen por dataset:')\n",
    "print(df_resumen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f43150b-eb7b-4ae8-a0a6-233f4b6ca5f1",
   "metadata": {},
   "source": [
    "ventas.csv  análisis de ventas, limpieza de datos y estadísticas descriptivas.\n",
    " \t\n",
    "clientes.csv  unirse a las ventas mediante el uso de funciones de combinación para analizar características de los clientes relacionados con sus \tcompras.\n",
    " \t\n",
    "marketing.csv analizar la efectividad de las campañas de marketing en las ventas y buscar correlaciones.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f5fcf9c-6140-4160-9ca0-ebf688f8c921",
   "metadata": {},
   "source": [
    "2) Claves candidatas para merge o concat\n",
    "\n",
    "Intersecciones observadas:\n",
    "\n",
    "ventas ∩ marketing → producto (clave natural para unir campañas con ventas por producto)\n",
    "\n",
    "ventas ∩ clientes → ninguna columna en común\n",
    "\n",
    "clientes ∩ marketing → ninguna columna en común\n",
    "\n",
    "Interpretación pedagógica:\n",
    "\n",
    "Puedes unir ventas con marketing por producto (p. ej. para ver qué canal promocionó qué producto y coste).\n",
    "\n",
    "No puedes unir directamente ventas con clientes porque ventas_limpio no contiene id_cliente ni email ni nombre_cliente. Para unir ventas↔clientes necesitas:\n",
    "\n",
    "que ventas_limpio tenga una columna id_cliente (recomendado), o\n",
    "\n",
    "un fichero/mapeo que vincule id_venta → id_cliente, o\n",
    "\n",
    "usar correspondencia por email/nombre (menos fiable) si esos campos existieran.\n",
    "\n",
    "3) Ejemplos de combinaciones útiles (casos prácticos)\n",
    "\n",
    "Ventas por canal de marketing (recomendado)\n",
    "\n",
    "Merge ventas_limpio ⟵ marketing_limpio por producto (left join): asignás a cada venta el canal y id_campanha. Luego agrupás por canal para métricas.\n",
    "\n",
    "Análisis de ticket promedio por categoría y canal\n",
    "\n",
    "Después del merge anterior, crear monto = precio * cantidad y agrupar por categoria y canal.\n",
    "\n",
    "Concatenación (vertical)\n",
    "\n",
    "Si tuvieras varios archivos de ventas de distintos periodos: pd.concat([ventas_periodo1, ventas_periodo2], axis=0).\n",
    "\n",
    "Unir clientes (si se dispone de id_cliente en ventas)\n",
    "\n",
    "ventas_limpio.merge(clientes_limpio, on='id_cliente', how='left') → permite segmentar ventas por edad, ciudad, ingresos.\n",
    "\n",
    "Si no hay id_cliente en ventas\n",
    "\n",
    "Crear un mapping table (archivo) que contenga id_venta → id_cliente y hacer merge por esa tabla.\n",
    "\n",
    "4) Código limpio en pandas (listo para ejecutar — adaptá nombres de columnas si quieres otro comportamiento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93885b49-e04a-401c-9c03-78439696cdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3) Tipos y limpiezas básicas\n",
    "ventas['fecha_venta'] = pd.to_datetime(ventas['fecha_venta'], errors='coerce')\n",
    "marketing['fecha_inicio'] = pd.to_datetime(marketing['fecha_inicio'], errors='coerce')\n",
    "marketing['fecha_fin'] = pd.to_datetime(marketing['fecha_fin'], errors='coerce')\n",
    "\n",
    "# Asegurar numéricos\n",
    "ventas['precio'] = pd.to_numeric(ventas['precio'], errors='coerce')\n",
    "ventas['cantidad'] = pd.to_numeric(ventas['cantidad'], errors='coerce')\n",
    "clientes['ingresos'] = pd.to_numeric(clientes['ingresos'], errors='coerce')\n",
    "marketing['costo'] = pd.to_numeric(marketing['costo'], errors='coerce')\n",
    "\n",
    "# 4) Crear columnas útiles\n",
    "ventas['monto'] = ventas['precio'] * ventas['cantidad']\n",
    "\n",
    "# 5) Merge ejemplo: ventas + marketing por 'producto' (asignar canal a cada venta)\n",
    "ventas_marketing = pd.merge(\n",
    "    ventas,\n",
    "    marketing[['producto', 'id_campanha', 'canal', 'costo', 'fecha_inicio', 'fecha_fin']],\n",
    "    on='producto',\n",
    "    how='left',   # left para conservar todas las ventas aunque no tengan campana asociada\n",
    "    validate='m:1'  # opcional: espera muchos registros ventas para 1 campaña por producto\n",
    ")\n",
    "\n",
    "# 6) Agregados: ventas por canal\n",
    "ventas_por_canal = (\n",
    "    ventas_marketing\n",
    "    .groupby('canal', dropna=False)\n",
    "    .agg(\n",
    "        total_monto=('monto', 'sum'),\n",
    "        cantidad_transacciones=('monto', 'count'),\n",
    "        ticket_promedio=('monto', 'mean')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# 7) Agregado: ventas por categoria y canal\n",
    "ventas_categoria_canal = (\n",
    "    ventas_marketing\n",
    "    .groupby(['categoria', 'canal'], dropna=False)\n",
    "    .agg(\n",
    "        total_monto     = ('monto', 'sum'),\n",
    "        transacciones   = ('monto', 'count'),\n",
    "        ticket_promedio = ('monto', 'mean')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# 8) Guardar resultados (opcional)\n",
    "ventas_por_canal.to_csv('/mnt/data/ventas_por_canal.csv', index=False)\n",
    "ventas_categoria_canal.to_csv('/mnt/data/ventas_categoria_canal.csv', index=False)\n",
    "\n",
    "# 9) ¿Y clientes? Si tienes id_cliente en ventas:\n",
    "# ventas_con_clientes = ventas.merge(clientes, on='id_cliente', how='left')\n",
    "\n",
    "# 10) Checks útiles\n",
    "# - Ver duplicados en claves: ventas['id_venta'].duplicated().sum()\n",
    "# - Ver clientes sin ventas: clientes[~clientes['id_cliente'].isin(ventas.get('id_cliente', []))]\n",
    "\n",
    "Recomendaciones prácticas (breves, accionables)\n",
    "\n",
    "Si querés unir ventas con clientes agregá id_cliente a ventas_limpio (registro en punto de venta o mapeo).\n",
    "\n",
    "Revisá duplicados en producto dentro de marketing (puede haber varias campañas por producto: decidir estrategia — por ejemplo filtrar la campaña activa por fecha).\n",
    "\n",
    "Elegí tipo de join con criterio pedagógico:\n",
    "\n",
    "left join para preservar todas las ventas (evitar perder datos).\n",
    "\n",
    "inner join si sólo te interesa el subset con campaña asociada.\n",
    "\n",
    "Creá fecha de periodo (día/semana/mes) para series temporales: ventas['mes'] = ventas['fecha_venta'].dt.to_period('M').\n",
    "\n",
    "Documentá supuestos: por qué usás how='left', cómo tratás ventas sin campaña, cómo imputás nulos en precio/cantidad.\n",
    "\n",
    "Ejemplo aplicado (interpretación cotidiana)\n",
    "\n",
    "Imaginá a Juan, vendedor en una pyme familiar con 2 hijos. Quiere saber si la campaña en Instagram está trayendo ventas: con el merge ventas + marketing por producto obtiene canal asignado a cada venta. Luego agrupa por canal y ve: “Instagram” tiene muchas visitas pero ticket promedio bajo — decisión: ajustar oferta o dirigir una campaña de cross-sell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40448d5e-62bf-4d58-9c5c-28da021964b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1373299604.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31ma mano\u001b[39m\n      ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "a mano\n",
    "ver que es lo que falta en drop na\n",
    "precios == 0 buscar en categoria el producto o promedo si no hay otro dato\n",
    "duplicate si el id es =\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a21ab2c8-5a03-4fe6-9f02-60d2dc73d966",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (909316874.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mproductos mas vendidos\u001b[39m\n              ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "productos mas vendidos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0737e69-7b61-4fa9-b678-3a178e09b62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ventas por mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d1d542-093a-49ad-b788-5e1b23af562d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
